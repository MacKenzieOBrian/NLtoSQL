# Methodology (open-source replica)

This is the working story for the project: we restage the Ojuri et al. idea with open models and Colab-level GPUs. The flow is simple: NL→SQL on ClassicModels, always feed schema context, try prompting vs QLoRA vs (soon) an agent loop, and keep it reproducible on cheap hardware.

For the pipeline I think of it as a straight line: get schema awareness, pick a strategy (prompt / QLoRA / agent) to spit out SQL, run a quick safety/postprocess, execute on the live DB, score VA/EX/EM (TS later). 

Prompt hygiene stays consistent: system message, schema summary, a few exemplars (k), then the NLQ. Decoding is deterministic. Schema text is ordered so key/name columns show up first. Post-processing grabs the first `SELECT …;`, drops chatter, and trims list-style outputs to minimal columns. k=0 and k=3 are reported separately to avoid leakage.

---

# 3. Domain and Data

- Domain: **ClassicModels (MySQL)**
- Benchmarks: **200 held-out NL→SQL test examples**
- Training set: **~200 supervised examples (JSONL)**

**Rationale:** ClassicModels provides realistic business workloads involving joins, aggregates, and ordering. It also matches the domain used in Ojuri et al. (2025), enabling like-for-like reproduction against GPT-4 agent baselines.


---

# 4. Model Adaptation Strategies

Three adaptation families are evaluated:

| Strategy | Modifies Weights | Training | Iterative | Expected Consistency |
|---|---|---|---|---|
| Zero-Shot ICL | ✗ | ✗ | ✗ | Low |
| Few-Shot ICL | ✗ | ✗ | ✗ | Moderate |
| QLoRA FT | ✓ (Adapters) | ✓ | ✗ | High |
| ReAct / ExCoT | Optional | ✗ | ✓ | Highest |

This mirrors the structure in Ojuri et al. (2025), who compared GPT-4 prompting vs GPT-4 fine-tuned vs GPT-4 agent.

**ReAct alignment (current build):** A minimal Thought/Action/Observation loop with a single tool (safe SELECT executor), strict prompt (“one SELECT only, no DDL/DML/comments”), schema prepended, deterministic decode, and a short 3-step limit. Defaults to a small slice for debugging, then the full 200.

**Current refinements:** Prompt stripping (decode only the reply), a lightweight projection guard (NLQ pattern → canonical minimal SELECT) to prevent extra columns/ORDER BY, and a result-aware retry (only mark success when SQL executes). Small slice hits VA/EX/EM=1.0; full-set run is VA=1.0 but EX≈0.05 (mostly projection/order/logic drift).

**Planned next tweaks:** add lightweight clamps (remove ORDER BY unless requested; trim to requested column count for simple “list/which” queries), try a 3-candidate deterministic beam+rerank on executable SQL, and strengthen prompt hints for aggregates/joins (use customers.country; totals require orderdetails; no invented fields). If EX remains low, expand/retrain adapters with more supervised pairs targeting country/status counts and aggregate/top-N patterns.

**References by design choice (quick map)**
- Schema-grounded prompting and baselines: Zhu et al., 2024 (LLM text-to-SQL survey); Rajkumar et al., 2024 (evaluating LLMs for text-to-SQL) justify VA/EX/TS-style evaluation.
- In-context vs fine-tuning: Mosbach et al., 2023 (few-shot vs FT stability); Chia et al., 2023 and Li et al., 2023 (ICE/C3 exemplar selection) motivate reporting k=0 and k=3.
- PEFT/QLoRA: Hu et al., 2021 (LoRA) and Dettmers et al., 2023 (QLoRA) support adapter-based FT under VRAM limits; Ding et al., 2023 (PEFT survey) as general rationale.
- Safety/SQL constraints: Scholak et al., 2021 (PICARD) and Binder/SQL-PaLM style constrained decoding motivate “single SELECT, no DDL/DML/comments,” prompt stripping, and minimal projection.
- Agentic refinement: Yao et al., 2023 (ReAct) and Shinn et al., 2023 (Reflexion) for Thought/Action/Observation + retries; Gao et al., 2023 and Chen et al., DIN-SQL/ValueNet for execution-based reranking/refinement.
- Benchmarks/context: BIRD (Wang et al., 2023), Spider, and Hong et al., 2025 (LLM DB interfaces) situate using ClassicModels/VA–EX–TS.
- Statistical comparison: Dietterich, 1998 (McNemar) for paired EX comparisons between prompt, QLoRA, and ReAct.

---

# 5. Training Configuration (Current Best Practice)

- Base model: **Llama-3-8B-Instruct**
- Quantization: **4-bit NF4**
- PEFT: **LoRA adapters only**
- Optimizer: `paged_adamw_8bit`
- VRAM target: **≤ 10GB (Colab T4)**
- Dtype: **bf16 on Ampere, fp16 fallback**
- Hyperparameters:
  - r = 32
  - α = 64
  - dropout = 0.05
  - LR ~ 1e-4
  - epochs ≈ 3
  - batch size = 1 w/ gradient accumulation = 8

This configuration allows fine-tuning without full model gradient updates, avoiding >60GB VRAM requirements of full FT.

---


# 9. Comparative Method Justification

### Why not full fine-tuning?
Requires >60GB VRAM; infeasible for open-source reproduction.

### Why not prompting-only?
Few-shot ICL exhibited instability and schema sensitivity, consistent with Mosbach et al. (2023).

### Why agent loops?
Execution feedback reduces logical/aggregation/join errors beyond static decoding (Yao et al., 2023; Ojuri et al., 2025).

### Why ClassicModels?
Matches proprietary baseline domain and workloads; avoids distributional confounds.

### Implementation influences (provenance)
The implementation drew on three pillars: (1) NL→SQL literature for schema grounding, refinement, and execution-based evaluation; (2) open-source ecosystems (HF Transformers/PEFT, SQLAlchemy, Cloud SQL connectors) for idiomatic model loading, fine-tuning, and DB access; and (3) official framework docs for API-level details on inference, quantization, and execution. The orchestration here is original to this reproduction, but follows standard applied NLP practice of combining established tools to answer new research questions.

---

# 10. Narrative for dissertation write-up (learning journey)

- **Baseline (few-shot, deterministic):** Stable VA/EX/EM anchor; schema-grounded prompt + `guarded_postprocess` kept projections minimal. This is the primary result set to report.
- **QLoRA adapters:** Improved VA and modest EX uplift with k=3; limited gains at k=0. Justifies keeping adapters as “best prompt+FT” comparator but not relying on them for zero-shot.
- **ReAct agent:** Multiple iterations: projection/ORDER clamps → select-only filters → execution-guided retry → sampled candidates + one-shot repair. Result: higher VA, but EX still volatile. Frame as exploratory; discuss failure modes (hallucinated columns, aggregation drift) and why execution feedback helps but is not yet converged.
- **Design choices tied to literature:** ReAct loop (Yao et al., 2023), constrained decoding/PICARD (Scholak et al., 2021) for projection/ORDER discipline, Spider EM strictness (Yu et al., 2018) motivating minimal outputs, execution-guided repair (DIN-SQL/ValueNet-style) for error recovery.
- **Reporting plan:** Present baseline and QLoRA as main quantitative results; include ReAct as an interpretability/agentic experiment with trace examples and a candid limitations paragraph. Note future work: richer error-aware retries, schema-level allowlist/grammar constraints, TS metric once schema perturbations are ready.

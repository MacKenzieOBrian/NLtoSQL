# Decisions (Traceable, Literature‑Backed)

## At‑a‑Glance
- **Ladder:** prompt → QLoRA → execution‑guided agent (ablation‑style)
- **Metrics:** VA/EX (EM diagnostic; TS planned)
- **Agent:** execution‑guided reranker with constrained decoding, intent checks, and repair
- **Goal:** causal attribution of gains under Colab‑class constraints

---

## 1) Research Design Decisions
| Decision | Motivation | Alternatives Rejected | Provenance |
|---|---|---|---|
| 200 supervised examples | feasible SFT on Colab; aligns with small‑domain adaptation [10], [12] | >5k examples; synthetic augmentation | `data/train/*.jsonl` |
| SQL validated before training | avoid training on invalid SQL; execution grounding [18] | unvalidated SQL | `04_build_training_set.ipynb` |

**Justification:** ablation‑style comparisons are standard in NL→SQL (Spider/BIRD/Ojuri) [1], [8], [9], [10], [19].

---

## 2) Dataset & Evaluation Integrity
**Decision:** leakage checks, schema validation, NLQ de‑duplication.  
**Motivation:** execution‑based metrics require schema fidelity; leakage invalidates EX.  
**Grounding:** Spider/BIRD practice [1], [19]; survey guidance [8], [9].

---

## 3) Prompt Hygiene (Controlled Variables)
**Decision:** fixed system instruction, fixed schema summary, k ∈ {0,3}, deterministic decoding, guarded first‑SELECT extraction.  
**Motivation:** isolate model competence from prompt engineering [3], [6], [8].

---

## 4) Adaptation Ladder (Prompt → QLoRA → Agent)
| Stage | Mechanism | Expected Gain | Grounding |
|---|---|---|---|
| Prompt‑only | ICL | syntax / schema grounding | [3], [6], [8], [20] |
| QLoRA | adapters | semantic mappings (joins/aggregates) | [10], [12], [4], [5] |
| Agentic | execution feedback | robustness + self‑correction | [2], [10], [16], [18] |

---

## 5) Metrics
**VA** = executable; **EX** = result‑set match; **EM** = diagnostic; **TS** = planned.  
**Justification:** execution‑centric metrics are required due to SQL surface variability [18], [19].

---

## 6) Agent Design Decisions (Execution‑Guided)

### Decision: Staged Control Framework (STAGE 0–3)
**Problem:** multi‑component agents obscure what improved EX.  
**Change:** stage‑gated toggles (0=minimal, 1=clamps, 2=rerank/diversity, 3=repair).  
**Grounding:** ablation‑style evaluation in execution‑guided decoding and ReAct ablations.  
**Outcome:** controlled attribution of gains.

### Decision: Minimal vs Full Agent
**Problem:** no baseline for agentic additions.  
**Change:** STAGE 0 = execution‑gated minimal loop; STAGE ≥1 = ReAct‑style loop with clamps/rerank/repair.  
**Grounding:** execution‑guided decoding (baseline) + ReAct (refinement loop).  
**Outcome:** separates gains from execution guidance vs agentic refinement.

### Decision: Output Control + Intent Constraints
**Problem:** valid‑but‑irrelevant SQL (VA=1, EX=0) and prompt‑echo junk.  
**Change:** stop‑on‑semicolon, prompt‑echo trimming, FROM/dual/dangling‑clause filters, intent checks (grouping/measure).  
**Grounding:** constrained decoding (PICARD) [13], execution‑guided decoding [18], agentic acceptance criteria [16].  
**Outcome:** stability and traceability without weight updates.

### Decision: Projection Contract (Column‑Level Output Control)
**Problem:** EX failures caused by extra or missing columns even when the logic is otherwise correct.  
**Change:** parse NLQs for explicitly requested fields and deterministically drop extraneous SELECT items.  
**Grounding:** schema‑grounded prompting and constrained decoding identify projection drift as a common failure mode.  
**Outcome:** improves EX by aligning output shape with task intent without altering query logic.  
**Refs:** [1], [8]

### Decision: Intent Classifier (Query‑Type Constraints)
**Problem:** models answer the wrong question type (e.g., aggregate instead of list).  
**Change:** classify NLQ as lookup / aggregate / grouped‑aggregate / top‑k and enforce structural constraints accordingly.  
**Grounding:** execution‑guided and agentic NL→SQL work recommends intent‑aligned constraints to prevent valid‑but‑wrong outputs.  
**Outcome:** reduces semantic drift and improves EX without retraining.  
**Refs:** [2], [8], [20]

### Decision: Schema‑Subset Prompting (Lightweight Schema Linking)
**Problem:** wrong table selection and guessed joins dominate semantic errors.  
**Change:** build a minimal schema subset using keyword‑to‑table matching and join hints; prompt with subset rather than full schema.  
**Grounding:** schema‑linking is a dominant factor in NL→SQL performance; lightweight linking improves table selection without heavy models.  
**Outcome:** reduces wrong‑table errors and stabilises candidate generation.  
**Refs:** [17], [8], [9]

### Decision: Semantic Reranking
**Problem:** “fewest columns” heuristic picked wrong semantics.  
**Change:** semantic_score − λ·num_cols.  
**Grounding:** critic‑style reranking (ValueNet/DIN‑SQL) [1], [8], [20].  
**Refs:** [2], [20]

### Decision: ORDER/LIMIT Clamping
**Problem:** hallucinated ordering/projections.  
**Change:** remove ORDER/LIMIT unless requested; trim projections for list queries.  
**Grounding:** constrained decoding [13] and post‑processing in execution‑guided pipelines [18].  
**Refs:** [13], [18]

### Decision: Error‑Aware Repair
**Problem:** no self‑correction path.  
**Change:** one‑shot repair using SQL + DB error + schema.  
**Grounding:** ExCoT and execution‑guided decoding [2], [18].  
**Refs:** [2], [18]

### Decision: Deterministic Fallback
**Problem:** agent failure reduces coverage.  
**Change:** deterministic few‑shot fallback.  
**Grounding:** backoff strategies and self‑consistency baselines [6].

### Decision: Structured Trace Logging
**Problem:** no attribution of failures.  
**Change:** log raw → cleaned → post‑clamp → exec → repair.  
**Outcome:** reproducible failure‑mode analysis.

---

## 7) Safety & Security
Read‑only execution, DDL/DML blocked, role‑constrained DB access. Standard practice for tool‑calling systems.

---

## 8) Reproducibility & Integrity
Pinned deps, deterministic decode, declared env vars, git‑tracked configs.

---

## 9) Compute Constraints
Colab‑class GPU targets, OSS‑only stack. Aligns with SME / regulated deployment assumptions.

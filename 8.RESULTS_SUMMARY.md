# Results Summary

This document summarizes interim results for the NL→SQL replication study. Results quantify the impact of prompting, supervised PEFT fine-tuning (QLoRA), and planned agentic execution-feedback loops on execution-based correctness metrics.

---

## 1. Metrics Evaluated

The following metrics are used:

| Metric | Meaning |
|---|---|
| VA | Valid SQL (syntactic correctness) |
| EX | Execution accuracy (semantic correctness) |
| EM | Exact string match (surface baseline) |
| TS | Test-suite accuracy (planned) |

Execution-based metrics (VA/EX/TS) are considered primary for NL→SQL correctness.

---

## 2. Baselines

### 2.1 Zero-Shot Prompting (k = 0)

- No exemplars
- Schema included in prompt
- Deterministic decoding

**Observed trends**

- VA generally high (LLM respects SQL syntax)
- EX near zero (model lacks relational grounding)

| Metric | Value |
|---|---|
| VA | ~0.80–0.82 |
| EX | ~0.00 |
| EM | ~0.00 |

Interpretation: syntactic accuracy is insufficient for semantic correctness.

---

### 2.2 Few-Shot Prompting (k = 3)

- Exemplar conditioning
- Same model, no supervised training

| Metric | Value |
|---|---|
| VA | ~0.86 |
| EX | ~0.25 |
| EM | >0.00 (but low) |

Interpretation:

- exemplars improve column/table selection
- schema grounding emerges
- matches expectations from Mosbach et al. (2023)

---

## 3. QLoRA Fine-Tuning

Base model: `Llama-3-8B-Instruct`  
PEFT: 4-bit QLoRA (see run configs below)

Two evaluation modes per run: k=0 (adapter-only) and k=3 (adapter + exemplars).

### Run 1 (r=16, 1 epoch, 4-bit)

| Metric | k | VA | EX | EM | Notes |
|---|---|---|---|---|---|
| QLoRA | 0 | 0.73 | 0.03 | 0.005 | adapter-only under baseline EX |
| QLoRA | 3 | 0.86 | 0.305 | 0.260 | roughly baseline few-shot EX |

### Run 2 (r=32, α=64, 3 epochs, warmup, 4-bit)

| Metric | k | VA | EX | EM | Saved |
|---|---|---|---|---|---|
| QLoRA | 0 | 0.865 | 0.065 | 0.000 | `results/qlora/results_zero_shot_200.json` |
| QLoRA | 3 | 0.875 | 0.380 | 0.305 | `results/qlora/results_few_shot_k3_200.json` |

### Run 3 (re-run for reproducibility, same config as Run 2)

| Metric | k | VA | EX | EM | Saved |
|---|---|---|---|---|---|
| QLoRA | 0 | 0.865 | 0.065 | 0.000 | `results/qlora/results_zero_shot_200.json` (overwritten) |
| QLoRA | 3 | 0.870 | 0.380 | 0.310 | `results/qlora/results_few_shot_k3_200.json` (overwritten) |

Interpretation:
- Run 2 improves VA and lifts k=3 EX above the prompt-only few-shot baseline (~0.325→0.380).
- Adapter-only EX is still low; more steps/TS/agentic refinement are likely needed to raise semantic accuracy at k=0. This gap is the main motivation to add a ReAct-style agent loop.

---

## 4. Planned: ReAct / ExCoT Agents

Expected behavior based on Ojuri et al. (2025) and Zhai et al. (2025):

| Strategy | Expected Impact |
|---|---|
| ReAct | +EX, +TS |
| ExCoT | +TS (semantic robustness) |
| Multi-turn | improved join & aggregation reasoning |

TS metric is expected to increase the most since execution-feedback mitigates accidental correctness.

---

## 5. Discussion and Interpretation

Key findings so far:

1. **Syntax ≠ semantics**  
   High VA does not imply correct answers.

2. **Few-shot prompting is strong in low-data regimes**  
   Confirms common NL→SQL literature findings.

3. **Fine-tuning does not trivially outperform prompting**  
   Null result is scientifically meaningful.

4. **ICL + SFT is complementary**  
   Suggests hybrid strategies may win for OSS models.

5. **Agentic refinement is likely the largest lever**  
   Matches results from Ojuri et al. and ExCoT work.

---

## 6. Future Evaluation Work

- implement TS
- add ReAct agent loop
- add ExCoT-style execution trace
- categorize failures by SQL pattern

These extensions would complete the replication.

# Literature Review (NL→SQL)

## At-a-Glance (Why this matters)
- Proprietary LLM agents (GPT‑4 + ReAct) define the upper bound but are non‑reproducible.
- Open‑source models need **PEFT** + **execution‑guided control** to close the semantic gap.
- Execution‑centric metrics (VA/EX/TS) are required; EM alone is misleading.
- The field converges on a **prompt → fine‑tune → agent** ladder for controllable gains.

---

## 1) Proprietary LLM Agents (Upper Bound)
Proprietary systems (e.g., GPT‑4) achieve strong execution accuracy on complex joins and aggregations when paired with agentic reasoning frameworks (Yao et al., 2023; Xi et al., 2025). Ojuri et al. (2025) show GPT‑4 in a ReAct‑style loop can iteratively refine SQL via execution feedback on ClassicModels.

**Why not use them here?**
- closed weights / training data
- opaque internal reasoning
- API costs + rate limits
- limited reproducibility for academic comparison

Surveys (Hong et al., 2025; Zhu et al., 2024) explicitly motivate open‑source alternatives for compliance, research transparency, and on‑prem deployment.

---

## 2) Open‑Source LLMs for NL→SQL
Instruction‑tuned open models (e.g., Llama‑3‑8B‑Instruct) generate syntactically valid SQL with schema context but struggle with **semantic grounding** (joins, aggregates, filters) (Li et al., 2023). This motivates **PEFT / QLoRA** to encode domain‑specific semantics without full fine‑tuning (Ding et al., 2023; Goswami et al., 2024).

Ojuri et al. (2025) report QLoRA on ClassicModels materially improves EX. This project reproduces that trend: zero‑shot = valid but wrong, while QLoRA improves semantic correctness under VRAM constraints.

---

## 3) Prompting vs Model Adaptation
In‑context learning (Brown et al., 2020) enables adaptation without weight updates, but survey evidence shows sensitivity to example selection and schema complexity (Zhu et al., 2024). Few‑shot prompting improves structure, yet remains unstable and weak on compositional joins/aggregates.

Comparative studies (Mosbach et al., 2023) show supervised fine‑tuning outperforms ICL for domain tasks. This supports QLoRA as the **middle path** between prompting and full fine‑tuning.

---

## 4) Execution Feedback & Agentic Reasoning
ReAct (Yao et al., 2023) formalises interleaving reasoning with tool use. In NL→SQL, execution feedback reduces invalid SQL and improves semantic alignment (Zhong et al., 2017; Zhai et al., 2025). Ojuri et al. (2025) attribute part of their gains to execution feedback.

**Key limitation:**
Surveys (Zhu et al., 2024; Rajkumar et al., 2022) observe that execution validity (VA) is **not sufficient** for semantic correctness (EX). Models frequently emit executable but **semantically misaligned** SQL, motivating constrained decoding (Scholak et al., 2021), schema linking (RAT‑SQL), and critic‑based reranking (ValueNet/DIN‑SQL).

---

## 5) Metrics & Evaluation Norms
Early NL→SQL work used string match; modern evaluation uses:
- **Execution Accuracy (EX)** — DB result equivalence (Zhong et al., 2020)
- **Validity Accuracy (VA)** — execution success
- **Test‑Suite (TS)** — robustness under data perturbations

Large‑scale benchmarks (Li et al., 2023; Hong et al., 2025) and Ojuri et al. (2025) emphasise execution‑centric metrics. This project follows that norm: VA/EX are primary; EM is diagnostic.

---

## 6) Resource‑Aware Adaptation
Full fine‑tuning of 7B+ models requires >60GB VRAM; QLoRA reduces this to ~8–12GB, enabling experiments on Colab‑class GPUs (Ding et al., 2023). This aligns with enterprise constraints that require on‑prem or low‑cost inference (Hong et al., 2025).

---

## 7) SOTA Takeaways (for this reproduction)
- Proprietary agents define the upper bound but are not reproducible.
- Open‑source models benefit from PEFT for schema semantics.
- Execution‑guided loops improve stability but not all semantics.
- VA/EX/TS are mandatory for meaningful evaluation.
- VRAM constraints shape methodological design.

**Implication for this project:** a controlled **prompt → QLoRA → execution‑guided agent** ladder is the most defensible, literature‑aligned way to compare mechanisms under realistic hardware constraints.

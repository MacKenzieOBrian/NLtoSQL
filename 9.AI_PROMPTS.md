

## Appendix — Use of AI Tools (Coding & Debugging Assistance)

AI tools were used similarly to StackOverflow, documentation search, and rubber-duck debugging. Usage focused on low-risk implementation details, environment setup, and small utility scaffolding. Research decisions (datasets, metrics, adaptation strategies, evaluation design) were informed by literature and experimental evidence, not AI outputs.

### Typical interactions (representative examples)

**(1) Environment / dependency friction**
> “bitsandbytes import fails with triton… correct pinned versions for Colab T4?”

Result: provided compatible install commands; no design implications.

**(2) CUDA / NumPy ABI mismatches**
> “Colab preloads NumPy. Is `dtype size changed` fixed by reinstalling NumPy before bitsandbytes?”

Used to resolve import errors encountered during QLoRA setup.

**(3) Minor glue utilities**
> “Given a chatty LLM output, extract the first SELECT; no explanations. Tiny regex or split?”

Produced a 4–5 line helper; logic was reviewed and modified before usage.

**(4) Trainer boilerplate**
> “Minimal TRL SFTTrainer + LoRA config for 4-bit Llama-3. Only essential params please.”

Saved time reconstructing the TRL/PEFT training stack from docs.

**(5) Safety scaffolding**
> “SQLAlchemy read-only connection with SELECT-only guard—example creator?”

Used to enforce safety for execution-based evaluation.

**(6) Evaluation stubs**
> “Given pred SQL + gold SQL + engine, pseudocode for VA/EX/EM loop?”

Helped initialize the evaluation loop before integrating the project’s metrics.

**(7) Debugging ambiguous column errors**
> “MySQL ambiguous column in join—quick repro pattern to isolate which table needs qualification?”

Aided in diagnosing join-related failure modes in generated SQL.

**(8) Execution-guided repair sketch**
> “If a query errors, how to feed {sql, error msg, schema} into a single repair prompt? No Thought/Action grammar.”

Used as a rubber-duck for designing the repair prompt structure.

**(9) Result analysis script (error taxonomy)**
> “Extend the existing `scripts/analyze_results.py` to categorize EX failures (projection mismatch, intent mismatch, join mismatch, literal mismatch) and print a few examples.”

Outcome: a simple, deterministic post‑hoc analysis script used to summarize failure modes in the ReAct results JSON. The taxonomy and thresholds were implemented transparently and reviewed before inclusion.

### Usage characterization

- **Not used for:** model selection, dataset construction, evaluation methodology, interpretation, or conclusions.
- **Used for:** environment helpers, boilerplate, glue code, and debugging friction.
- **Net effect:** reduced implementation overhead; did not influence research direction.

### Rationale for disclosure

Modern NL→SQL pipelines involve non-trivial plumbing (DB connectors, schema formatting, post-processing, evaluation). AI tools served as an auxiliary coding assistant, analogous to StackOverflow or IDE code suggestions, while research decisions were guided by literature, experiments, and evaluation results.

---

# Results Summary

What we’ve measured so far: prompt baselines, QLoRA adapters, and what’s next.

---

# Results Summary

What we’ve measured so far: prompt baselines, QLoRA adapters, and what’s next.

## Metrics

VA (syntactic validity), EX (execution accuracy), EM (string match), TS (planned test-suite). Execution-based metrics (VA/EX/TS) matter most for NL→SQL.

## Baselines

**Zero-shot (k=0):** schema in prompt, deterministic decoding. VA ~0.80–0.82; EX ~0.00; EM ~0.00. Take: syntax alone doesn’t give correct answers.

---

**Few-shot (k=3):** same model, plus exemplars. VA ~0.86; EX ~0.25; EM low. Take: exemplars help schema grounding; matches usual few-shot NL→SQL behaviour.

---

## 3. QLoRA Fine-Tuning

Base model: `Llama-3-8B-Instruct`  
PEFT: 4-bit QLoRA (see run configs below)

Two evaluation modes per run: k=0 (adapter-only) and k=3 (adapter + exemplars).

**QLoRA SFT (r=32, ~3 epochs, 4-bit):** k=0 VA 0.865 / EX 0.065 / EM 0.000; k=3 VA 0.870 / EX 0.380 / EM 0.310 (JSONs saved under `results/qlora/`). k=3 beats the prompt-only few-shot baseline; k=0 is still low, so TS + agentic refinement should help.

Take:
- k=3 EX beats the prompt-only few-shot baseline (~0.325→0.380).
- k=0 EX is still low; TS + agentic refinement should help semantic accuracy without exemplars.

---

## 4. Agentic (ReAct) Experiments — status

- Loop design: 3-step Thought/Action/Observation with a single tool (safe SELECT executor), schema-grounded prompt, ORDER/LIMIT and projection clamps, execution-guided retry (unknown-column, repair prompt), and sampled multi-candidates (top-p/temperature).
- Outcomes so far: VA improves vs early attempts (junk/non-SELECT filtered; retries fix some missing joins), but EX remains low on the full set; best small-slice runs reach VA>0 with modest EX. Full 200-item EX still trails few-shot/QLoRA.
- Failure modes: hallucinated columns/tables, aggregation/GROUP BY drift, occasional over-trimming of projections, and limited diversity when candidates converge.
- Next steps (planned, not yet run): add simple rerank by column-overlap with NLQ, broaden retries to ambiguous-column/unknown-table errors, and consider a grammar/allowlist clamp if time permits.
- Dissertation positioning: treat ReAct as exploratory; report traces and limitations, with baseline/QLoRA as primary quantitative results.

---

## 5. Learning journey (runs tied to commits)
- 2025-12-14 (baseline hygiene, commit ref: see logbook 2025-12-14): Prompt structuring + minimal postprocess → VA/EX jump on small slice; established deterministic eval protocol.
- 2026-01-12 (QLoRA r=32, 3 epochs, commit ref: logbook 2026-01-12): k=0 VA 0.865 / EX 0.065; k=3 VA 0.875 / EX 0.380. Takeaway: adapters + exemplars beat prompt-only k=3.
- 2026-01-24–31 (ReAct iterations, commits through `a7afd60`): added clamps, select-only filtering, execution-guided retry, sampling, and one-shot repair. Result: VA improved; EX still below few-shot/QLoRA on full set. Position as exploratory and report traces.

Use this section to anchor dissertation claims to dated logbook entries and commits.

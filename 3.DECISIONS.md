# Decisions (traceable and comparative)

This section records key methodological decisions, their motivations, rejected alternatives, and traceability. The goal is to make the experimental framing auditable, theoretically justified, and reproducible. These decisions complement the Methodology and Evaluation chapters and reflect established practice in NL→SQL research, where ablation-style comparisons and disclosure of evaluation conditions are standard.

## 1. Research Design

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| 200 supervised examples | realistic small-scale SFT | >5k examples → infeasible on Colab | `data/train/` |
| SQL validated before training | enforces VA=True → avoids garbage learning | train on synthetic → unknown distribution drift | `04_build_training_set.ipynb` |

**Justification:** Comparative framing aligns with contemporary NL→SQL evaluation practice, where prompting, fine-tuning, and agentic refinement are treated as separable adaptation mechanisms for structured tasks. This avoids single-configuration claims and enables attribution of performance gains to specific mechanisms.

## 2. Dataset & Evaluation

Leakage and schema omission are known threats to validity; we validate datasets against the live ClassicModels schema and de-duplicate NLQs across train/test.

This follows evaluation discipline used in Spider, BIRD, and Ojuri et al., where schema grounding and leakage control are treated as basic validity requirements for NL→SQL benchmarks.

## 3. Prompt Hygiene

Controls: fixed system message, ordered schema summary, k∈{0,3} exemplars, deterministic decoding, guarded first-SELECT extraction.

**Interpretation:** Prompt hygiene isolates intrinsic model SQL competence from confounding prompt engineering effects. Few-shot prompting improves structural patterns (syntax, projection) but cannot reliably recover semantic mappings (joins, aggregates) without weight adaptation.

## 4. Adaptation Ladder (k=0 vs k=3; PEFT; Agent)

| Stage | Mechanism | Expected gain |
|---|---|---|
| Prompt-only | in-context examples | syntax/schema stability |
| QLoRA | adapter fine-tuning | semantic mappings (joins/aggregates) |
| Execution-guided agent | candidate execution + rerank/repair | semantic robustness, self-correction |

**Methodological Value:** The ladder operationalises an ablation used in applied NLP: it helps infer where semantic competence is acquired (weights vs context vs feedback). Similar staged comparisons appear in Ojuri et al. (prompt → adapter → agent) and broader LLM reasoning studies that treat multi-step refinement as a distinct capability layer.

## 5. Metrics

- VA (validity): executable SQL
- EX (execution accuracy): result-set match vs gold
- EM (exact match): normalized string match
- TS (planned): suite-based robustness

**Justification:** Execution-based metrics (VA/EX/TS) are preferred because SQL has high surface variability. String match alone underestimates correctness and misclassifies alternate-but-valid programs. Test-suite robustness (TS) measures semantic stability under data perturbations.

## 6. Prompt/Adapter/Agent Choices

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| 8B scale + 4-bit NF4 | fits Colab VRAM; QLoRA-compatible | full FP16 7B/13B → 60–80GB | `CONFIG.md` |
| Few-shot + deterministic decode | reproducible, leakage-safe | aggressive sampling → instability | notebooks/02_baseline_prompting_eval.ipynb |
| agent_utils guardrails | SELECT-only, semantic rerank, error-classified repair | full ReAct grammar (out of scope for 8B) | `nl2sql/agent_utils.py` |

### Design Decision: ReAct Candidate Generation → Semantic Reranking
- **Problem:** “Fewest columns” ranking raised VA but often chose semantically wrong-yet-executable SQL (EX remained low).
- **Change:** Introduced intent-aware semantic_score penalised by projection size (semantic_score − λ·num_cols) to prefer candidates aligned with NLQ semantics.
- **Evidence:** Literature on agentic NL→SQL (Ojuri et al., 2025; Yao et al., 2023) recommends intent-aligned reranking over pure minimality.
- **Outcome:** Higher likelihood of selecting correct aggregates/joins while retaining execution validity.

### Design Decision: ORDER/LIMIT Clamping
- **Problem:** Hallucinated ORDER BY and wide projections when NLQ did not request ranking.
- **Change:** sqlparse-based clamps remove ORDER BY unless ranking cues exist; trim projections for simple list/which NLQs.
- **Outcome:** Aligns SQL outputs with user intent; reduces EX loss from projection/order drift.

### Design Decision: Error-Aware SQL Repair
- **Problem:** Common failures from unknown columns/joins/grouping; ReAct loop lacked self-correction.
- **Change:** One-shot repair prompt takes failing SQL + DB error + schema to propose corrected SELECT.
- **Outcome:** Lowers execution failure without weight updates; consistent with execution-guided decoding practice.

### Design Decision: Baseline Fallback
- **Problem:** Agent could underperform the deterministic few-shot baseline.
- **Change:** Added vanilla_candidate fallback so the agent never scores below non-agentic prompting.
- **Outcome:** Ensures lower-bound parity while keeping agentic improvements.

### Design Decision: Candidate Diversity & History Formatting
- **Change:** Switched generation to mild sampling (top-p 0.9) for diversity; reformatted history with readable newlines.
- **Outcome:** More diverse candidate pool; clearer traces for debugging and model conditioning.

### Decision X — Introduce execution-guided, agentic refinement pipeline
- **Problem:** Prompting-only pipeline showed high VA but insufficient EX.
- **Options considered:** (A) full fine-tuning, (B) QLoRA adapters, (C) agentic refinement without weight updates.
- **Chosen:** (C) to add semantic correctness without increasing compute or model size.
- **Justification:** Echoes execution-guided NL→SQL (Zhong et al., 2017), constrained decoding (Scholak et al., 2021), and agentic refinement (Yao et al., 2023).
- **Expected impact:** Higher EX; fewer hallucinated ORDER BY / projections.
- **Evaluation:** Before/after EX comparisons (see Evaluation Framework).

## 7. Safety, Security & Data Access

Read-only execution, DDL/DML blocked, role-constrained DB access via Cloud SQL connector and SQLAlchemy creator.

**Relevance:** Executing model-generated SQL introduces safety concerns absent in typical NLP tasks. Read-only execution and DB roles are standard governance in enterprise deployments and agentic systems involving tool calling.

## 8. Reproducibility & Integrity

Pinned dependencies, deterministic decoding, declared env vars, and git-tracked configs.

**Rationale:** Reproducibility is non-trivial in text-to-SQL due to dependency drift (CUDA/BNB/Transformers), schema coupling, and execution variance. Pinned deps, deterministic decode, and declarative config align with reproducibility recommendations in recent LLM systems work.

## 9. PEFT (QLoRA) Feasibility

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| QLoRA adapters (r=32, α=64) | domain adaptation under ≤10GB VRAM | full FT or larger models | `CONFIG.md`, `05_qlora_train_eval.ipynb` |

**Justification:** PEFT/QLoRA enable domain adaptation under strict VRAM budgets; literature reports similar feasibility windows (8–12GB VRAM) for 7B–8B models with 4-bit quantization and LoRA heads.

## 10. Architecture Notes (for traceability)

- Components: schema access + prompting + strategy (prompt/QLoRA/agent) + safety/postprocess + execution/eval. Library code in `nl2sql/`; notebooks orchestrate per strategy.
- Flow: NLQ → schema text → strategy → SQL candidate → safety filter → DB execution → metrics (VA/EX/EM; TS planned).

**Design Considerations:** Architecture balances: (1) schema fidelity vs prompt length, (2) zero-shot vs few-shot conditioning, (3) fine-tuning vs agentic correction, and (4) execution robustness vs evaluation determinism. The execution-guided agent implements minimal semantic self-correction without full ReAct Thought/Action grammars; execution feedback improves VA but EX gains remain limited without multi-step reasoning traces.

## 11. Compute Constraints & Deployment Reality

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| Colab/T4-scale GPU | matches SME/teaching constraint, reproducible | dedicated clusters | `CONFIG.md` |
| OSS-only stack | transparency/compliance | proprietary APIs (GPT-4, etc.) | repo-wide |

**Motivation:** These constraints reflect realistic SME/regulated settings where proprietary APIs may be disallowed and GPU clusters unavailable—matching motivations in Ojuri et al. and recent open-source NL→SQL studies.

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f2b652",
   "metadata": {},
   "source": [
    "# Agentic Evaluation (ReAct-style)\n",
    "\n",
    "This notebook adds a minimal ReAct-style loop for NL→SQL. It reuses the same benchmark (`data/classicmodels_test_200.json`) and metrics (VA/EX/EM; TS planned) to measure gains over prompt-only and QLoRA runs.\n",
    "\n",
    "Plan (step-by-step):\n",
    "1) Clone repo (Colab) + install deps\n",
    "2) Environment + DB connection\n",
    "3) Load schema summary + test set\n",
    "4) Load model (base or QLoRA adapters)\n",
    "5) Define ReAct prompt + loop (Thought → Action → Observation → Refinement)\n",
    "6) Run evaluation (VA/EX/EM) and save to `results/agent/…`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c44350",
   "metadata": {},
   "source": [
    "Docs I leaned on: HF Transformers quantization (https://huggingface.co/docs/transformers/main_classes/quantization), PEFT/TRL (https://huggingface.co/docs/peft/, https://huggingface.co/docs/trl/), Cloud SQL connector + SQLAlchemy creator (https://cloud.google.com/sql/docs/mysql/connect-run, https://docs.sqlalchemy.org/en/20/core/engines.html#custom-dbapi-connect), ReAct (https://arxiv.org/abs/2210.03629)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faade3c2",
   "metadata": {},
   "source": [
    "## Setup (run first, then restart)\n",
    "In a fresh Colab GPU runtime, run this one cell to clean preinstalls and pin the CUDA 12.1 torch/bitsandbytes/triton stack. When it finishes, **Runtime → Restart runtime**, then run the rest of the notebook from the clone cell onward without more restarts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79779a6",
   "metadata": {},
   "source": [
    "**Docs (setup):** HF Transformers quantization + BitsAndBytes (4-bit) https://huggingface.co/docs/transformers/main_classes/quantization, bnb https://github.com/TimDettmers/bitsandbytes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef67421",
   "metadata": {},
   "source": [
    "Top-down: Freeze the runtime so model outputs + metrics are reproducible.\n",
    "\n",
    "Why this cell exists:\n",
    "- Small version changes (torch/transformers/bitsandbytes) can change generation and postprocessing, which shows up as VA/EX/TS drift.\n",
    "\n",
    "Code pointers:\n",
    "- `requirements.txt` (pinned deps used by notebook + scripts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b144d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "set -e\n",
    "export PIP_DEFAULT_TIMEOUT=120\n",
    "\n",
    "# Clean conflicting preinstalls\n",
    "pip uninstall -y torch torchvision torchaudio bitsandbytes triton transformers accelerate peft trl datasets numpy pandas fsspec requests google-auth || true\n",
    "\n",
    "# Base deps\n",
    "pip install -q --no-cache-dir --force-reinstall   numpy==1.26.4 pandas==2.2.1 fsspec==2024.5.0 requests==2.31.0 google-auth==2.43.0\n",
    "\n",
    "# Torch + CUDA 12.1\n",
    "pip install -q --no-cache-dir --force-reinstall   torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121   --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# bitsandbytes + triton + HF stack\n",
    "pip install -q --no-cache-dir --force-reinstall   bitsandbytes==0.43.3 triton==2.3.1   transformers==4.44.2 accelerate==0.33.0 peft==0.17.0 trl==0.9.6 datasets==2.20.0\n",
    "\n",
    "echo \"Setup complete. Restart runtime once, then run the rest of the notebook top-to-bottom.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b0b11",
   "metadata": {},
   "source": [
    "Model load: HF 4-bit NF4 + BitsAndBytes; deterministic decoding. If adapters exist, we load them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b6008",
   "metadata": {},
   "source": [
    "Top-down: Bootstrap the repo inside the notebook runtime.\n",
    "\n",
    "Why this cell exists:\n",
    "- Keeps the notebook focused on experiments; reusable logic lives in `nl2sql/` and can be reused by scripts.\n",
    "\n",
    "Code pointers:\n",
    "- `scripts/run_full_pipeline.py` (CLI version of the flow)\n",
    "- `nl2sql/` (modules imported throughout the notebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47936fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Clone repo (Colab) + install deps\n",
    "import os\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not os.path.exists('/content/NLtoSQL'):\n",
    "        !git clone https://github.com/MacKenzieOBrian/NLtoSQL.git /content/NLtoSQL\n",
    "    %cd /content/NLtoSQL\n",
    "    !pip -q install -r requirements.txt\n",
    "    import torch, transformers, accelerate, peft\n",
    "    print('torch', torch.__version__, 'cuda', torch.cuda.is_available())\n",
    "else:\n",
    "    print('Not in Colab; using existing workspace')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24721f1",
   "metadata": {},
   "source": [
    "Prompt/eval: build prompts (system+schema+k exemplars), generate SQL, postprocess, and compute VA/EX/EM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b7e43",
   "metadata": {},
   "source": [
    "**Ref:** Colab clone/install pattern; keeps notebooks thin and code in `nl2sql/`. Hugging Face/Colab standard workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20974344",
   "metadata": {},
   "source": [
    "### Reference notes (what this builds on)\n",
    "- DB access: Cloud SQL Connector + SQLAlchemy creator (GCP docs: https://cloud.google.com/sql/docs/mysql/connect-run) for secure pooled ClassicModels access.\n",
    "- Schema/prompting: uses repo helpers (`nl2sql.schema`, `prompting`) aligned with schema-grounded NL→SQL prompting (survey: https://arxiv.org/abs/2410.06011).\n",
    "- Model load: HF Transformers 4-bit NF4 with BitsAndBytes (quantization docs: https://huggingface.co/docs/transformers/main_classes/quantization), same pattern as QLoRA.\n",
    "- Agent loop: ReAct-style Thought→Action→Observation→Refinement, inspired by Yao et al. 2023 (https://arxiv.org/abs/2210.03629) and agentic NL→SQL in Ojuri et al. 2025.\n",
    "- Eval: repo harness (`nl2sql.eval`, `QueryRunner`) for VA/EX/EM; TS planned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f75fce",
   "metadata": {},
   "source": [
    "## Optional: use gcloud ADC (without a key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ff993",
   "metadata": {},
   "source": [
    "**Ref:** GCP ADC flow (docs: https://cloud.google.com/docs/authentication/provide-credentials-adc). Optional fallback if no service account JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58d00e",
   "metadata": {},
   "source": [
    "Top-down: Optional ADC auth (no key file) for Cloud SQL access.\n",
    "\n",
    "Why this cell exists:\n",
    "- Lets the notebook use Application Default Credentials instead of embedding secrets.\n",
    "\n",
    "Code pointers:\n",
    "- `nl2sql/db.py` (`create_engine_with_connector`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this only if you prefer gcloud-based ADC (no JSON key)\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    %pip install -q --upgrade google-auth google-auth-oauthlib\n",
    "    !gcloud auth application-default login\n",
    "else:\n",
    "    print(\"Not in Colab; skip gcloud auth.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21847187",
   "metadata": {},
   "source": [
    "**Ref:** Pinned CUDA12.1 torch/bitsandbytes/triton stack per HF/BnB guidance for 4-bit loads on Colab GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5528bc3a",
   "metadata": {},
   "source": [
    "**Ref:** Cloud SQL Connector + SQLAlchemy creator (GCP MySQL docs: https://cloud.google.com/sql/docs/mysql/connect-run) for secure ClassicModels access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e745f2",
   "metadata": {},
   "source": [
    "**Docs (auth/DB):** Cloud SQL connector pattern https://cloud.google.com/sql/docs/mysql/connect-run; SQLAlchemy creator hook https://docs.sqlalchemy.org/en/20/core/engines.html#custom-dbapi-connect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada87478",
   "metadata": {},
   "source": [
    "Top-down: Create the base DB Engine and a SELECT-only QueryRunner.\n",
    "\n",
    "Why this cell exists:\n",
    "- `QueryRunner.run(sql)` is the agent's \"Act\" step and the source of VA (executability).\n",
    "- The safety guard prevents accidental DDL/DML when executing model-generated SQL.\n",
    "\n",
    "Code pointers:\n",
    "- `nl2sql/db.py` (`create_engine_with_connector`, `safe_connection`)\n",
    "- `nl2sql/query_runner.py` (`QueryRunner.run`, `_safety_check`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Environment + DB\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "from nl2sql.db import create_engine_with_connector, safe_connection\n",
    "\n",
    "# Expected env vars (set these in a Colab cell):\n",
    "# INSTANCE_CONNECTION_NAME, DB_USER, DB_PASS, DB_NAME\n",
    "INSTANCE_CONNECTION_NAME = os.getenv(\"INSTANCE_CONNECTION_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\") or \"classicmodels\"\n",
    "\n",
    "if not INSTANCE_CONNECTION_NAME:\n",
    "    INSTANCE_CONNECTION_NAME = input(\"Enter INSTANCE_CONNECTION_NAME: \").strip()\n",
    "if not DB_USER:\n",
    "    DB_USER = input(\"Enter DB_USER: \").strip()\n",
    "if not DB_PASS:\n",
    "    DB_PASS = getpass(\"Enter DB_PASS: \")\n",
    "\n",
    "# Canonical engine builder (shared with scripts + other notebooks).\n",
    "# Uses Cloud SQL Connector under the hood and ADC for credentials.\n",
    "engine, connector = create_engine_with_connector(\n",
    "    instance_connection_name=INSTANCE_CONNECTION_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASS,\n",
    "    db_name=DB_NAME,\n",
    ")\n",
    "\n",
    "with safe_connection(engine) as conn:\n",
    "    conn.execute(text(\"SELECT 1\"))\n",
    "print(\"DB connection OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9a6fb",
   "metadata": {},
   "source": [
    "Top-down: Engine factory for Test-Suite (TS) databases.\n",
    "\n",
    "Why this cell exists:\n",
    "- TS runs the same (gold, pred) SQL across multiple perturbed DB replicas (classicmodels_ts_*).\n",
    "- A factory keeps base DB evaluation separate from TS evaluation and makes TS reproducible.\n",
    "\n",
    "Code pointers:\n",
    "- This notebook cell: `make_engine(db_name)`\n",
    "- `nl2sql/eval.py` (`test_suite_accuracy_for_item` uses `make_engine_fn`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b) Engine factory for TS (multiple DB names)\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "\n",
    "def make_engine(db_name: str) -> Engine:\n",
    "    \"\"\"Create a new engine bound to a specific TS replica DB name.\n",
    "\n",
    "    TS (test-suite accuracy) executes the same (gold, pred) SQL across multiple\n",
    "    replica databases (classicmodels_ts_XX). We keep separate engines so each\n",
    "    replica is evaluated independently.\n",
    "    \"\"\"\n",
    "\n",
    "    def getconn_for_db():\n",
    "        return connector.connect(\n",
    "            INSTANCE_CONNECTION_NAME,\n",
    "            \"pymysql\",\n",
    "            user=DB_USER,\n",
    "            password=DB_PASS,\n",
    "            db=db_name,\n",
    "        )\n",
    "\n",
    "    return sqlalchemy.create_engine(\"mysql+pymysql://\", creator=getconn_for_db, future=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a583e16",
   "metadata": {},
   "source": [
    "**Ref:** Schema helper in `nl2sql.schema`; schema-grounded prompting per NL→SQL survey (https://arxiv.org/abs/2410.06011)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7620ff8",
   "metadata": {},
   "source": [
    "**Docs (schema prompts):** NL→SQL schema-grounded prompting survey https://arxiv.org/abs/2410.06011; Spider-style listings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6cfbfc",
   "metadata": {},
   "source": [
    "Top-down: Build schema text and load a small debug slice of the test set.\n",
    "\n",
    "Why this cell exists:\n",
    "- Schema summaries reduce hallucinated tables/columns during prompting.\n",
    "- A small slice lets you iterate on postprocess + ReAct logic quickly before full evaluation.\n",
    "\n",
    "Code pointers:\n",
    "- `nl2sql/schema.py` (`build_schema_summary`)\n",
    "- `data/classicmodels_test_200.json` (NLQ + gold SQL items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load schema summary + test set (small slice for now)\n",
    "import json\n",
    "from nl2sql.schema import build_schema_summary\n",
    "\n",
    "SCHEMA_SUMMARY = build_schema_summary(engine, db_name=DB_NAME)\n",
    "\n",
    "test_path = Path(\"data/classicmodels_test_200.json\")\n",
    "full_set = json.loads(test_path.read_text(encoding=\"utf-8\"))\n",
    "# default to a small slice while debugging\n",
    "test_set = full_set[:5]\n",
    "print(\"Demo items:\", len(test_set))\n",
    "# For full run, switch to: test_set = full_set; print(\"Test items:\", len(test_set))\n",
    "\n",
    "TABLES = {line.split('(', 1)[0].strip() for line in SCHEMA_SUMMARY.splitlines() if '(' in line}\n",
    "TABLES_LOWER = {t.lower(): t for t in TABLES}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700fe5c2",
   "metadata": {},
   "source": [
    "**Ref:** HF Transformers 4-bit NF4 + BitsAndBytes (quantization docs: https://huggingface.co/docs/transformers/main_classes/quantization); adapters via PEFT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92a2b8",
   "metadata": {},
   "source": [
    "**Docs (model load):** HF 4-bit NF4 quantization https://huggingface.co/docs/transformers/main_classes/quantization; PEFT/QLoRA https://huggingface.co/docs/peft/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b3611c",
   "metadata": {},
   "source": [
    "Top-down: Load the base model (4-bit) and optional PEFT adapters.\n",
    "\n",
    "Why this cell exists:\n",
    "- The baseline path prefers deterministic decoding so VA/EX/TS are comparable across reruns.\n",
    "- The ReAct helper layer may enable sampling to get diverse candidates, but the weights remain fixed.\n",
    "\n",
    "Code pointers:\n",
    "- `nl2sql/llm.py` (`generate_sql_from_messages`, `extract_first_select`)\n",
    "- `scripts/run_full_pipeline.py` (model + adapter config mirror)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f38fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Load model (base or QLoRA adapters)\n",
    "import os\n",
    "from getpass import getpass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "ADAPTER_PATH = os.getenv(\"ADAPTER_PATH\") or \"results/adapters/qlora_classicmodels\"  # set to None to use base model\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if not HF_TOKEN:\n",
    "    HF_TOKEN = getpass(\"Enter HF_TOKEN (https://huggingface.co/settings/tokens): \").strip()\n",
    "\n",
    "cc_major, cc_minor = torch.cuda.get_device_capability(0) if torch.cuda.is_available() else (0, 0)\n",
    "use_bf16 = cc_major >= 8\n",
    "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"Using bf16:\", use_bf16)\n",
    "print(\"Adapter path:\", ADAPTER_PATH)\n",
    "\n",
    "# Tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# Quantized base model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map={\"\": 0} if torch.cuda.is_available() else None,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "base_model.generation_config.do_sample = False\n",
    "base_model.generation_config.temperature = 1.0\n",
    "base_model.generation_config.top_p = 1.0\n",
    "\n",
    "# Load adapters if present locally; otherwise use base model\n",
    "adapter_dir = Path(ADAPTER_PATH) if ADAPTER_PATH else None\n",
    "if adapter_dir and adapter_dir.exists():\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_dir, token=HF_TOKEN)\n",
    "    print(\"Loaded adapters from\", adapter_dir)\n",
    "else:\n",
    "    print(\"Adapter path missing; using base model only. Set ADAPTER_PATH to your local adapter folder or upload it to Colab.\")\n",
    "    model = base_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afb59ed",
   "metadata": {},
   "source": [
    "## Optional adapter sanity check (run before ReAct)\n",
    "Quick check to see if the loaded model/adapters produce valid SQL on a tiny slice. Uses the prompt harness (k=0/k=3) and executes the SQL to report VA/EX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db24b08d",
   "metadata": {},
   "source": [
    "**Docs (prompt/eval):** ICL patterns https://arxiv.org/abs/2005.14165; execution-based metrics (VA/EX) https://aclanthology.org/2020.emnlp-main.29/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7065790e",
   "metadata": {},
   "source": [
    "Top-down: Quick end-to-end check: prompt -> generate -> postprocess -> execute -> compare.\n",
    "\n",
    "Why this cell exists:\n",
    "- Catches common early failure modes (prompt echo, non-SELECT text, invalid SQL) before running the full agent loop.\n",
    "\n",
    "Code pointers:\n",
    "- `nl2sql/prompting.py` (`make_few_shot_messages`)\n",
    "- `nl2sql/postprocess.py` (`guarded_postprocess`)\n",
    "- `nl2sql/query_runner.py` (`QueryRunner.run` provides VA + error messages)\n",
    "- `nl2sql/eval.py` (`execution_accuracy` computes EX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fbae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nl2sql.prompting import make_few_shot_messages\n",
    "from nl2sql.llm import extract_first_select\n",
    "from nl2sql.postprocess import guarded_postprocess\n",
    "from nl2sql.query_runner import QueryRunner\n",
    "from nl2sql.eval import execution_accuracy\n",
    "\n",
    "runner_check = QueryRunner(engine)\n",
    "# reuse existing test_set (default small slice); pick 3 exemplars\n",
    "exemplars = test_set[:3]\n",
    "\n",
    "def run_quick_check(k: int = 0, limit: int = 3):\n",
    "    print(f\"Quick check k={k}\")\n",
    "    for sample in test_set[:limit]:\n",
    "        shots = exemplars if k > 0 else []\n",
    "        msgs = make_few_shot_messages(\n",
    "            schema=SCHEMA_SUMMARY,\n",
    "            exemplars=shots,\n",
    "            nlq=sample['nlq'],\n",
    "        )\n",
    "        prompt_preview = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tok(prompt_preview, return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "\n",
    "        # strip the prompt before decoding the generation\n",
    "        gen_ids = out[0][inputs.input_ids.shape[-1]:]\n",
    "        text = tok.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "        raw_sql = extract_first_select(text) or text\n",
    "        sql = guarded_postprocess(raw_sql, sample['nlq'])\n",
    "\n",
    "        meta = runner_check.run(sql, capture_df=False)\n",
    "        va = meta.success\n",
    "        ex_ok, _, _ = execution_accuracy(engine=engine, pred_sql=sql, gold_sql=sample['sql'])\n",
    "        print(f\"Q: {sample['nlq']}\\nSQL: {sql}\\nVA: {va} EX: {ex_ok}\\n\")\n",
    "\n",
    "run_quick_check(k=0)\n",
    "run_quick_check(k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d3d737",
   "metadata": {},
   "source": [
    "**Ref:** ReAct pattern (Yao et al. 2023: https://arxiv.org/abs/2210.03629) adapted for NL→SQL with `QueryRunner` as the Act step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2d11f",
   "metadata": {},
   "source": [
    "**Docs (ReAct):** ReAct loop (Yao et al. 2023) https://arxiv.org/abs/2210.03629; safe Act via SELECT-only executor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21452dc1",
   "metadata": {},
   "source": [
    "Top-down: Import shared heuristics used by the agent.\n",
    "\n",
    "Why this cell exists:\n",
    "- Separates \"policy\" (intent checks, semantic scoring, schema subset) from notebook orchestration, so it can be reviewed and defended.\n",
    "\n",
    "Code pointers:\n",
    "- `nl2sql/agent_utils.py` (`build_schema_subset`, `intent_constraints`, `semantic_score`, `count_select_columns`, `enforce_projection_contract`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ff33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper imports (optional; used for interactive inspection)\n",
    "# Main agent loop is in `nl2sql/agent.py`.\n",
    "from nl2sql.agent_utils import intent_constraints, semantic_score, count_select_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0973b",
   "metadata": {},
   "source": [
    "### Agent status (for dissertation)\n",
    "Current loop = execution-guided reranker: sampled candidates, SELECT-only filter, semantic rerank, error-classified repair, deterministic few-shot fallback.\n",
    "Not yet full ReAct: we don’t enforce structured `Thought / Action: SCHEMA_LOOKUP[...] / Action: EXEC_SQL[...] / Observation: ... / FINISH[...]`, so the model isn’t forced to read and react to its own Observations.\n",
    "Planned upgrade (if time permits): add an explicit tool grammar and feed Observations back into the prompt so the model can revise after execution errors (Yao et al., 2023).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13e02a",
   "metadata": {},
   "source": [
    "**Ref:** Repo eval (`nl2sql.eval`) for VA/EX/EM; execution-based metrics align with Ojuri et al. 2025 and EMNLP’20 TS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52889a3",
   "metadata": {},
   "source": [
    "**Docs (prompt/eval):** ICL patterns https://arxiv.org/abs/2005.14165; execution-based metrics (VA/EX) https://aclanthology.org/2020.emnlp-main.29/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9001e3",
   "metadata": {},
   "source": [
    "## ReAct execution-guided pipeline (best version so far)\n",
    "These cells mirror the committed helper layer (`nl2sql/agent_utils.py`) and set up the current execution-guided reranker + evaluation harness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0685cc3d",
   "metadata": {},
   "source": [
    "## Reference Map (Code ↔ Literature)\n",
    "- **Execution guidance & repair:** execution feedback loop + repair step (ExCoT [2], ReAct [16], TS/semantic eval [18]).\n",
    "- **Constrained decoding/output hygiene:** stop‑on‑semicolon + clean_candidate (PICARD [13], surveys [8], [9]).\n",
    "- **Projection contract:** output‑shape control to reduce EX projection drift (survey [8], BigBench Text‑to‑SQL [1]).\n",
    "- **Intent constraints:** query‑type guardrails (ExCoT [2], survey [8], benchmark eval [20]).\n",
    "- **Schema‑subset prompting:** lightweight schema linking (RESDSQL [17], surveys [8], [9]).\n",
    "- **TS evaluation:** semantic equivalence across perturbed DBs (Zhong et al. [18]).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730eda73",
   "metadata": {},
   "source": [
    "Top-down: Reload schema + full test set and instantiate the runner for real evaluation.\n",
    "\n",
    "Why this cell exists:\n",
    "- The earlier debug slice is for iteration; this cell switches to the full evaluation workload.\n",
    "\n",
    "Code pointers:\n",
    "- `nl2sql/schema.py` (`build_schema_summary`)\n",
    "- `nl2sql/query_runner.py` (`QueryRunner`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe76f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Schema summary + test set + QueryRunner\n",
    "import json\n",
    "from pathlib import Path\n",
    "from nl2sql.schema import build_schema_summary\n",
    "from nl2sql.query_runner import QueryRunner\n",
    "\n",
    "DB_NAME = os.getenv(\"DB_NAME\") or \"classicmodels\"\n",
    "\n",
    "SCHEMA_SUMMARY = build_schema_summary(engine, db_name=DB_NAME)\n",
    "# Schema summary is used in prompts to ground column/table choices.\n",
    "test_path = Path(\"data/classicmodels_test_200.json\")\n",
    "full_set = json.loads(test_path.read_text(encoding=\"utf-8\"))\n",
    "test_set = full_set  # change to full_set[:20] when debugging\n",
    "\n",
    "print(\"Loaded test set size:\", len(test_set))\n",
    "runner = QueryRunner(engine)  # QueryRunner enforces SELECT-only execution and records errors for VA/EX.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f4b1f",
   "metadata": {},
   "source": [
    "Top-down: Defensive re-import before defining the helper/control layer.\n",
    "\n",
    "Why this cell exists:\n",
    "- Notebook runs are not always linear; this keeps the following cells self-contained if you re-run from here.\n",
    "\n",
    "Code pointers:\n",
    "- `nl2sql/llm.py`, `nl2sql/postprocess.py`, `nl2sql/agent_utils.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b07b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Agent utilities (used inside `nl2sql/agent.py`)\n",
    "from nl2sql.agent_utils import intent_constraints, semantic_score, count_select_columns, vanilla_candidate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952b8ec",
   "metadata": {},
   "source": [
    "## 6. Agent Implementation (Module-Based, Explainable ReAct Loop)\n",
    "\n",
    "This notebook is designed to be **explainable in a viva**: the core agent logic lives in importable Python modules so there is a single source of truth shared by:\n",
    "- the notebook (interactive debugging + trace inspection)\n",
    "- CLI scripts (reproducible runs)\n",
    "\n",
    "The canonical agent is `nl2sql/agent.py` (`ReactSqlAgent`). It implements a bounded ReAct-style loop:\n",
    "- **Prompt**: build a ReAct prompt (history + last observation) and an optional tabular prompt.\n",
    "- **Generate**: sample a small number of candidate SQL strings (bounded by config).\n",
    "- **Clean + postprocess (deterministic)**: keep one SELECT, strip common prompt echo, and apply lightweight guardrails (`guarded_postprocess`, optional projection contract).\n",
    "- **Execution gate (Act)**: execute against the DB via `QueryRunner.run` (SELECT-only guard).\n",
    "- **Intent gate**: reject executable-but-wrong-type queries (`intent_constraints`).\n",
    "- **Score**: use a simple, auditable reranker (`semantic_score` minus a column-width penalty).\n",
    "- **Repair (optional, bounded)**: on execution errors, ask the model to fix SQL using the DB error message (still re-gated).\n",
    "- **Fallback**: if all steps fail, return a deterministic baseline candidate (`vanilla_candidate`).\n",
    "\n",
    "The key academic point is that **no weights are changed** in this stage: improvements come from deterministic constraints and bounded control logic around execution feedback (ReAct / execution-guided ideas).\n",
    "\n",
    "Code pointers:\n",
    "- Agent loop: `nl2sql/agent.py` (`ReactSqlAgent.react_sql`, `evaluate_candidate`, `repair_sql`)\n",
    "- Deterministic postprocess: `nl2sql/postprocess.py` (`guarded_postprocess`)\n",
    "- Gates + scoring: `nl2sql/agent_utils.py` (`clean_candidate_with_reason`, `intent_constraints`, `semantic_score`, `count_select_columns`, `enforce_projection_contract`)\n",
    "- Execution tool: `nl2sql/query_runner.py` (`QueryRunner.run`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d555900",
   "metadata": {},
   "source": [
    "Top-down: configure the agent in one place (`ReactConfig`) and import the canonical implementation.\n",
    "\n",
    "Why this cell exists:\n",
    "- The viva questions are usually about **bounds** (cost/latency) and **auditability** (why did the agent accept/reject something?).\n",
    "- Keeping `ReactConfig(...)` explicit makes it easy to defend: max steps, candidates per step, sampling settings, and whether repair is enabled.\n",
    "- The heavy logic is not hidden in the notebook: it is imported from `nl2sql/agent.py` so scripts + notebook stay consistent.\n",
    "\n",
    "Code pointers:\n",
    "- `nl2sql/agent.py` (`ReactConfig`, `ReactSqlAgent`)\n",
    "- `nl2sql/agent_utils.py` (gates + scoring helpers)\n",
    "- `nl2sql/postprocess.py` (deterministic guardrails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed5d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Agent implementation (imported)\n",
    "\n",
    "from nl2sql.agent import ReactConfig, ReactSqlAgent\n",
    "\n",
    "# Keep config explicit so it is easy to justify in a viva (bounded steps/candidates/repair).\n",
    "CFG = ReactConfig(\n",
    "    max_steps=3,\n",
    "    num_cands=6,\n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=128,\n",
    "    enable_repair=True,\n",
    "    repair_num_cands=4,\n",
    "    use_tabular_prompt=True,\n",
    "    use_schema_subset=True,\n",
    "    use_projection_contract=True,\n",
    ")\n",
    "\n",
    "agent = ReactSqlAgent(model=model, tok=tok, runner=runner, cfg=CFG)\n",
    "# Preserve the old function name used later in the notebook.\n",
    "react_sql = agent.react_sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3feb26",
   "metadata": {},
   "source": [
    "Top-down: run the bounded ReAct loop and capture an explainable trace.\n",
    "\n",
    "What you can say out loud:\n",
    "- \"The agent proposes a few SQL candidates, executes them against the DB, and uses the DB response as feedback.\"\n",
    "- \"A candidate only becomes eligible if it passes an execution gate (it runs) and an intent gate (it matches the question type).\"\n",
    "- \"Every rejection is logged with a reason so failures can be defended with evidence, not hand-waving.\"\n",
    "\n",
    "Code pointers:\n",
    "- Main loop: `nl2sql/agent.py` (`ReactSqlAgent.react_sql`)\n",
    "- Execution gate: `nl2sql/query_runner.py` (`QueryRunner.run`)\n",
    "- Intent/scoring helpers: `nl2sql/agent_utils.py` (`intent_constraints`, `semantic_score`, `count_select_columns`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f604abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) ReAct loop\n",
    "# The loop lives in `nl2sql/agent.py` (ReactSqlAgent.react_sql).\n",
    "# This notebook calls `react_sql(...)` for quick checks and full evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a77d6f",
   "metadata": {},
   "source": [
    "## EX Troubleshooting Checklist\n",
    "\n",
    "If EX is low but VA is high, the error is usually *semantic alignment* (projection, intent, join choice).\n",
    "\n",
    "**Quick checks:**\n",
    "- **Projection drift**: NLQ lists fields but SQL returns extras or wrong order → tighten `enforce_projection_contract`.\n",
    "- **Wrong intent**: list questions returning aggregates or groupings → check `intent_constraints`.\n",
    "- **Wrong table/join**: NLQ terms not reflected in SQL tables → verify schema‑subset prompt and join hints.\n",
    "- **Literal mismatch**: NLQ mentions a literal (e.g., ‘USA’, ‘San Francisco’) but SQL misses it.\n",
    "\n",
    "**Debug workflow:**\n",
    "1. Run quick check on 5–10 items.\n",
    "2. Inspect trace phases: `clean → exec → intent` to locate failure.\n",
    "3. Adjust projection/intent/schema subset before touching repair.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca4830",
   "metadata": {},
   "source": [
    "Top-down: Manual spot-checks before running a full eval sweep.\n",
    "\n",
    "Why this cell exists:\n",
    "- Lets you inspect trace objects and failure reasons (intent gate, execution error, repair) before spending time on TS/full runs.\n",
    "\n",
    "Code pointers:\n",
    "- This notebook cell: prints the `trace` returned by `react_sql`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Quick sanity check on a few items\n",
    "schema_text = SCHEMA_SUMMARY\n",
    "for sample in test_set[:5]:\n",
    "    nlq = sample[\"nlq\"]\n",
    "    gold = sample[\"sql\"]\n",
    "    pred, trace = react_sql(\n",
    "        nlq=nlq,\n",
    "        schema_text=schema_text,\n",
    "        schema_summary=SCHEMA_SUMMARY,\n",
    "        exemplars=test_set[:3],\n",
    "    )\n",
    "    print(\"NLQ:\", nlq)\n",
    "    print(\"PRED:\", pred)\n",
    "    print(\"GOLD:\", gold)\n",
    "    print(\"TRACE LEN:\", len(trace))\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc47b20",
   "metadata": {},
   "source": [
    "### Stage 3 Interpretation (29 Jan 2026)\n",
    "\n",
    "- **Valid SQL stability:** Stage 3 generally returns executable SQL; remaining issues are **projection bloat** (extra columns), and **unnecessary ORDER BY/GROUP BY**.\n",
    "- **Metric impact:** These are EM regressions more than EX regressions. Use clamps + final normalization to keep outputs canonical.\n",
    "- **Trace logging upgrade:** The ReAct loop now logs **raw → cleaned → post‑clamp → exec error → repair attempt**, so failures can be attributed to generation vs cleaning vs execution vs repair.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceeca8b",
   "metadata": {},
   "source": [
    "## Run Order (recommended)\n",
    "\n",
    "1. **Runtime + deps** (once per fresh Colab session)\n",
    "   - Run environment install cell → restart runtime → run clone + requirements.\n",
    "   - Sanity: `torch.cuda.is_available() == True`, model loads without OOM.\n",
    "\n",
    "2. **Cloud SQL connector + base engine**\n",
    "   - Run connector + `engine = create_engine(..., creator=getconn ...)`.\n",
    "   - Sanity: `runner.run(\"SELECT 1;\").success`.\n",
    "\n",
    "3. **TS engine factory (make_engine)**\n",
    "   - Run the `make_engine(db_name)` cell.\n",
    "   - Sanity:\n",
    "     ```python\n",
    "     eng = make_engine(\"classicmodels_ts_01\")\n",
    "     with eng.connect() as c:\n",
    "         print(c.execute(text(\"SELECT COUNT(*) FROM customers\")).fetchone())\n",
    "     ```\n",
    "\n",
    "4. **Schema + dataset**\n",
    "   - Build `SCHEMA_SUMMARY`, load `test_set`.\n",
    "   - Sanity: `len(test_set) == 200` (or your chosen slice).\n",
    "\n",
    "5. **ReAct utilities + loop**\n",
    "   - Run helper cell(s) and `react_sql` cell.\n",
    "   - Sanity (3 items):\n",
    "     ```python\n",
    "     for s in test_set[:3]:\n",
    "         pred, trace = react_sql(s[\"nlq\"], SCHEMA_SUMMARY, SCHEMA_SUMMARY, exemplars=test_set[:3])\n",
    "         print(s[\"nlq\"], \"->\", pred)\n",
    "     ```\n",
    "\n",
    "6. **TS harness**\n",
    "   - Run the `test_suite_accuracy_for_item(...)` cell.\n",
    "   - Sanity:\n",
    "     ```python\n",
    "     s = test_set[0]\n",
    "     ts, dbg = test_suite_accuracy_for_item(\n",
    "         make_engine_fn=make_engine,\n",
    "         suite_db_names=[f\"classicmodels_ts_{i:02d}\" for i in range(1,4)],\n",
    "         gold_sql=s[\"sql\"],\n",
    "         pred_sql=s[\"sql\"],\n",
    "         max_rows=500,\n",
    "         strict_gold=True,\n",
    "     )\n",
    "     print(ts, dbg[\"usable_dbs\"])\n",
    "     ```\n",
    "\n",
    "7. **Evaluation**\n",
    "   - Quick run: `QUICK_LIMIT=20`, `TS_N=3`, `MAX_ROWS_TS=500`.\n",
    "   - Full run: `QUICK_LIMIT=None`, `TS_N=10`, `MAX_ROWS_TS=2000`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89984dc7",
   "metadata": {},
   "source": [
    "Top-down: Import Test-Suite Accuracy (TS) evaluator.\n",
    "\n",
    "Why this cell exists:\n",
    "- EX can be \"lucky\" on a single DB; TS checks robustness by comparing gold vs pred across perturbed replicas.\n",
    "\n",
    "Code pointers:\n",
    "- `nl2sql/eval.py` (`test_suite_accuracy_for_item`, `_results_match_ts`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Test Suite Accuracy (TS) evaluation ===\n",
    "# Harness now lives in nl2sql.eval for reuse in scripts.\n",
    "from nl2sql.eval import test_suite_accuracy_for_item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c61e8a",
   "metadata": {},
   "source": [
    "Top-down: Set cost guards for TS/EX so debugging is fast.\n",
    "\n",
    "Why this cell exists:\n",
    "- TS multiplies cost by number of replica DBs; these toggles let you run a small, safe subset during development.\n",
    "\n",
    "Code pointers:\n",
    "- `nl2sql/eval.py` (`test_suite_accuracy_for_item` uses `max_rows`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ae08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Quick test toggles (set before full eval) ===\n",
    "# Use small values to sanity‑check TS/EX before full runs.\n",
    "QUICK_LIMIT = 20   # number of NLQs to evaluate (set None for full set)\n",
    "TS_N = 3           # number of TS DBs (set 10 for full TS)\n",
    "MAX_ROWS_TS = 500  # row cap per query in TS (raise for full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f9403",
   "metadata": {},
   "source": [
    "Top-down: Full evaluation loop (VA/EM/EX/TS) + save results JSON.\n",
    "\n",
    "Why this cell exists:\n",
    "- Produces per-item results (pred_sql, trace, metrics) and aggregated rates for dissertation tables/plots.\n",
    "\n",
    "Code pointers:\n",
    "- This notebook cell: evaluation loop + result aggregation\n",
    "- `nl2sql/eval.py` (`execution_accuracy`, `test_suite_accuracy_for_item`)\n",
    "- `nl2sql/query_runner.py` (`QueryRunner.run` for VA + error messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b771d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Full ReAct-style evaluation (VA/EX/EM/TS) over test_set\n",
    "\n",
    "import json\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "from nl2sql.eval import execution_accuracy, test_suite_accuracy_for_item\n",
    "from nl2sql.postprocess import normalize_sql\n",
    "\n",
    "results = []\n",
    "\n",
    "TS_PREFIX = \"classicmodels_ts\"\n",
    "SUITE_DBS = [f\"{TS_PREFIX}_{i:02d}\" for i in range(1, TS_N + 1)]\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=32)\n",
    "def make_engine_cached(db_name: str) -> Engine:\n",
    "    return make_engine(db_name)\n",
    "\n",
    "\n",
    "def make_engine_fn(db_name: str) -> Engine:\n",
    "    return make_engine_cached(db_name)\n",
    "\n",
    "\n",
    "LIMIT = QUICK_LIMIT  # override from quick toggles\n",
    "items = test_set[:LIMIT] if LIMIT else test_set\n",
    "schema_text = SCHEMA_SUMMARY\n",
    "\n",
    "# Per-item evaluation: generate SQL and compute VA/EM/EX/TS.\n",
    "for i, sample in enumerate(items, start=1):\n",
    "    nlq = sample[\"nlq\"]\n",
    "    gold_sql = sample[\"sql\"]\n",
    "\n",
    "    pred_sql, trace = react_sql(\n",
    "        nlq=nlq,\n",
    "        schema_text=schema_text,\n",
    "        schema_summary=SCHEMA_SUMMARY,\n",
    "        exemplars=test_set[:3],\n",
    "    )\n",
    "\n",
    "    # EM is strict (normalized) string match; kept as a diagnostic signal.\n",
    "    em = int(normalize_sql(pred_sql) == normalize_sql(gold_sql))\n",
    "\n",
    "    # VA = executability on base DB.\n",
    "    meta = runner.run(pred_sql, capture_df=False)\n",
    "    va = int(meta.success)\n",
    "\n",
    "    # EX = result equivalence on base DB (only meaningful if VA=1).\n",
    "    ex = 0\n",
    "    ex_pred_err = None\n",
    "    ex_gold_err = None\n",
    "    if va:\n",
    "        ex_ok, ex_pred_err, ex_gold_err = execution_accuracy(\n",
    "            engine=engine,\n",
    "            pred_sql=pred_sql,\n",
    "            gold_sql=gold_sql,\n",
    "        )\n",
    "        ex = int(ex_ok)\n",
    "\n",
    "    # TS is expensive (runs across N replica DBs). Skip if pred_sql does not\n",
    "    # execute on the base DB (VA=0) because TS would be 0 anyway.\n",
    "    ts = 0\n",
    "    if not va:\n",
    "        ts_debug = {\"skipped\": True, \"reason\": \"va=0\", \"error\": meta.error}\n",
    "    else:\n",
    "        ts, ts_debug = test_suite_accuracy_for_item(\n",
    "            make_engine_fn=make_engine_fn,\n",
    "            suite_db_names=SUITE_DBS,\n",
    "            gold_sql=gold_sql,\n",
    "            pred_sql=pred_sql,\n",
    "            max_rows=MAX_ROWS_TS,\n",
    "            strict_gold=True,\n",
    "        )\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"nlq\": nlq,\n",
    "            \"gold_sql\": gold_sql,\n",
    "            \"pred_sql\": pred_sql,\n",
    "            \"va\": va,\n",
    "            \"em\": em,\n",
    "            \"ex\": ex,\n",
    "            \"ts\": ts,\n",
    "            \"error\": meta.error or ex_pred_err,\n",
    "            \"gold_error\": ex_gold_err,\n",
    "            \"ts_debug\": ts_debug,\n",
    "            \"trace\": trace,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {i}/{len(items)}\")\n",
    "\n",
    "va_rate = sum(r[\"va\"] for r in results) / max(len(results), 1)\n",
    "ex_rate = sum(r[\"ex\"] for r in results) / max(len(results), 1)\n",
    "em_rate = sum(r[\"em\"] for r in results) / max(len(results), 1)\n",
    "ts_rate = sum(r[\"ts\"] for r in results) / max(len(results), 1)\n",
    "print(\"ReAct VA:\", va_rate, \"EX:\", ex_rate, \"EM:\", em_rate, \"TS:\", ts_rate)\n",
    "\n",
    "Path(\"results/agent\").mkdir(parents=True, exist_ok=True)\n",
    "save_path = Path(\"results/agent/results_react_200.json\")\n",
    "save_path.write_text(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"va_rate\": va_rate,\n",
    "            \"ex_rate\": ex_rate,\n",
    "            \"em_rate\": em_rate,\n",
    "            \"ts_rate\": ts_rate,\n",
    "            \"items\": results,\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    ),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "print(\"Saved to\", save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

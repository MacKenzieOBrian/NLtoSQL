{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50805a1c",
   "metadata": {
    "id": "50805a1c"
   },
   "source": [
    "# QLoRA Fine-tuning + Evaluation (ClassicModels NL→SQL)\n",
    "\n",
    "This notebook fine-tunes the base model with **QLoRA** on a **training set that must not overlap** with `data/classicmodels_test_200.json`, then re-runs evaluation using the same `nl2sql.eval.eval_run` harness.\n",
    "\n",
    "## Expected inputs\n",
    "- Test set (fixed): `data/classicmodels_test_200.json`\n",
    "- Training set (you create): `data/train/classicmodels_train_200.jsonl` (JSON Lines with `nlq` + `sql` per row)\n",
    "\n",
    "## Outputs\n",
    "- Adapter checkpoint: `results/adapters/qlora_classicmodels/`\n",
    "- Eval outputs: `results/qlora/results_*_200.json`\n",
    "\n",
    "Note: `results/` is gitignored by default. Download the outputs from Colab when finished.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc53cff",
   "metadata": {},
   "source": [
    "Imports quick guide: nl2sql harness handles schema, prompts, postprocess, safe execution, and eval so this notebook stays small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da54560",
   "metadata": {},
   "source": [
    "Docs I leaned on: HF Transformers quantization (https://huggingface.co/docs/transformers/main_classes/quantization), PEFT/TRL (https://huggingface.co/docs/peft/, https://huggingface.co/docs/trl/), Cloud SQL connector + SQLAlchemy creator (https://cloud.google.com/sql/docs/mysql/connect-run, https://docs.sqlalchemy.org/en/20/core/engines.html#custom-dbapi-connect), ReAct (https://arxiv.org/abs/2210.03629), NL→SQL prompting survey (https://arxiv.org/abs/2410.06011)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd258b",
   "metadata": {},
   "source": [
    "Docs I leaned on: HF Transformers quantization (https://huggingface.co/docs/transformers/main_classes/quantization), PEFT/TRL (https://huggingface.co/docs/peft/, https://huggingface.co/docs/trl/), Cloud SQL connector + SQLAlchemy creator (https://cloud.google.com/sql/docs/mysql/connect-run, https://docs.sqlalchemy.org/en/20/core/engines.html#custom-dbapi-connect), ReAct (https://arxiv.org/abs/2210.03629)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf68093",
   "metadata": {},
   "source": [
    "Setup pins Colab deps (torch/bnb/triton). Run once in a fresh runtime, restart, then continue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeca115",
   "metadata": {},
   "source": [
    "Imports quick guide: we load schema helpers (`nl2sql.schema`), prompt builder/postprocess (`nl2sql.prompting`, `nl2sql.postprocess`), safe executor (`nl2sql.query_runner`), and model loader (`nl2sql.llm` or direct HF). These are small utilities we wrote to keep the notebooks thin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f32ede",
   "metadata": {},
   "source": [
    "Auth/DB: HF token + Cloud SQL connector/SQLAlchemy so the ClassicModels DB stays private."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4edb6f6",
   "metadata": {},
   "source": [
    "## One-time setup (run first in a fresh Colab GPU runtime)\n",
    "Run this cell as the first step in a fresh runtime. Let it finish, then **Runtime → Restart runtime** once, and run the rest of the notebook top-to-bottom. This pins torch/bitsandbytes/triton to CUDA 12.1 so 4-bit loading works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5860bc6",
   "metadata": {},
   "source": [
    "Schema + test set: load schema summary to ground prompts and a slice of the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26806e83",
   "metadata": {},
   "source": [
    "### Reference notes (what this code builds on)\n",
    "- Model loading/quantization follows Hugging Face Transformers docs (`AutoModelForCausalLM`, `BitsAndBytesConfig`) and PEFT/QLoRA examples (`peft`, `bitsandbytes`).\n",
    "- Prompt/eval pipeline uses the repo harness (`nl2sql/`), grounded in schema-aware prompting practices from NL→SQL literature.\n",
    "- Training/eval (05): `SFTTrainer` from `trl` with PEFT adapters per TRL/PEFT docs; LoRA config mirrors PEFT examples.\n",
    "- Auth/DB: Hugging Face token for gated Llama 3 (HF docs); Cloud SQL Connector + SQLAlchemy creator pattern from GCP docs for secure MySQL access.\n",
    "- Pinned runtime stack: torch/cu121 + bitsandbytes + triton per HF/BnB guidance for 4-bit load on Colab GPUs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f870077",
   "metadata": {},
   "source": [
    "Model load: HF 4-bit NF4 + BitsAndBytes; deterministic decoding. If adapters exist, we load them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13224190",
   "metadata": {},
   "source": [
    "**Docs (setup):** HF Transformers quantization + BitsAndBytes (4-bit) https://huggingface.co/docs/transformers/main_classes/quantization, bnb https://github.com/TimDettmers/bitsandbytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed567a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "set -e\n",
    "export PIP_DEFAULT_TIMEOUT=120\n",
    "\n",
    "# Clean conflicting preinstalls\n",
    "pip uninstall -y torch torchvision torchaudio bitsandbytes triton transformers accelerate peft trl datasets numpy pandas fsspec requests google-auth || true\n",
    "\n",
    "# Base deps\n",
    "pip install -q --no-cache-dir --force-reinstall   numpy==1.26.4 pandas==2.2.1 fsspec==2024.5.0 requests==2.31.0 google-auth==2.43.0\n",
    "\n",
    "# Torch + CUDA 12.1\n",
    "pip install -q --no-cache-dir --force-reinstall   torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121   --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# bitsandbytes + triton + HF stack\n",
    "pip install -q --no-cache-dir --force-reinstall   bitsandbytes==0.43.3 triton==2.3.1   transformers==4.44.2 accelerate==0.33.0 peft==0.17.0 trl==0.9.6 datasets==2.20.0\n",
    "\n",
    "echo \"Setup complete. Restart runtime once, then run the rest of the notebook.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ea1e2",
   "metadata": {},
   "source": [
    "Prompt/eval: build prompts (system+schema+k exemplars), generate SQL, postprocess, and compute VA/EX/EM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495e4fc",
   "metadata": {
    "id": "f495e4fc"
   },
   "outputs": [],
   "source": [
    "## Runtime setup (run second, then restart)\n",
    "Pinned torch/bitsandbytes/triton stack for Colab GPUs. Run this immediately after the guard, let it finish, then **Runtime → Restart runtime** and run the rest of the notebook top-to-bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f724ef84",
   "metadata": {},
   "source": [
    "QLoRA train/eval: TRL SFTTrainer + PEFT LoRA on 4-bit Llama-3; saves adapters and eval JSONs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18236266",
   "metadata": {},
   "source": [
    "**Docs (schema prompts):** NL→SQL schema-grounded prompting survey https://arxiv.org/abs/2410.06011; Spider-style listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786cb02c",
   "metadata": {
    "id": "786cb02c"
   },
   "outputs": [],
   "source": [
    "import os, sys, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# If opened directly in Colab, clone the repo first\n",
    "if Path(\"data/classicmodels_test_200.json\").exists() is False and Path(\"/content\").exists():\n",
    "    repo_dir = Path(\"/content/NLtoSQL\")\n",
    "    if repo_dir.exists():\n",
    "        shutil.rmtree(repo_dir)\n",
    "    !git clone https://github.com/MacKenzieOBrian/NLtoSQL.git \"{repo_dir}\"\n",
    "    os.chdir(repo_dir)\n",
    "\n",
    "sys.path.insert(0, os.getcwd())\n",
    "print(\"cwd:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce274f7",
   "metadata": {
    "id": "2ce274f7"
   },
   "source": [
    "## 0) Install dependencies (Colab)\n",
    "\n",
    "Install pinned dependencies from `requirements.txt`. Colab often needs a **runtime restart** after installs (Runtime → Restart runtime), then rerun from the top.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc1044",
   "metadata": {
    "id": "a9bc1044"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip -q install -r requirements.txt\n",
    "\n",
    "    import torch\n",
    "    import accelerate\n",
    "    import peft\n",
    "    import transformers\n",
    "    import trl\n",
    "\n",
    "    print('torch:', torch.__version__, 'cuda:', torch.cuda.is_available())\n",
    "    print('transformers:', transformers.__version__)\n",
    "    print('accelerate:', accelerate.__version__)\n",
    "    print('peft:', peft.__version__)\n",
    "    print('trl:', trl.__version__)\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        print('WARNING: CUDA is not available. In Colab, use a GPU runtime and avoid installing CPU-only torch wheels.')\n",
    "        print('If you just changed torch packages, do: Runtime -> Restart runtime, then run from the top.')\n",
    "else:\n",
    "    print('Not in Colab; ensure requirements are installed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0f3751",
   "metadata": {
    "id": "7e0f3751"
   },
   "source": [
    "## 1) Authentication (GCP + Hugging Face)\n",
    "\n",
    "- GCP auth is required for Cloud SQL access (VA evaluation).\n",
    "- HF auth is required for gated models (Meta Llama 3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9612d759",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "aef4b022d1234ef4aa54aa6165598220",
      "b1f1bbbcc93c480790e8bda49cb9deed",
      "6ee7c8bde67543159ea57f99777e7eba",
      "cb4a9b84d4734cee8cf680d1ddefd2a1",
      "627775b91ca2481391edd8b046401cdd",
      "54826113194d4df793adf94507d0c2de",
      "90d80fb8037e4fa59c32680bc7ad67c5",
      "a7e2a228207e45cba7a0658a5a2bbb67",
      "929fd603c6a444109964777013c9aecb",
      "0e3c4ebad9b745baacb1dccdcf914889",
      "9194c8df1d984dd7bea8fc183d42ddc9",
      "2d2fb4d67c404daea320222e5d18b596",
      "845fc0eae44143a191dde36685215024",
      "b7af619d67a746c6b474f1d30399cf47",
      "a20f2c42b8bb408fb6adcd321803c24e",
      "fd01cfa9ac134260ba7e48d6dd544a79",
      "570ce41cb38c4ab1b292f250b56144c1",
      "af667a7e59804144aec24b486ba054cb",
      "2b9fff4c8c5c4ff582cadafb7a6cbaa1",
      "33c78060c90e48cd933232bd0a65bbb2"
     ]
    },
    "id": "9612d759",
    "outputId": "df62133e-bf9c-4093-d448-84bb9ca2eae2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef4b022d1234ef4aa54aa6165598220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GCP auth (Colab) — safe to skip locally if using ADC\n",
    "try:\n",
    "    from google.colab import auth\n",
    "except ModuleNotFoundError:\n",
    "    auth = None\n",
    "if auth:\n",
    "    auth.authenticate_user()\n",
    "else:\n",
    "    print(\"Not running in Colab; ensure ADC/service account auth is configured.\")\n",
    "\n",
    "# Hugging Face auth\n",
    "hf_token = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if hf_token:\n",
    "    os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = hf_token\n",
    "    print(\"Using HF token from env\")\n",
    "else:\n",
    "    try:\n",
    "        from huggingface_hub import notebook_login\n",
    "        notebook_login()\n",
    "    except Exception as e:\n",
    "        print(\"HF auth not configured:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc190ec3",
   "metadata": {
    "id": "bc190ec3"
   },
   "source": [
    "## 2) Load benchmark + training set\n",
    "\n",
    "Training set must be separate from the 200-item benchmark.\n",
    "\n",
    "Recommended workflow:\n",
    "- Run `notebooks/04_build_training_set.ipynb` to validate (and edit if needed) `data/train/classicmodels_train_200.jsonl`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7ccb8",
   "metadata": {},
   "source": [
    "**Docs (schema prompts):** NL→SQL schema-grounded prompting survey https://arxiv.org/abs/2410.06011; Spider-style listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529e213",
   "metadata": {
    "id": "7529e213"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "test_path = Path(\"data/classicmodels_test_200.json\")\n",
    "train_path = Path(\"data/train/classicmodels_train_200.jsonl\")\n",
    "\n",
    "test_set = json.loads(test_path.read_text(encoding=\"utf-8\"))\n",
    "print(\"Test items:\", len(test_set))\n",
    "\n",
    "if not train_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing training set at {train_path}. Create it before running QLoRA. \"\n",
    "        \"Expected JSONL lines with keys: nlq, sql.\"\n",
    "    )\n",
    "\n",
    "train_records = []\n",
    "for line in train_path.read_text(encoding=\"utf-8\").splitlines():\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    train_records.append(json.loads(line))\n",
    "\n",
    "print(\"Train items:\", len(train_records))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca8b59",
   "metadata": {
    "id": "77ca8b59"
   },
   "source": [
    "### Leakage check (train vs test)\n",
    "\n",
    "At minimum, ensure there is no exact NLQ overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2905be06",
   "metadata": {
    "id": "2905be06"
   },
   "outputs": [],
   "source": [
    "test_nlqs = {item[\"nlq\"].strip() for item in test_set}\n",
    "train_nlqs = [r.get(\"nlq\", \"\").strip() for r in train_records]\n",
    "overlap = sorted({nlq for nlq in train_nlqs if nlq in test_nlqs})\n",
    "\n",
    "print(\"NLQ overlap count:\", len(overlap))\n",
    "if overlap:\n",
    "    print(\"Example overlaps:\")\n",
    "    for x in overlap[:10]:\n",
    "        print(\"-\", x)\n",
    "    raise ValueError(\"Training set overlaps test set; remove overlapping items before training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6401a9",
   "metadata": {
    "id": "6c6401a9"
   },
   "source": [
    "## 3) DB engine + schema summary\n",
    "\n",
    "Schema grounding is kept consistent with the baseline by using `nl2sql.schema.build_schema_summary`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab014ee8",
   "metadata": {},
   "source": [
    "**Ref:** Schema summary helper from this repo (`nl2sql.schema`) aligned with schema-grounded NL→SQL prompting (survey: https://arxiv.org/abs/2410.06011) to cut schema/join errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e3e880",
   "metadata": {},
   "source": [
    "**Docs (auth/DB):** Cloud SQL connector pattern https://cloud.google.com/sql/docs/mysql/connect-run; SQLAlchemy creator hook https://docs.sqlalchemy.org/en/20/core/engines.html#custom-dbapi-connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1f1e3",
   "metadata": {
    "id": "fab1f1e3"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from nl2sql.db import create_engine_with_connector\n",
    "from nl2sql.schema import build_schema_summary\n",
    "\n",
    "INSTANCE_CONNECTION_NAME = os.getenv(\"INSTANCE_CONNECTION_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"classicmodels\")\n",
    "\n",
    "if not INSTANCE_CONNECTION_NAME:\n",
    "    INSTANCE_CONNECTION_NAME = input(\"Enter INSTANCE_CONNECTION_NAME: \").strip()\n",
    "if not DB_USER:\n",
    "    DB_USER = input(\"Enter DB_USER: \").strip()\n",
    "if not DB_PASS:\n",
    "    DB_PASS = getpass(\"Enter DB_PASS: \")\n",
    "\n",
    "engine, connector = create_engine_with_connector(\n",
    "    instance_connection_name=INSTANCE_CONNECTION_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASS,\n",
    "    db_name=DB_NAME,\n",
    ")\n",
    "\n",
    "SCHEMA_SUMMARY = build_schema_summary(engine, db_name=DB_NAME, max_cols_per_table=50)\n",
    "print(\"Schema summary length:\", len(SCHEMA_SUMMARY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f078ecd9",
   "metadata": {
    "id": "f078ecd9"
   },
   "source": [
    "## 4) Load base model (4-bit) + configure QLoRA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43be14b",
   "metadata": {},
   "source": [
    "**Ref:** HF Transformers 4-bit NF4 load with BitsAndBytes (quantization docs: https://huggingface.co/docs/transformers/main_classes/quantization) following PEFT/QLoRA patterns for gated Llama 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9263d",
   "metadata": {},
   "source": [
    "**Docs (model load):** HF 4-bit NF4 quantization https://huggingface.co/docs/transformers/main_classes/quantization; PEFT/QLoRA https://huggingface.co/docs/peft/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cd208f",
   "metadata": {
    "id": "11cd208f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError('CUDA is not available. In Colab, switch to a GPU runtime: Runtime -> Change runtime type -> GPU.')\n",
    "\n",
    "cc_major, cc_minor = torch.cuda.get_device_capability(0)\n",
    "use_bf16 = cc_major >= 8  # Ampere+ (e.g., A100). T4 (7.5) does NOT support bf16.\n",
    "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "\n",
    "print('GPU:', torch.cuda.get_device_name(0))\n",
    "print('Compute capability:', (cc_major, cc_minor))\n",
    "print('Using bf16:', use_bf16, '| compute_dtype:', compute_dtype)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, token=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "# transformers/bitsandbytes 4-bit quantization does not allow some layers to be auto-offloaded\n",
    "# to CPU/disk. Force the whole model onto GPU:0. If you OOM, restart runtime and close other\n",
    "# notebooks/tabs, or use a higher-memory GPU (A100/L4).\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map=device_map,\n",
    "    token=True,\n",
    ")\n",
    "\n",
    "# Deterministic defaults for later evaluation\n",
    "base_model.generation_config.do_sample = False\n",
    "base_model.generation_config.temperature = 1.0\n",
    "base_model.generation_config.top_p = 1.0\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5e4a45",
   "metadata": {
    "id": "ae5e4a45"
   },
   "source": [
    "## 5) Build the SFT dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc08f220",
   "metadata": {
    "id": "cc08f220"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from nl2sql.prompting import SYSTEM_INSTRUCTIONS\n",
    "\n",
    "def format_example(nlq: str, sql: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_INSTRUCTIONS},\n",
    "        {\"role\": \"user\", \"content\": \"Schema:\\n\" + SCHEMA_SUMMARY},\n",
    "        {\"role\": \"user\", \"content\": f\"NLQ: {nlq}\"},\n",
    "        {\"role\": \"assistant\", \"content\": sql.rstrip(\";\") + \";\"},\n",
    "    ]\n",
    "    return tok.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "train_texts = [format_example(r[\"nlq\"], r[\"sql\"]) for r in train_records]\n",
    "train_ds = Dataset.from_dict({\"text\": train_texts})\n",
    "print(train_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74808373",
   "metadata": {
    "id": "74808373"
   },
   "source": [
    "## 6) Train (SFT with TRL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b099d12",
   "metadata": {},
   "source": [
    "**Ref:** TRL `SFTTrainer` + PEFT LoRA config (TRL docs: https://huggingface.co/docs/trl/main/en/sft_trainer; PEFT docs: https://huggingface.co/docs/peft/index). This is the standard QLoRA-style supervised fine-tuning loop on our 200 NL→SQL pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf92f6",
   "metadata": {},
   "source": [
    "**Docs (QLoRA train):** TRL SFTTrainer https://huggingface.co/docs/trl/main/en/sft_trainer; PEFT LoRA https://huggingface.co/docs/peft/index; bnb 4-bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17933a6",
   "metadata": {
    "id": "f17933a6"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"results/adapters/qlora_classicmodels\"\n",
    "\n",
    "# T4 GPUs in Colab do not support bf16; use fp16 in that case.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.05,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    bf16=use_bf16,\n",
    "    fp16=(not use_bf16),\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tok,\n",
    "    train_dataset=train_ds,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=training_args,\n",
    "    max_seq_length=1024,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tok.save_pretrained(output_dir)\n",
    "print(\"Saved adapters to:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a6f4c",
   "metadata": {
    "id": "e53a6f4c"
   },
   "source": [
    "## 7) Evaluate adapters on the same 200-item test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3adb00",
   "metadata": {},
   "source": [
    "**Ref:** HF Transformers 4-bit NF4 load with BitsAndBytes (quantization docs: https://huggingface.co/docs/transformers/main_classes/quantization) following PEFT/QLoRA patterns for gated Llama 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ccc438",
   "metadata": {},
   "source": [
    "**Docs (model load):** HF 4-bit NF4 quantization https://huggingface.co/docs/transformers/main_classes/quantization; PEFT/QLoRA https://huggingface.co/docs/peft/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30897b4",
   "metadata": {
    "id": "d30897b4"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from nl2sql.eval import eval_run\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "eval_base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map=device_map,\n",
    "    token=True,\n",
    ")\n",
    "eval_base.generation_config.do_sample = False\n",
    "eval_base.generation_config.temperature = 1.0\n",
    "eval_base.generation_config.top_p = 1.0\n",
    "\n",
    "eval_model = PeftModel.from_pretrained(eval_base, output_dir)\n",
    "\n",
    "try:\n",
    "    commit = subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"]).decode().strip()\n",
    "except Exception:\n",
    "    commit = \"unknown\"\n",
    "\n",
    "run_metadata = {\n",
    "    \"commit\": commit,\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"method\": \"qlora\",\n",
    "    \"adapter_dir\": output_dir,\n",
    "}\n",
    "\n",
    "Path(\"results/qlora\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "qlora_zero_200 = eval_run(\n",
    "    test_set=test_set,\n",
    "    exemplar_pool=test_set,\n",
    "    k=0,\n",
    "    limit=None,\n",
    "    seed=7,\n",
    "    engine=engine,\n",
    "    model=eval_model,\n",
    "    tokenizer=tok,\n",
    "    schema_summary=SCHEMA_SUMMARY,\n",
    "    save_path=\"results/qlora/results_zero_shot_200.json\",\n",
    "    run_metadata=run_metadata,\n",
    "    avoid_exemplar_leakage=True,\n",
    ")\n",
    "\n",
    "qlora_few_200 = eval_run(\n",
    "    test_set=test_set,\n",
    "    exemplar_pool=test_set,\n",
    "    k=3,\n",
    "    limit=None,\n",
    "    seed=7,\n",
    "    engine=engine,\n",
    "    model=eval_model,\n",
    "    tokenizer=tok,\n",
    "    schema_summary=SCHEMA_SUMMARY,\n",
    "    save_path=\"results/qlora/results_few_shot_k3_200.json\",\n",
    "    run_metadata=run_metadata,\n",
    "    avoid_exemplar_leakage=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d656bc",
   "metadata": {
    "id": "75d656bc"
   },
   "source": [
    "## 8) Compare against baseline outputs (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b6ce72",
   "metadata": {
    "id": "e0b6ce72"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "baseline_zero = Path(\"results/baseline/results_zero_shot_200.json\")\n",
    "baseline_few  = Path(\"results/baseline/results_few_shot_k3_200.json\")\n",
    "\n",
    "if baseline_zero.exists() and baseline_few.exists():\n",
    "    b0 = json.loads(baseline_zero.read_text(encoding=\"utf-8\"))\n",
    "    b3 = json.loads(baseline_few.read_text(encoding=\"utf-8\"))\n",
    "    q0 = json.loads(Path(\"results/qlora/results_zero_shot_200.json\").read_text(encoding=\"utf-8\"))\n",
    "    q3 = json.loads(Path(\"results/qlora/results_few_shot_k3_200.json\").read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    print(\"Baseline zero-shot:\", \"VA\", round(b0[\"va_rate\"], 3), \"EM\", round(b0.get(\"em_rate\", 0.0), 3), \"EX\", round(b0[\"ex_rate\"], 3))\n",
    "    print(\"QLoRA   zero-shot:\", \"VA\", round(q0[\"va_rate\"], 3), \"EM\", round(q0.get(\"em_rate\", 0.0), 3), \"EX\", round(q0[\"ex_rate\"], 3))\n",
    "    print(\"Baseline few-shot :\", \"VA\", round(b3[\"va_rate\"], 3), \"EM\", round(b3.get(\"em_rate\", 0.0), 3), \"EX\", round(b3[\"ex_rate\"], 3))\n",
    "    print(\"QLoRA   few-shot :\", \"VA\", round(q3[\"va_rate\"], 3), \"EM\", round(q3.get(\"em_rate\", 0.0), 3), \"EX\", round(q3[\"ex_rate\"], 3))\n",
    "else:\n",
    "    print(\"Baseline JSONs not found under results/baseline/. Run the baseline notebook first (or upload the JSONs).\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

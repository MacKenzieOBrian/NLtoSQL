{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f2b652",
   "metadata": {},
   "source": [
    "# Agentic Evaluation (Tool-Driven ReAct Loop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef67421",
   "metadata": {},
   "source": [
    "### Setup (Colab only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b144d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "set -e\n",
    "export PIP_DEFAULT_TIMEOUT=120\n",
    "\n",
    "# Clean conflicting preinstalls\n",
    "pip uninstall -y torch torchvision torchaudio bitsandbytes triton transformers accelerate peft trl datasets numpy pandas fsspec requests google-auth || true\n",
    "\n",
    "# Base deps\n",
    "pip install -q --no-cache-dir --force-reinstall   numpy==1.26.4 pandas==2.2.1 fsspec==2024.5.0 requests==2.31.0 google-auth==2.43.0\n",
    "\n",
    "# Torch + CUDA 12.1\n",
    "pip install -q --no-cache-dir --force-reinstall   torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121   --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# bitsandbytes + triton + HF stack\n",
    "pip install -q --no-cache-dir --force-reinstall   bitsandbytes==0.43.3 triton==2.3.1   transformers==4.44.2 accelerate==0.33.0 peft==0.17.0 trl==0.9.6 datasets==2.20.0\n",
    "\n",
    "echo \"Setup complete. Restart runtime once, then run the rest of the notebook top-to-bottom.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b6008",
   "metadata": {},
   "source": [
    "### Repo setup (Colab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47936fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Clone repo (Colab) + install deps\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not os.path.exists('/content/NLtoSQL'):\n",
    "        !git clone https://github.com/MacKenzieOBrian/NLtoSQL.git /content/NLtoSQL\n",
    "    %cd /content/NLtoSQL\n",
    "    !pip -q install -r requirements.txt\n",
    "    import torch, transformers, accelerate, peft\n",
    "    print('torch', torch.__version__, 'cuda', torch.cuda.is_available())\n",
    "else:\n",
    "    print('Not in Colab; using existing workspace')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58d00e",
   "metadata": {},
   "source": [
    "### Optional: ADC auth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this only if you prefer gcloud-based ADC (no JSON key)\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    %pip install -q --upgrade google-auth google-auth-oauthlib\n",
    "    !gcloud auth application-default login\n",
    "else:\n",
    "    print(\"Not in Colab; skip gcloud auth.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada87478",
   "metadata": {},
   "source": [
    "### Database Connection and Query Execution Layer\n",
    "\n",
    "This block establishes a reproducible connection to ClassicModels and initializes the query runner\n",
    "used for validation and execution accuracy. It ensures all downstream metrics are computed\n",
    "against a consistent database state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Environment + DB\n",
    "from getpass import getpass\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "from nl2sql.db import create_engine_with_connector, safe_connection\n",
    "\n",
    "# Expected env vars (set these in a Colab cell):\n",
    "# INSTANCE_CONNECTION_NAME, DB_USER, DB_PASS, DB_NAME\n",
    "INSTANCE_CONNECTION_NAME = os.getenv(\"INSTANCE_CONNECTION_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\") or \"classicmodels\"\n",
    "\n",
    "if not INSTANCE_CONNECTION_NAME:\n",
    "    INSTANCE_CONNECTION_NAME = input(\"Enter INSTANCE_CONNECTION_NAME: \").strip()\n",
    "if not DB_USER:\n",
    "    DB_USER = input(\"Enter DB_USER: \").strip()\n",
    "if not DB_PASS:\n",
    "    DB_PASS = getpass(\"Enter DB_PASS: \")\n",
    "\n",
    "# Canonical engine builder (shared with scripts + other notebooks).\n",
    "# Uses Cloud SQL Connector under the hood and ADC for credentials.\n",
    "engine, connector = create_engine_with_connector(\n",
    "    instance_connection_name=INSTANCE_CONNECTION_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASS,\n",
    "    db_name=DB_NAME,\n",
    ")\n",
    "\n",
    "with safe_connection(engine) as conn:\n",
    "    conn.execute(text(\"SELECT 1\"))\n",
    "print(\"DB connection OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9a6fb",
   "metadata": {},
   "source": [
    "### TS engine factory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b) Engine factory for TS (multiple DB names)\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "\n",
    "def make_engine(db_name: str) -> Engine:\n",
    "    \"\"\"Create a new engine bound to a specific TS replica DB name.\n",
    "\n",
    "    TS (test-suite accuracy) executes the same (gold, pred) SQL across multiple\n",
    "    replica databases (classicmodels_ts_XX). We keep separate engines so each\n",
    "    replica is evaluated independently.\n",
    "    \"\"\"\n",
    "\n",
    "    def getconn_for_db():\n",
    "        return connector.connect(\n",
    "            INSTANCE_CONNECTION_NAME,\n",
    "            \"pymysql\",\n",
    "            user=DB_USER,\n",
    "            password=DB_PASS,\n",
    "            db=db_name,\n",
    "        )\n",
    "\n",
    "    return sqlalchemy.create_engine(\"mysql+pymysql://\", creator=getconn_for_db, future=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6cfbfc",
   "metadata": {},
   "source": [
    "### Schema Summary and Test Set\n",
    "\n",
    "We build a compact schema summary for prompt grounding and load the evaluation set used across runs.\n",
    "This keeps the context small while preserving the tables and columns required for correct SQL synthesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Schema summary + test set + QueryRunner\n",
    "QUICK_LIMIT = 20  # number of NLQs to evaluate (set None for full set)\n",
    "import json\n",
    "from pathlib import Path\n",
    "from nl2sql.schema import build_schema_summary\n",
    "from nl2sql.query_runner import QueryRunner\n",
    "\n",
    "DB_NAME = globals().get(\"DB_NAME\") or os.getenv(\"DB_NAME\") or \"classicmodels\"\n",
    "SCHEMA_SUMMARY = build_schema_summary(engine, db_name=DB_NAME)\n",
    "test_path = Path(\"data/classicmodels_test_200.json\")\n",
    "full_set = json.loads(test_path.read_text(encoding=\"utf-8\"))\n",
    "test_set = full_set  # slice later via QUICK_LIMIT\n",
    "print(\"Loaded test set size:\", len(test_set))\n",
    "\n",
    "# Small exemplar set (seeded to encourage join behavior).\n",
    "join_exemplars = [it for it in full_set if \"office\" in it[\"nlq\"].lower()]\n",
    "REACT_EXEMPLARS = []\n",
    "if join_exemplars:\n",
    "    REACT_EXEMPLARS.append(join_exemplars[0])\n",
    "for it in full_set:\n",
    "    if it not in REACT_EXEMPLARS:\n",
    "        REACT_EXEMPLARS.append(it)\n",
    "    if len(REACT_EXEMPLARS) >= 3:\n",
    "        break\n",
    "print(\"Exemplars:\", [e[\"nlq\"] for e in REACT_EXEMPLARS])\n",
    "\n",
    "runner = QueryRunner(engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b3611c",
   "metadata": {},
   "source": [
    "### Load model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f38fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Load model (base or QLoRA adapters)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "ADAPTER_PATH = os.getenv(\"ADAPTER_PATH\") or \"results/adapters/qlora_classicmodels\"  # set to None to use base model\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if not HF_TOKEN:\n",
    "    HF_TOKEN = getpass(\"Enter HF_TOKEN (https://huggingface.co/settings/tokens): \").strip()\n",
    "\n",
    "cc_major, cc_minor = torch.cuda.get_device_capability(0) if torch.cuda.is_available() else (0, 0)\n",
    "use_bf16 = cc_major >= 8\n",
    "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"Using bf16:\", use_bf16)\n",
    "print(\"Adapter path:\", ADAPTER_PATH)\n",
    "\n",
    "# Tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# Quantized base model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map={\"\": 0} if torch.cuda.is_available() else None,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "base_model.generation_config.do_sample = False\n",
    "base_model.generation_config.temperature = 1.0\n",
    "base_model.generation_config.top_p = 1.0\n",
    "\n",
    "# Load adapters if present locally; otherwise use base model\n",
    "adapter_dir = Path(ADAPTER_PATH) if ADAPTER_PATH else None\n",
    "if adapter_dir and adapter_dir.exists():\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_dir, token=HF_TOKEN)\n",
    "    print(\"Loaded adapters from\", adapter_dir)\n",
    "else:\n",
    "    print(\"Adapter path missing; using base model only. Set ADAPTER_PATH to your local adapter folder or upload it to Colab.\")\n",
    "    model = base_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7065790e",
   "metadata": {},
   "source": [
    "### Optional: smoke check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fbae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nl2sql.prompting import make_few_shot_messages\n",
    "from nl2sql.llm import extract_first_select\n",
    "from nl2sql.postprocess import guarded_postprocess\n",
    "from nl2sql.eval import execution_accuracy\n",
    "\n",
    "from nl2sql.agent_utils import _extract_required_columns\n",
    "\n",
    "runner_check = QueryRunner(engine)\n",
    "# reuse existing test_set (default small slice); pick 3 exemplars\n",
    "exemplars = test_set[:3]\n",
    "\n",
    "def run_quick_check(k: int = 0, limit: int = 3):\n",
    "    print(f\"Quick check k={k}\")\n",
    "    for sample in test_set[:limit]:\n",
    "        shots = exemplars if k > 0 else []\n",
    "        msgs = make_few_shot_messages(\n",
    "            schema=SCHEMA_SUMMARY,\n",
    "            exemplars=shots,\n",
    "            nlq=sample['nlq'],\n",
    "        )\n",
    "        prompt_preview = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tok(prompt_preview, return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "\n",
    "        # strip the prompt before decoding the generation\n",
    "        gen_ids = out[0][inputs.input_ids.shape[-1]:]\n",
    "        text = tok.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "        raw_sql = extract_first_select(text) or text\n",
    "        explicit_fields = _extract_required_columns(sample['nlq'])\n",
    "        sql = guarded_postprocess(raw_sql, sample['nlq'], explicit_fields=explicit_fields)\n",
    "\n",
    "        meta = runner_check.run(sql, capture_df=False)\n",
    "        va = meta.success\n",
    "        ex_ok, _, _ = execution_accuracy(engine=engine, pred_sql=sql, gold_sql=sample['sql'])\n",
    "        err = meta.error\n",
    "        print(f\"Q: {sample['nlq']}\\nSQL: {sql}\\nVA: {va} EX: {ex_ok}\")\n",
    "        if not va:\n",
    "            print(f\"ERR: {err}\")\n",
    "        print()\n",
    "\n",
    "run_quick_check(k=0)\n",
    "run_quick_check(k=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f4b1f",
   "metadata": {},
   "source": [
    "### Guardrails and Post-Processing Utilities\n",
    "\n",
    "Guardrails provide lightweight correctness constraints (SELECT-only, schema checks, and\n",
    "intent checks) that prevent \"runs but wrong\" SQL. Projection cleanup is conservative:\n",
    "explicit projection is enforced only when the NLQ enumerates fields (or a high-precision\n",
    "template requires it), and minimal-projection pruning is skipped when constraints require\n",
    "specific output fields. This keeps post-processing from changing query semantics while\n",
    "still reducing common formatting noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b07b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Agent utilities + guardrails\n",
    "import importlib\n",
    "import nl2sql.agent_utils as agent_utils\n",
    "importlib.reload(agent_utils)\n",
    "\n",
    "from nl2sql.agent_utils import (\n",
    "    _extract_required_columns,\n",
    "    intent_constraints,\n",
    "    classify_intent,\n",
    "    clean_candidate_with_reason,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952b8ec",
   "metadata": {},
   "source": [
    "## Tool-Driven ReAct Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d555900",
   "metadata": {},
   "source": [
    "### Define ReAct loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method: Tool-driven ReAct for NL->SQL\n",
    "\n",
    "We implement a tool-driven ReAct loop that alternates between reasoning steps and executable actions.\n",
    "The objective is to ground generation in schema facts, enforce constraints before execution, and\n",
    "use execution feedback to repair errors while keeping traces auditable.\n",
    "\n",
    "Decision logs and traces are retained to support error analysis and dissertation reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Canonical ReAct pipeline (module-backed)\n",
    "\n",
    "import importlib\n",
    "\n",
    "import nl2sql.agent_tools as agent_tools\n",
    "import nl2sql.react_pipeline as react_pipeline\n",
    "\n",
    "importlib.reload(agent_tools)\n",
    "importlib.reload(react_pipeline)\n",
    "\n",
    "from nl2sql.agent_tools import AgentContext, set_agent_context\n",
    "from nl2sql.react_pipeline import ReactAblationConfig, run_react_pipeline, evaluate_react_ablation\n",
    "\n",
    "# Bind runtime dependencies once for module-level tools.\n",
    "set_agent_context(\n",
    "    AgentContext(\n",
    "        engine=engine,\n",
    "        db_name=DB_NAME,\n",
    "        model=model,\n",
    "        tok=tok,\n",
    "        runner=runner,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Rationale (Concise)\n",
    "\n",
    "- **Schema grounding**: constrain the model to known tables/columns to reduce hallucinations.\n",
    "- **Constraint extraction**: infer structure (COUNT/GROUP BY/LIMIT) from the NLQ before SQL generation.\n",
    "- **Validation gates**: block invalid or structurally wrong SQL before execution.\n",
    "- **Execution-guided repair**: use runtime errors to guide correction instead of free-form regeneration.\n",
    "- **Determinism first**: default to non-sampling for reproducibility; enable sampling only for reranking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonical toggles + notebook compatibility helpers\n",
    "\n",
    "USE_LINK_SCHEMA = True\n",
    "USE_CONSTRAINT_POLICY = True\n",
    "USE_REPAIR_POLICY = True\n",
    "REACT_MAX_REPAIRS = 2\n",
    "REACT_LINK_MAX_TABLES = 6\n",
    "\n",
    "\n",
    "def _react_cfg(name: str = \"notebook_react\") -> ReactAblationConfig:\n",
    "    return ReactAblationConfig(\n",
    "        name=name,\n",
    "        use_schema_link=USE_LINK_SCHEMA,\n",
    "        use_constraint_policy=USE_CONSTRAINT_POLICY,\n",
    "        use_repair_policy=USE_REPAIR_POLICY,\n",
    "        max_repairs=REACT_MAX_REPAIRS,\n",
    "        link_max_tables=REACT_LINK_MAX_TABLES,\n",
    "    )\n",
    "\n",
    "\n",
    "def _trace_to_decisions(trace: list[dict]) -> list[dict]:\n",
    "    decisions: list[dict] = []\n",
    "    for i, t in enumerate(trace or []):\n",
    "        stage = t.get(\"stage\")\n",
    "        if not stage:\n",
    "            continue\n",
    "        d = {\"step\": i, \"decision\": stage, \"reason\": \"ok\", \"status\": \"ok\", \"data\": {}}\n",
    "\n",
    "        if stage in (\"validate_sql\", \"validate_constraints\"):\n",
    "            res = t.get(\"result\") or {}\n",
    "            d[\"reason\"] = res.get(\"reason\", \"ok\")\n",
    "            d[\"status\"] = \"ok\" if res.get(\"valid\") else \"reject\"\n",
    "            data = dict(res)\n",
    "            if t.get(\"sql\"):\n",
    "                data[\"sql\"] = t.get(\"sql\")\n",
    "            d[\"data\"] = data\n",
    "        elif stage == \"run_sql\":\n",
    "            res = t.get(\"result\") or {}\n",
    "            d[\"reason\"] = \"execute\"\n",
    "            d[\"status\"] = \"ok\" if res.get(\"success\") else \"reject\"\n",
    "            d[\"data\"] = {\n",
    "                \"sql\": t.get(\"sql\"),\n",
    "                \"success\": res.get(\"success\"),\n",
    "                \"rowcount\": res.get(\"rowcount\"),\n",
    "                \"error\": res.get(\"error\"),\n",
    "            }\n",
    "        elif stage == \"intent_check\":\n",
    "            d[\"reason\"] = t.get(\"reason\", \"ok\")\n",
    "            d[\"status\"] = \"ok\" if t.get(\"ok\") else \"reject\"\n",
    "            d[\"data\"] = {\"ok\": t.get(\"ok\")}\n",
    "        else:\n",
    "            d[\"data\"] = {k: v for k, v in t.items() if k != \"stage\"}\n",
    "\n",
    "        decisions.append(d)\n",
    "    return decisions\n",
    "\n",
    "\n",
    "def format_decision_log(decisions: list[dict], max_items: int | None = 20) -> str:\n",
    "    if not decisions:\n",
    "        return \"(no decisions logged)\"\n",
    "    out: list[str] = []\n",
    "    lim = max_items if max_items is not None else len(decisions)\n",
    "    for d in decisions[:lim]:\n",
    "        out.append(f\"[step {d.get('step')}] {d.get('decision')} - {d.get('reason')} ({d.get('status')})\")\n",
    "        data = d.get(\"data\")\n",
    "        if data:\n",
    "            s = str(data)\n",
    "            if len(s) > 320:\n",
    "                s = s[:317] + \"...\"\n",
    "            out.append(f\"  data: {s}\")\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "\n",
    "def summarize_trace(trace: list[dict]) -> dict:\n",
    "    actions = [t.get(\"stage\") for t in trace if t.get(\"stage\")]\n",
    "    repairs = sum(1 for a in actions if a == \"repair_sql\")\n",
    "    return {\n",
    "        \"actions\": actions,\n",
    "        \"attempted_actions\": [],\n",
    "        \"blocked_steps\": 0,\n",
    "        \"repairs\": repairs,\n",
    "        \"forced_repairs\": repairs,\n",
    "        \"compliance_ok\": True,\n",
    "        \"compliance_errors\": [],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Notes\n",
    "\n",
    "The loop is split into (1) setup + configuration, (2) helper utilities, and (3) the main `react_sql`\n",
    "controller. This keeps the notebook readable while preserving a single linear execution flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def react_sql(\n",
    "    *,\n",
    "    nlq: str,\n",
    "    schema_text: str | None = None,\n",
    "    schema_summary: str | None = None,\n",
    "    exemplars: list[dict] | None = None,\n",
    "    max_steps: int = 8,\n",
    "    debug: bool = False,\n",
    "    debug_sleep_s: float = 0.0,\n",
    "    debug_prompt_tail_lines: int = 0,\n",
    "    debug_rows_preview: int = 3,\n",
    "    auto_order: bool = False,\n",
    ") -> tuple[str, list[dict], list[dict]]:\n",
    "    cfg = _react_cfg(name=\"react_notebook\")\n",
    "    pred_sql, trace = run_react_pipeline(nlq=nlq, config=cfg)\n",
    "    decisions = _trace_to_decisions(trace)\n",
    "\n",
    "    if debug:\n",
    "        print(\"ReAct pipeline (module-backed) trace:\")\n",
    "        for i, t in enumerate(trace):\n",
    "            stage = t.get(\"stage\")\n",
    "            if not stage:\n",
    "                continue\n",
    "            if stage in (\"generate_sql\", \"repair_sql\"):\n",
    "                raw = t.get(\"raw_sql\")\n",
    "                cleaned = t.get(\"sql_after_guardrails\")\n",
    "                if raw:\n",
    "                    print(f\"[step {i}] {stage} raw_sql: {' '.join(str(raw).split())[:220]}\")\n",
    "                if cleaned:\n",
    "                    print(f\"[step {i}] {stage} cleaned_sql: {' '.join(str(cleaned).split())[:220]}\")\n",
    "            elif stage in (\"validate_sql\", \"validate_constraints\"):\n",
    "                res = t.get(\"result\") or {}\n",
    "                print(f\"[step {i}] {stage}: valid={res.get('valid')} reason={res.get('reason')}\")\n",
    "            elif stage == \"run_sql\":\n",
    "                res = t.get(\"result\") or {}\n",
    "                print(f\"[step {i}] run_sql: success={res.get('success')} rowcount={res.get('rowcount')} error={res.get('error')}\")\n",
    "            elif stage == \"intent_check\":\n",
    "                print(f\"[step {i}] intent_check: ok={t.get('ok')} reason={t.get('reason')}\")\n",
    "            else:\n",
    "                print(f\"[step {i}] {stage}\")\n",
    "\n",
    "    return pred_sql, trace, decisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0cb3f3",
   "metadata": {},
   "source": [
    "### Quick sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b8b3c4fd",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 6a) Interactive walkthrough: type an NLQ and watch the loop step-by-step\n",
    "DEMO_INTERACTIVE = True\n",
    "DEMO_DEFAULT_NLQ = \"Which customers are in France?\"\n",
    "DEMO_AUTO_ORDER = True  # keep the walkthrough linear (forces the next required step)\n",
    "DEMO_SLEEP_S = 0.8  # set 0 for fast\n",
    "DEMO_PROMPT_TAIL = 0  # set >0 to show the transcript tail the model sees\n",
    "SHOW_DECISIONS = False\n",
    "\n",
    "nlq = \"\"\n",
    "if DEMO_INTERACTIVE:\n",
    "    try:\n",
    "        nlq = input(\"Type a ClassicModels question (blank uses default): \").strip()\n",
    "    except Exception:\n",
    "        nlq = \"\"\n",
    "if not nlq:\n",
    "    nlq = DEMO_DEFAULT_NLQ\n",
    "\n",
    "pred, trace, decisions = react_sql(\n",
    "    nlq=nlq,\n",
    "    schema_summary=SCHEMA_SUMMARY,\n",
    "    exemplars=REACT_EXEMPLARS,\n",
    "    debug=True,\n",
    "    auto_order=DEMO_AUTO_ORDER,\n",
    "    debug_sleep_s=DEMO_SLEEP_S,\n",
    "    debug_prompt_tail_lines=DEMO_PROMPT_TAIL,\n",
    ")\n",
    "\n",
    "print(\"\\nFINAL SQL:\")\n",
    "print(pred)\n",
    "print(\"\\nTRACE SUMMARY:\", summarize_trace(trace))\n",
    "if SHOW_DECISIONS:\n",
    "    print(\"\\nDECISIONS:\\n\" + format_decision_log(decisions, max_items=40))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Sanity Check\n",
    "\n",
    "A small, fast slice used to validate end-to-end behavior before longer runs. It surfaces\n",
    "structural errors early and confirms the tool ordering and constraint checks behave as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Quick sanity check on a few items (ReAct full loop)\n",
    "\n",
    "from nl2sql.eval import execution_accuracy\n",
    "\n",
    "DEBUG_EX = False\n",
    "DEBUG_TRACE = False\n",
    "DEBUG_TRACE_ON_MISMATCH = True\n",
    "DEBUG_SQL_BUILD = True\n",
    "DEBUG_SQL_MAX_CHARS = 220\n",
    "\n",
    "\n",
    "def _normalize_sql_for_print(sql: str, max_chars: int = 220) -> str:\n",
    "    s = \" \".join(str(sql or \"\").split())\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    if max_chars and len(s) > max_chars:\n",
    "        s = s[: max_chars - 3] + \"...\"\n",
    "    return s\n",
    "\n",
    "\n",
    "def _collect_sql_timeline(decisions: list[dict]) -> list[tuple[int, str, str]]:\n",
    "    out: list[tuple[int, str, str]] = []\n",
    "    for d in decisions:\n",
    "        step = int(d.get(\"step\", -1))\n",
    "        dec = d.get(\"decision\")\n",
    "        data = d.get(\"data\") or {}\n",
    "        if dec in (\"generate_sql\", \"repair_sql\"):\n",
    "            raw = data.get(\"raw_sql\")\n",
    "            if raw:\n",
    "                out.append((step, \"raw_sql\", raw))\n",
    "            cleaned = data.get(\"sql_after_guardrails\")\n",
    "            if cleaned:\n",
    "                out.append((step, \"cleaned_sql\", cleaned))\n",
    "        elif dec in (\"validate_sql\", \"validate_constraints\", \"run_sql\"):\n",
    "            sql = data.get(\"sql\")\n",
    "            if sql:\n",
    "                out.append((step, dec + \"_sql\", sql))\n",
    "    return out\n",
    "\n",
    "\n",
    "def _print_sql_timeline(items: list[tuple[int, str, str]], max_chars: int = 220) -> None:\n",
    "    if not items:\n",
    "        print(\"SQL BUILD: (no SQL events captured)\")\n",
    "        return\n",
    "    print(\"SQL BUILD:\")\n",
    "    last = None\n",
    "    for step, label, sql in items:\n",
    "        s = _normalize_sql_for_print(sql, max_chars=max_chars)\n",
    "        if not s or s == last:\n",
    "            continue\n",
    "        last = s\n",
    "        print(f\"  [step {step}] {label}: {s}\")\n",
    "\n",
    "\n",
    "for sample in test_set[:5]:\n",
    "    nlq = sample[\"nlq\"]\n",
    "    gold = sample[\"sql\"]\n",
    "    pred, trace, decisions = react_sql(\n",
    "        nlq=nlq,\n",
    "        schema_summary=SCHEMA_SUMMARY,\n",
    "        exemplars=REACT_EXEMPLARS,\n",
    "        auto_order=True,\n",
    "    )\n",
    "\n",
    "    print(\"NLQ:\", nlq)\n",
    "    print(\"PRED:\", pred)\n",
    "    print(\"GOLD:\", gold)\n",
    "\n",
    "    if pred:\n",
    "        meta = runner.run(pred, capture_df=False)\n",
    "        print(\"VA:\", int(meta.success), \"ERR:\", meta.error)\n",
    "        ok, why = intent_constraints(nlq, pred)\n",
    "        print(\"INTENT:\", ok, why)\n",
    "    else:\n",
    "        print(\"VA:\", 0, \"ERR:\", \"no prediction\")\n",
    "        print(\"INTENT:\", False, \"no prediction\")\n",
    "\n",
    "    if DEBUG_EX and pred:\n",
    "        ex_ok, pred_err, gold_err = execution_accuracy(engine=engine, pred_sql=pred, gold_sql=gold)\n",
    "        print(\"EX:\", int(ex_ok), \"PRED_ERR:\", pred_err, \"GOLD_ERR:\", gold_err)\n",
    "\n",
    "    mismatch = (pred or \"\").strip().lower() != (gold or \"\").strip().lower()\n",
    "    if trace and (DEBUG_TRACE or (DEBUG_TRACE_ON_MISMATCH and mismatch)):\n",
    "        summary = summarize_trace(trace)\n",
    "        print(\"TRACE LEN:\", len(trace))\n",
    "        print(\"EXECUTED ACTIONS:\", summary.get(\"actions\"))\n",
    "        print(\"BLOCKED STEPS:\", summary.get(\"blocked_steps\"))\n",
    "        print(\"COMPLIANCE:\", summary.get(\"compliance_ok\"), summary.get(\"compliance_errors\"))\n",
    "        print(\"TRACE SUMMARY:\", summary)\n",
    "        print(\"DECISIONS:\\n\" + format_decision_log(decisions, max_items=20))\n",
    "        if DEBUG_SQL_BUILD:\n",
    "            _print_sql_timeline(_collect_sql_timeline(decisions), max_chars=DEBUG_SQL_MAX_CHARS)\n",
    "    else:\n",
    "        print(\"TRACE LEN:\", len(trace))\n",
    "\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89984dc7",
   "metadata": {},
   "source": [
    "### Test Suite Accuracy (TS)\n",
    "\n",
    "TS measures logical correctness by comparing execution outcomes across multiple databases.\n",
    "It is stricter than string match and highlights schema-linking and join errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Test Suite Accuracy (TS) evaluation ===\n",
    "# Harness now lives in nl2sql.eval for reuse in scripts.\n",
    "from nl2sql.eval import test_suite_accuracy_for_item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c61e8a",
   "metadata": {},
   "source": [
    "### Quick toggles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ae08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Quick test toggles (set before full eval) ===\n",
    "# Use small values to sanity-check TS/EX before full runs.\n",
    "\n",
    "QUICK_LIMIT = 20\n",
    "TS_N = 5\n",
    "MAX_ROWS_TS = 500\n",
    "\n",
    "# Canonical ablation toggles\n",
    "USE_LINK_SCHEMA = True\n",
    "USE_CONSTRAINT_POLICY = True\n",
    "USE_REPAIR_POLICY = True\n",
    "REACT_MAX_REPAIRS = 2\n",
    "REACT_LINK_MAX_TABLES = 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f9403",
   "metadata": {},
   "source": [
    "### Full Evaluation (VA/EX/EM/TS)\n",
    "\n",
    "This run computes standard metrics on the full test set and records outputs for later analysis\n",
    "and comparison between system variants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b771d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Full agentic evaluation (VA/EX/EM/TS) over test_set\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from functools import lru_cache\n",
    "from sqlalchemy import Engine\n",
    "\n",
    "\n",
    "def strip_trace_cycles(trace):\n",
    "    # Canonical trace in react_pipeline is acyclic; keep helper for compatibility.\n",
    "    return trace\n",
    "\n",
    "\n",
    "TS_PREFIX = \"classicmodels_ts\"\n",
    "SUITE_DBS = [f\"{TS_PREFIX}_{i:02d}\" for i in range(1, TS_N + 1)]\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=32)\n",
    "def make_engine_cached(db_name: str) -> Engine:\n",
    "    return make_engine(db_name)\n",
    "\n",
    "\n",
    "def make_engine_fn(db_name: str) -> Engine:\n",
    "    return make_engine_cached(db_name)\n",
    "\n",
    "\n",
    "cfg = _react_cfg(name=\"react_eval\")\n",
    "report = evaluate_react_ablation(\n",
    "    test_set=test_set,\n",
    "    engine=engine,\n",
    "    config=cfg,\n",
    "    limit=QUICK_LIMIT,\n",
    "    allow_extra_columns_ex=False,\n",
    "    ts_suite_db_names=SUITE_DBS,\n",
    "    ts_make_engine_fn=make_engine_fn,\n",
    "    ts_max_rows=MAX_ROWS_TS,\n",
    "    progress_every=20,\n",
    ")\n",
    "\n",
    "# Back-compat shape used by existing analysis cells.\n",
    "results = []\n",
    "for item in report[\"items\"]:\n",
    "    trace = strip_trace_cycles(item.get(\"trace\") or [])\n",
    "    decisions = _trace_to_decisions(trace)\n",
    "    results.append(\n",
    "        {\n",
    "            \"nlq\": item[\"nlq\"],\n",
    "            \"gold_sql\": item[\"gold_sql\"],\n",
    "            \"pred_sql\": item[\"pred_sql\"],\n",
    "            \"va\": item[\"va\"],\n",
    "            \"em\": item[\"em\"],\n",
    "            \"ex\": item[\"ex\"],\n",
    "            \"ts\": item.get(\"ts\"),\n",
    "            \"ts_debug\": item.get(\"ts_debug\"),\n",
    "            \"pred_err\": item.get(\"error\"),\n",
    "            \"gold_err\": item.get(\"gold_error\"),\n",
    "            \"trace\": trace,\n",
    "            \"trace_summary\": summarize_trace(trace),\n",
    "            \"decision_log\": decisions,\n",
    "        }\n",
    "    )\n",
    "\n",
    "va_rate = report[\"va_rate\"]\n",
    "em_rate = report[\"em_rate\"]\n",
    "ex_rate = report[\"ex_rate\"]\n",
    "ts_rate = report[\"ts_rate\"] if report[\"ts_rate\"] is not None else 0.0\n",
    "\n",
    "print(\"ReAct VA:\", round(va_rate, 3), \"EX:\", round(ex_rate, 3), \"EM:\", round(em_rate, 3), \"TS:\", round(ts_rate, 3))\n",
    "\n",
    "out = {\n",
    "    \"va_rate\": va_rate,\n",
    "    \"ex_rate\": ex_rate,\n",
    "    \"em_rate\": em_rate,\n",
    "    \"ts_rate\": ts_rate,\n",
    "    \"items\": results,\n",
    "}\n",
    "out_path = Path(\"results/agent/results_react_200.json\")\n",
    "out_path.write_text(json.dumps(out, indent=2, default=str))\n",
    "print(\"Saved to\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 7b) EX failure profiling (quick categories)\n",
    "from collections import Counter\n",
    "\n",
    "def _ex_reasons(decision_log):\n",
    "    reasons = []\n",
    "    for d in decision_log or []:\n",
    "        r = d.get('reason')\n",
    "        if r and r not in ('ok', 'success'):\n",
    "            reasons.append(r)\n",
    "    return reasons\n",
    "\n",
    "def categorize_ex_failure(item):\n",
    "    if not item.get('pred_sql'):\n",
    "        return 'no_pred'\n",
    "    if item.get('va') == 0:\n",
    "        return 'invalid_sql'\n",
    "    if item.get('ex') == 1:\n",
    "        return 'correct'\n",
    "    reasons = _ex_reasons(item.get('decision_log'))\n",
    "    if any('missing_value_hint' in r for r in reasons):\n",
    "        return 'missing_value_hint'\n",
    "    if any('missing_location_table' in r for r in reasons):\n",
    "        return 'missing_location_table'\n",
    "    if any('missing_location_column' in r for r in reasons):\n",
    "        return 'missing_location_column'\n",
    "    if any(r.startswith('missing_agg') for r in reasons):\n",
    "        return 'missing_agg'\n",
    "    if any('missing_group_by' in r for r in reasons):\n",
    "        return 'missing_group_by'\n",
    "    if any('missing_order_by' in r for r in reasons):\n",
    "        return 'missing_order_by'\n",
    "    if any('missing_limit' in r for r in reasons):\n",
    "        return 'missing_limit'\n",
    "    return 'other'\n",
    "\n",
    "counts = Counter(categorize_ex_failure(r) for r in results)\n",
    "print('EX failure categories:')\n",
    "for k, v in counts.most_common():\n",
    "    print(f'  {k}: {v}')\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
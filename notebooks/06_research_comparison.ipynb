{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Comparison Statistics (Base + QLoRA, Llama + Qwen)\n",
    "\n",
    "This notebook runs one script that reads run files from:\n",
    "- `results/baseline/runs/**/results_k*_seed*.json`\n",
    "\n",
    "It includes any valid run matching this matrix:\n",
    "- models: Llama, Qwen\n",
    "- methods: Base, QLoRA\n",
    "- k values used for hypothesis tests: `k=0` and `k=3`\n",
    "\n",
    "Documentation:\n",
    "- [Shapiro-Wilk (SciPy)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html)\n",
    "- [Paired t-test (SciPy)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Run the Stats Script\n",
    "\n",
    "No configuration needed if your run JSON files are in `results/baseline/runs`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"results\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "from scripts.generate_research_comparison import generate\n",
    "\n",
    "RUNS_ROOT = PROJECT_ROOT / \"results\" / \"baseline\" / \"runs\"\n",
    "OUT_DIR = PROJECT_ROOT / \"results\" / \"analysis\"\n",
    "PER_ITEM_CSV = OUT_DIR / \"per_item_metrics_primary_raw.csv\"\n",
    "\n",
    "summary = generate(runs_root=RUNS_ROOT, per_item_csv=PER_ITEM_CSV, out_dir=OUT_DIR)\n",
    "print(json.dumps(summary, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the Output Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "manifest_df = pd.read_csv(OUT_DIR / \"run_manifest.csv\")\n",
    "shapiro_df = pd.read_csv(OUT_DIR / \"stats_mean_median_shapiro.csv\")\n",
    "ttest_df = pd.read_csv(OUT_DIR / \"stats_paired_ttests.csv\")\n",
    "\n",
    "print(\"Run manifest:\")\n",
    "display(manifest_df)\n",
    "\n",
    "print(\"\\nMean / Median / Shapiro:\")\n",
    "display(shapiro_df.sort_values([\"condition_id\", \"seed\", \"metric\"]).reset_index(drop=True))\n",
    "\n",
    "print(\"\\nPaired t-tests:\")\n",
    "display(ttest_df.sort_values([\"comparison\", \"metric\"]).reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Plain-Language Hypothesis Results (EX only)\n",
    "\n",
    "Decision rule:\n",
    "- if `p_value < 0.05`: reject `H0`\n",
    "- else: fail to reject `H0`\n",
    "\n",
    "Each row in `stats_paired_ttests.csv` is one predefined hypothesis comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "ex_rows = ttest_df[ttest_df[\"metric\"] == \"ex\"].copy()\n",
    "\n",
    "if ex_rows.empty:\n",
    "    print(\"No EX comparisons available yet. Add matching run pairs first.\")\n",
    "else:\n",
    "    for _, row in ex_rows.sort_values(\"comparison\").iterrows():\n",
    "        delta = row[\"mean_diff_right_minus_left\"]\n",
    "        pval = row[\"p_value\"]\n",
    "        decision = row[\"decision_alpha_0_05\"]\n",
    "        direction = \"improved\" if pd.notna(delta) and delta > 0 else \"decreased\"\n",
    "        sig_text = \"significant\" if decision == \"reject_H0\" else \"not significant\"\n",
    "\n",
    "        print(f\"Comparison: {row['comparison']}\")\n",
    "        if pd.isna(delta):\n",
    "            print(\"  EX delta: unavailable (insufficient matched pairs)\")\n",
    "        else:\n",
    "            print(f\"  EX {direction} by {abs(delta):.3f}\")\n",
    "        print(f\"  p-value: {pval:.4g} ({sig_text})\")\n",
    "        print(f\"  Decision: {decision}\")\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f2b652",
   "metadata": {},
   "source": [
    "# Agentic Evaluation (Tool-Driven ReAct Loop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef67421",
   "metadata": {},
   "source": [
    "### Setup (Colab only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b144d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "set -e\n",
    "export PIP_DEFAULT_TIMEOUT=120\n",
    "\n",
    "# Clean conflicting preinstalls\n",
    "pip uninstall -y torch torchvision torchaudio bitsandbytes triton transformers accelerate peft trl datasets numpy pandas fsspec requests google-auth || true\n",
    "\n",
    "# Base deps\n",
    "pip install -q --no-cache-dir --force-reinstall   numpy==1.26.4 pandas==2.2.1 fsspec==2024.5.0 requests==2.31.0 google-auth==2.43.0\n",
    "\n",
    "# Torch + CUDA 12.1\n",
    "pip install -q --no-cache-dir --force-reinstall   torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121   --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# bitsandbytes + triton + HF stack\n",
    "pip install -q --no-cache-dir --force-reinstall   bitsandbytes==0.43.3 triton==2.3.1   transformers==4.44.2 accelerate==0.33.0 peft==0.17.0 trl==0.9.6 datasets==2.20.0\n",
    "\n",
    "echo \"Setup complete. Restart runtime once, then run the rest of the notebook top-to-bottom.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b6008",
   "metadata": {},
   "source": [
    "### Repo setup (Colab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47936fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Clone repo (Colab) + install deps\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not os.path.exists('/content/NLtoSQL'):\n",
    "        !git clone https://github.com/MacKenzieOBrian/NLtoSQL.git /content/NLtoSQL\n",
    "    %cd /content/NLtoSQL\n",
    "    !pip -q install -r requirements.txt\n",
    "    import torch, transformers, accelerate, peft\n",
    "    print('torch', torch.__version__, 'cuda', torch.cuda.is_available())\n",
    "else:\n",
    "    print('Not in Colab; using existing workspace')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58d00e",
   "metadata": {},
   "source": [
    "### Optional: ADC auth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this only if you prefer gcloud-based ADC (no JSON key)\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    %pip install -q --upgrade google-auth google-auth-oauthlib\n",
    "    !gcloud auth application-default login\n",
    "else:\n",
    "    print(\"Not in Colab; skip gcloud auth.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada87478",
   "metadata": {},
   "source": [
    "### Database Connection and Query Execution Layer\n",
    "\n",
    "This block establishes a reproducible connection to ClassicModels and initializes the query runner\n",
    "used for validation and execution accuracy. It ensures all downstream metrics are computed\n",
    "against a consistent database state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Environment + DB\n",
    "from getpass import getpass\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "from nl2sql.db import create_engine_with_connector, safe_connection\n",
    "\n",
    "# Expected env vars (set these in a Colab cell):\n",
    "# INSTANCE_CONNECTION_NAME, DB_USER, DB_PASS, DB_NAME\n",
    "INSTANCE_CONNECTION_NAME = os.getenv(\"INSTANCE_CONNECTION_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\") or \"classicmodels\"\n",
    "\n",
    "if not INSTANCE_CONNECTION_NAME:\n",
    "    INSTANCE_CONNECTION_NAME = input(\"Enter INSTANCE_CONNECTION_NAME: \").strip()\n",
    "if not DB_USER:\n",
    "    DB_USER = input(\"Enter DB_USER: \").strip()\n",
    "if not DB_PASS:\n",
    "    DB_PASS = getpass(\"Enter DB_PASS: \")\n",
    "\n",
    "# Canonical engine builder (shared with scripts + other notebooks).\n",
    "# Uses Cloud SQL Connector under the hood and ADC for credentials.\n",
    "engine, connector = create_engine_with_connector(\n",
    "    instance_connection_name=INSTANCE_CONNECTION_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASS,\n",
    "    db_name=DB_NAME,\n",
    ")\n",
    "\n",
    "with safe_connection(engine) as conn:\n",
    "    conn.execute(text(\"SELECT 1\"))\n",
    "print(\"DB connection OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9a6fb",
   "metadata": {},
   "source": [
    "### TS engine factory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b) Engine factory for TS (multiple DB names)\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "\n",
    "def make_engine(db_name: str) -> Engine:\n",
    "    \"\"\"Create a new engine bound to a specific TS replica DB name.\n",
    "\n",
    "    TS (test-suite accuracy) executes the same (gold, pred) SQL across multiple\n",
    "    replica databases (classicmodels_ts_XX). We keep separate engines so each\n",
    "    replica is evaluated independently.\n",
    "    \"\"\"\n",
    "\n",
    "    def getconn_for_db():\n",
    "        return connector.connect(\n",
    "            INSTANCE_CONNECTION_NAME,\n",
    "            \"pymysql\",\n",
    "            user=DB_USER,\n",
    "            password=DB_PASS,\n",
    "            db=db_name,\n",
    "        )\n",
    "\n",
    "    return sqlalchemy.create_engine(\"mysql+pymysql://\", creator=getconn_for_db, future=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6cfbfc",
   "metadata": {},
   "source": [
    "### Schema Summary and Test Set\n",
    "\n",
    "We build a compact schema summary for prompt grounding and load the evaluation set used across runs.\n",
    "This keeps the context small while preserving the tables and columns required for correct SQL synthesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Schema summary + test set + QueryRunner\n",
    "\n",
    "QUICK_LIMIT = 20  # number of NLQs to evaluate (set None for full set)\n",
    "import json\n",
    "from pathlib import Path\n",
    "from nl2sql.schema import build_schema_summary\n",
    "from nl2sql.query_runner import QueryRunner\n",
    "\n",
    "DB_NAME = globals().get(\"DB_NAME\") or os.getenv(\"DB_NAME\") or \"classicmodels\"\n",
    "SCHEMA_SUMMARY = build_schema_summary(engine, db_name=DB_NAME)\n",
    "test_path = Path(\"data/classicmodels_test_200.json\")\n",
    "full_set = json.loads(test_path.read_text(encoding=\"utf-8\"))\n",
    "test_set = full_set  # slice later via QUICK_LIMIT\n",
    "print(\"Loaded test set size:\", len(test_set))\n",
    "\n",
    "# Small exemplar set (seeded to encourage join behavior).\n",
    "join_exemplars = [it for it in full_set if \"office\" in it[\"nlq\"].lower()]\n",
    "REACT_EXEMPLARS = []\n",
    "if join_exemplars:\n",
    "    REACT_EXEMPLARS.append(join_exemplars[0])\n",
    "for it in full_set:\n",
    "    if it not in REACT_EXEMPLARS:\n",
    "        REACT_EXEMPLARS.append(it)\n",
    "    if len(REACT_EXEMPLARS) >= 3:\n",
    "        break\n",
    "print(\"Exemplars:\", [e[\"nlq\"] for e in REACT_EXEMPLARS])\n",
    "\n",
    "runner = QueryRunner(engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b3611c",
   "metadata": {},
   "source": [
    "### Load model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f38fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Load model (base or QLoRA adapters)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "ADAPTER_PATH = os.getenv(\"ADAPTER_PATH\") or \"results/adapters/qlora_classicmodels\"  # set to None to use base model\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if not HF_TOKEN:\n",
    "    HF_TOKEN = getpass(\"Enter HF_TOKEN (https://huggingface.co/settings/tokens): \").strip()\n",
    "\n",
    "cc_major, cc_minor = torch.cuda.get_device_capability(0) if torch.cuda.is_available() else (0, 0)\n",
    "use_bf16 = cc_major >= 8\n",
    "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"Using bf16:\", use_bf16)\n",
    "print(\"Adapter path:\", ADAPTER_PATH)\n",
    "\n",
    "# Tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# Quantized base model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map={\"\": 0} if torch.cuda.is_available() else None,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "base_model.generation_config.do_sample = False\n",
    "base_model.generation_config.temperature = 1.0\n",
    "base_model.generation_config.top_p = 1.0\n",
    "\n",
    "# Load adapters if present locally; otherwise use base model\n",
    "adapter_dir = Path(ADAPTER_PATH) if ADAPTER_PATH else None\n",
    "if adapter_dir and adapter_dir.exists():\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_dir, token=HF_TOKEN)\n",
    "    print(\"Loaded adapters from\", adapter_dir)\n",
    "else:\n",
    "    print(\"Adapter path missing; using base model only. Set ADAPTER_PATH to your local adapter folder or upload it to Colab.\")\n",
    "    model = base_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7065790e",
   "metadata": {},
   "source": [
    "### Optional: smoke check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fbae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nl2sql.prompting import make_few_shot_messages\n",
    "from nl2sql.llm import extract_first_select\n",
    "from nl2sql.postprocess import guarded_postprocess\n",
    "from nl2sql.eval import execution_accuracy\n",
    "from nl2sql.constraint_policy import build_constraints\n",
    "import random\n",
    "\n",
    "runner_check = QueryRunner(engine)\n",
    "exemplars = test_set[:3]\n",
    "\n",
    "\n",
    "def run_quick_check(k: int = 0, limit: int = 3, seed: int = 7) -> None:\n",
    "    \"\"\"Small smoke test before full runs.\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    pool = test_set if limit is None else rng.sample(test_set, k=min(limit, len(test_set)))\n",
    "    print(f\"Quick check | k={k} | n={len(pool)}\")\n",
    "\n",
    "    for sample in pool:\n",
    "        shots = exemplars if k > 0 else []\n",
    "        messages = make_few_shot_messages(\n",
    "            schema=SCHEMA_SUMMARY,\n",
    "            exemplars=shots,\n",
    "            nlq=sample['nlq'],\n",
    "        )\n",
    "        prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tok(prompt, return_tensors='pt').to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "\n",
    "        gen_ids = out[0][inputs.input_ids.shape[-1]:]\n",
    "        text = tok.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "        raw_sql = extract_first_select(text) or text\n",
    "        constraints = build_constraints(sample['nlq'], SCHEMA_SUMMARY)\n",
    "        explicit_fields = constraints.get('explicit_fields')\n",
    "        explicit_projection = constraints.get('explicit_projection')\n",
    "        required_fields = constraints.get('required_output_fields')\n",
    "\n",
    "        sql = guarded_postprocess(\n",
    "            raw_sql,\n",
    "            sample['nlq'],\n",
    "            explicit_fields=explicit_fields if explicit_projection else None,\n",
    "            required_fields=required_fields,\n",
    "        )\n",
    "\n",
    "        meta = runner_check.run(sql, capture_df=False)\n",
    "        ex_ok, _, _ = execution_accuracy(engine=engine, pred_sql=sql, gold_sql=sample['sql'])\n",
    "\n",
    "        print(f\"Q: {sample['nlq']}\")\n",
    "        print(f\"SQL: {sql}\")\n",
    "        print(f\"VA: {int(meta.success)} EX: {int(ex_ok)}\")\n",
    "        if meta.error:\n",
    "            print(f\"ERR: {meta.error}\")\n",
    "        print('-' * 80)\n",
    "\n",
    "\n",
    "run_quick_check(k=0, limit=3)\n",
    "run_quick_check(k=3, limit=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f4b1f",
   "metadata": {},
   "source": [
    "### Guardrails and Post-Processing Utilities\n",
    "\n",
    "Guardrails provide lightweight correctness constraints (SELECT-only, schema checks, and\n",
    "intent checks) that prevent \"runs but wrong\" SQL. Projection cleanup is conservative:\n",
    "explicit projection is enforced only when the NLQ enumerates fields (or a high-precision\n",
    "template requires it), and minimal-projection pruning is skipped when constraints require\n",
    "specific output fields. This keeps post-processing from changing query semantics while\n",
    "still reducing common formatting noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b07b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Agent utilities + guardrails\n",
    "import importlib\n",
    "import nl2sql.agent_utils as agent_utils\n",
    "import nl2sql.postprocess as postprocess\n",
    "import nl2sql.eval as eval_mod\n",
    "importlib.reload(agent_utils)\n",
    "importlib.reload(postprocess)\n",
    "importlib.reload(eval_mod)\n",
    "\n",
    "from nl2sql.agent_utils import (\n",
    "    _extract_required_columns,\n",
    "    intent_constraints,\n",
    "    classify_intent,\n",
    "    clean_candidate_with_reason,\n",
    ")\n",
    "from nl2sql.postprocess import guarded_postprocess\n",
    "from nl2sql.eval import execution_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952b8ec",
   "metadata": {},
   "source": [
    "## Tool-Driven ReAct (Interactive Evaluation Harness)\n",
    "\n",
    "This section is refactored as an experiment control surface so you can quickly test code changes,\n",
    "inspect step-by-step traces, and run reproducible metric evaluations from one place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d555900",
   "metadata": {},
   "source": [
    "### Why These Tools Exist (and Where They Come From)\n",
    "\n",
    "The loop uses explicit actions defined in `nl2sql/agent_tools.py` and orchestrated by\n",
    "`nl2sql/react_pipeline.py`.\n",
    "\n",
    "Design motivation:\n",
    "- ReAct action-observation control (`REFERENCES.md#ref-yao2023-react`)\n",
    "- Execution-guided repair and rejection (`REFERENCES.md#ref-wang2018-eg-decoding`)\n",
    "- Constraint-first validation (`REFERENCES.md#ref-scholak2021-picard`)\n",
    "- Schema/value linking pressure points (`REFERENCES.md#ref-wang2020-ratsql`, `REFERENCES.md#ref-li2023-resdsql`, `REFERENCES.md#ref-lin2020-bridge`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Controls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5a) Runtime controls + module binding\n",
    "\n",
    "RELOAD_REACT_MODULES = True\n",
    "RUN_NAME = 'react_core_notebook'\n",
    "\n",
    "# Core loop toggles (keep minimal by default)\n",
    "USE_LINK_SCHEMA = True\n",
    "USE_CONSTRAINT_POLICY = True\n",
    "USE_REPAIR_POLICY = True\n",
    "USE_INTENT_GATE = False\n",
    "STOP_ON_FIRST_SUCCESS = True\n",
    "REACT_MAX_REPAIRS = 2\n",
    "REACT_LINK_MAX_TABLES = 6\n",
    "\n",
    "# Single-question walkthrough controls\n",
    "DEMO_INTERACTIVE = False  # set True to type your own NLQ\n",
    "DEMO_DEFAULT_NLQ = 'Which customers are in France?'\n",
    "DEMO_DEBUG = True\n",
    "DEMO_SHOW_DECISIONS = True\n",
    "\n",
    "# Batch smoke controls\n",
    "BATCH_SAMPLE_SIZE = 5\n",
    "BATCH_RANDOM_SEED = 7\n",
    "BATCH_DEBUG_ON_MISMATCH = True\n",
    "DEBUG_SQL_MAX_CHARS = 220\n",
    "\n",
    "# Full evaluation controls\n",
    "EVAL_LIMIT = QUICK_LIMIT if 'QUICK_LIMIT' in globals() else 20\n",
    "TS_N = 5\n",
    "TS_PREFIX = 'classicmodels_ts'\n",
    "MAX_ROWS_TS = 500\n",
    "EVAL_OUTPUT_PATH = 'results/agent/results_react_200.json'\n",
    "\n",
    "import importlib\n",
    "import nl2sql.agent_tools as agent_tools\n",
    "import nl2sql.react_pipeline as react_pipeline\n",
    "\n",
    "if RELOAD_REACT_MODULES:\n",
    "    importlib.reload(agent_tools)\n",
    "    importlib.reload(react_pipeline)\n",
    "\n",
    "from nl2sql.agent_tools import AgentContext, set_agent_context\n",
    "from nl2sql.react_pipeline import (\n",
    "    ReactAblationConfig,\n",
    "    core_react_config,\n",
    "    run_react_pipeline,\n",
    "    evaluate_react_ablation,\n",
    ")\n",
    "\n",
    "set_agent_context(\n",
    "    AgentContext(\n",
    "        engine=engine,\n",
    "        db_name=DB_NAME,\n",
    "        model=model,\n",
    "        tok=tok,\n",
    "        runner=runner,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    ")\n",
    "\n",
    "print('ReAct context bound.')\n",
    "print('Run name:', RUN_NAME)\n",
    "print('Eval limit:', EVAL_LIMIT, '| TS replicas:', TS_N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Justification Matrix\n",
    "\n",
    "Use this table in viva/write-up to explain each tool in one sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5b) Tool justifications (purpose + code source + research anchor)\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "tool_rows = [\n",
    "    {\n",
    "        'tool': 'get_schema',\n",
    "        'why_used': 'Ground generation in real tables/columns and avoid hallucinated schema.',\n",
    "        'source': 'nl2sql/agent_tools.py:get_schema',\n",
    "        'research_anchor': 'Schema-grounded parsing; Spider setup (ref-yu2018-spider)',\n",
    "    },\n",
    "    {\n",
    "        'tool': 'link_schema',\n",
    "        'why_used': 'Reduce prompt scope to relevant tables before generation.',\n",
    "        'source': 'nl2sql/agent_tools.py:link_schema',\n",
    "        'research_anchor': 'RAT-SQL / RESDSQL schema linking (ref-wang2020-ratsql, ref-li2023-resdsql)',\n",
    "    },\n",
    "    {\n",
    "        'tool': 'extract_constraints',\n",
    "        'why_used': 'Convert NL intent into structural checks (agg/order/limit/projection hints).',\n",
    "        'source': 'nl2sql/agent_tools.py:extract_constraints',\n",
    "        'research_anchor': 'Constraint-first decoding and validation (ref-scholak2021-picard)',\n",
    "    },\n",
    "    {\n",
    "        'tool': 'generate_sql',\n",
    "        'why_used': 'Produce initial SQL candidate from NLQ + focused schema + constraints.',\n",
    "        'source': 'nl2sql/agent_tools.py:generate_sql',\n",
    "        'research_anchor': 'LLM generation baseline for text-to-SQL (ref-zhu2024-survey)',\n",
    "    },\n",
    "    {\n",
    "        'tool': 'validate_sql',\n",
    "        'why_used': 'Block invalid schema references and malformed SQL before execution.',\n",
    "        'source': 'nl2sql/agent_tools.py:validate_sql',\n",
    "        'research_anchor': 'Validity gating before runtime execution (ref-scholak2021-picard)',\n",
    "    },\n",
    "    {\n",
    "        'tool': 'validate_constraints',\n",
    "        'why_used': 'Catch \"runs-but-wrong-shape\" SQL early (missing grouping/order/fields).',\n",
    "        'source': 'nl2sql/agent_tools.py:validate_constraints',\n",
    "        'research_anchor': 'Semantic structure checks for execution reliability (ref-zhai2025-excot)',\n",
    "    },\n",
    "    {\n",
    "        'tool': 'run_sql',\n",
    "        'why_used': 'Provide execution observation used for VA and repair triggers.',\n",
    "        'source': 'nl2sql/agent_tools.py:run_sql',\n",
    "        'research_anchor': 'Execution-guided feedback (ref-wang2018-eg-decoding)',\n",
    "    },\n",
    "    {\n",
    "        'tool': 'repair_sql',\n",
    "        'why_used': 'Revise SQL only after concrete validation/execution failure evidence.',\n",
    "        'source': 'nl2sql/agent_tools.py:repair_sql',\n",
    "        'research_anchor': 'Feedback-driven correction loops (ref-yao2023-react, ref-zhai2025-excot)',\n",
    "    },\n",
    "]\n",
    "\n",
    "tool_justification_df = pd.DataFrame(tool_rows)\n",
    "display(tool_justification_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct Wrapper and Trace Helpers\n",
    "\n",
    "These helpers make experiments repeatable and easier to inspect after each code change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5c) Notebook wrapper around module-level ReAct\n",
    "\n",
    "from nl2sql.postprocess import normalize_sql\n",
    "\n",
    "\n",
    "def _react_cfg(name: str | None = None) -> ReactAblationConfig:\n",
    "    base = core_react_config(name=name or RUN_NAME)\n",
    "    return ReactAblationConfig(\n",
    "        name=base.name,\n",
    "        use_schema_link=USE_LINK_SCHEMA,\n",
    "        use_constraint_policy=USE_CONSTRAINT_POLICY,\n",
    "        use_repair_policy=USE_REPAIR_POLICY,\n",
    "        use_intent_gate=USE_INTENT_GATE,\n",
    "        stop_on_first_success=STOP_ON_FIRST_SUCCESS,\n",
    "        max_repairs=REACT_MAX_REPAIRS,\n",
    "        link_max_tables=REACT_LINK_MAX_TABLES,\n",
    "    )\n",
    "\n",
    "\n",
    "def _trace_to_decisions(trace: list[dict]) -> list[dict]:\n",
    "    decisions = []\n",
    "    for i, t in enumerate(trace or []):\n",
    "        stage = t.get('stage')\n",
    "        if not stage:\n",
    "            continue\n",
    "        entry = {'step': i, 'decision': stage, 'status': 'ok', 'reason': 'ok', 'data': {}}\n",
    "\n",
    "        if stage in ('validate_sql', 'validate_constraints'):\n",
    "            res = t.get('result') or {}\n",
    "            entry['status'] = 'ok' if res.get('valid') else 'reject'\n",
    "            entry['reason'] = res.get('reason', 'ok')\n",
    "            entry['data'] = dict(res)\n",
    "            if t.get('sql'):\n",
    "                entry['data']['sql'] = t.get('sql')\n",
    "        elif stage == 'run_sql':\n",
    "            res = t.get('result') or {}\n",
    "            entry['status'] = 'ok' if res.get('success') else 'reject'\n",
    "            entry['reason'] = 'execution'\n",
    "            entry['data'] = {\n",
    "                'sql': t.get('sql'),\n",
    "                'success': res.get('success'),\n",
    "                'rowcount': res.get('rowcount'),\n",
    "                'error': res.get('error'),\n",
    "            }\n",
    "        elif stage == 'repair_sql':\n",
    "            entry['reason'] = t.get('reason', 'repair')\n",
    "            entry['data'] = {\n",
    "                'raw_sql': t.get('raw_sql'),\n",
    "                'cleaned_sql': t.get('sql_after_guardrails'),\n",
    "                'repair_count': t.get('repair_count'),\n",
    "            }\n",
    "        else:\n",
    "            entry['data'] = {k: v for k, v in t.items() if k != 'stage'}\n",
    "\n",
    "        decisions.append(entry)\n",
    "    return decisions\n",
    "\n",
    "\n",
    "def summarize_trace(trace: list[dict]) -> dict:\n",
    "    actions = [t.get('stage') for t in (trace or []) if t.get('stage')]\n",
    "    repairs = sum(1 for a in actions if a == 'repair_sql')\n",
    "    return {\n",
    "        'actions': actions,\n",
    "        'repairs': repairs,\n",
    "        'first_success_stop': STOP_ON_FIRST_SUCCESS,\n",
    "        'intent_gate': USE_INTENT_GATE,\n",
    "    }\n",
    "\n",
    "\n",
    "def format_decision_log(decisions: list[dict], max_items: int = 25) -> str:\n",
    "    if not decisions:\n",
    "        return '(no decisions)'\n",
    "    out = []\n",
    "    for d in decisions[:max_items]:\n",
    "        out.append(f\"[step {d['step']}] {d['decision']} -> {d['status']} ({d['reason']})\")\n",
    "        if d.get('data'):\n",
    "            text = str(d['data'])\n",
    "            if len(text) > 320:\n",
    "                text = text[:317] + '...'\n",
    "            out.append(f'  data: {text}')\n",
    "    return '\\n'.join(out)\n",
    "\n",
    "\n",
    "def react_sql(nlq: str, debug: bool = False) -> tuple[str, list[dict], list[dict]]:\n",
    "    cfg = _react_cfg()\n",
    "    pred_sql, trace = run_react_pipeline(nlq=nlq, config=cfg)\n",
    "    decisions = _trace_to_decisions(trace)\n",
    "\n",
    "    if debug:\n",
    "        print('Trace summary:', summarize_trace(trace))\n",
    "        print(format_decision_log(decisions, max_items=40))\n",
    "\n",
    "    return pred_sql, trace, decisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0cb3f3",
   "metadata": {},
   "source": [
    "## Interactive Walkthrough (Single NLQ)\n",
    "\n",
    "Use this cell while developing new guardrails/constraints to inspect behavior step-by-step.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b8b3c4fd",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 6a) Single-question interactive walkthrough\n",
    "\n",
    "from nl2sql.constraint_policy import build_constraints\n",
    "\n",
    "nlq = ''\n",
    "if DEMO_INTERACTIVE:\n",
    "    try:\n",
    "        nlq = input('Type a ClassicModels question (blank uses default): ').strip()\n",
    "    except Exception:\n",
    "        nlq = ''\n",
    "if not nlq:\n",
    "    nlq = DEMO_DEFAULT_NLQ\n",
    "\n",
    "print('NLQ:', nlq)\n",
    "pred, trace, decisions = react_sql(nlq=nlq, debug=DEMO_DEBUG)\n",
    "\n",
    "print('\\nFINAL SQL:')\n",
    "print(pred or '(no prediction)')\n",
    "print('\\nTRACE SUMMARY:', summarize_trace(trace))\n",
    "\n",
    "if pred:\n",
    "    meta = runner.run(pred, capture_df=False)\n",
    "    print('VA:', int(meta.success), '| Error:', meta.error)\n",
    "else:\n",
    "    print('VA: 0 | Error: no prediction')\n",
    "\n",
    "constraints = build_constraints(nlq, SCHEMA_SUMMARY)\n",
    "interesting_keys = [\n",
    "    'explicit_projection',\n",
    "    'required_output_fields',\n",
    "    'required_tables',\n",
    "    'needs_location',\n",
    "    'value_hints',\n",
    "]\n",
    "print('Constraints:', {k: constraints.get(k) for k in interesting_keys})\n",
    "\n",
    "if DEMO_SHOW_DECISIONS:\n",
    "    print('\\nDECISIONS:\\n' + format_decision_log(decisions, max_items=50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Smoke Test (Fast Iteration)\n",
    "\n",
    "Run this after each pipeline change before full evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b) Batch smoke test on random items\n",
    "\n",
    "import random\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "from nl2sql.constraint_policy import build_constraints\n",
    "from nl2sql.validation import validate_constraints\n",
    "from nl2sql.postprocess import normalize_sql\n",
    "from nl2sql.eval import execution_accuracy\n",
    "\n",
    "\n",
    "def _short_sql(sql: str | None, max_chars: int = 220) -> str:\n",
    "    s = ' '.join(str(sql or '').split())\n",
    "    if max_chars and len(s) > max_chars:\n",
    "        s = s[:max_chars - 3] + '...'\n",
    "    return s\n",
    "\n",
    "\n",
    "rng = random.Random(BATCH_RANDOM_SEED)\n",
    "samples = rng.sample(test_set, k=min(BATCH_SAMPLE_SIZE, len(test_set)))\n",
    "rows = []\n",
    "\n",
    "for sample in samples:\n",
    "    nlq = sample['nlq']\n",
    "    gold = sample['sql']\n",
    "    pred, trace, decisions = react_sql(nlq=nlq, debug=False)\n",
    "\n",
    "    if pred:\n",
    "        meta = runner.run(pred, capture_df=False)\n",
    "        va = int(meta.success)\n",
    "        ex_ok, pred_err, gold_err = execution_accuracy(\n",
    "            engine=engine,\n",
    "            pred_sql=pred,\n",
    "            gold_sql=gold,\n",
    "            allow_extra_columns=False,\n",
    "        )\n",
    "        ex = int(ex_ok)\n",
    "        error = meta.error or pred_err\n",
    "    else:\n",
    "        va = 0\n",
    "        ex = 0\n",
    "        error = 'no_prediction'\n",
    "        gold_err = None\n",
    "\n",
    "    em = int(normalize_sql(pred or '') == normalize_sql(gold or ''))\n",
    "\n",
    "    constraints = build_constraints(nlq, SCHEMA_SUMMARY)\n",
    "    v_constraints = (\n",
    "        validate_constraints(pred, constraints, schema_text=SCHEMA_SUMMARY)\n",
    "        if pred\n",
    "        else {'valid': False, 'reason': 'no_prediction'}\n",
    "    )\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            'nlq': nlq,\n",
    "            'va': va,\n",
    "            'em': em,\n",
    "            'ex': ex,\n",
    "            'pred_sql': _short_sql(pred, DEBUG_SQL_MAX_CHARS),\n",
    "            'gold_sql': _short_sql(gold, DEBUG_SQL_MAX_CHARS),\n",
    "            'error': error,\n",
    "            'constraint_valid': int(bool(v_constraints.get('valid'))),\n",
    "            'constraint_reason': v_constraints.get('reason'),\n",
    "            'repairs': summarize_trace(trace).get('repairs', 0),\n",
    "            'decision_log': decisions,\n",
    "            'trace_summary': summarize_trace(trace),\n",
    "            'gold_error': gold_err,\n",
    "        }\n",
    "    )\n",
    "\n",
    "smoke_df = pd.DataFrame(rows)\n",
    "display(smoke_df[['nlq', 'va', 'em', 'ex', 'constraint_valid', 'repairs', 'error']])\n",
    "\n",
    "print('Smoke metrics:')\n",
    "print({\n",
    "    'n': len(smoke_df),\n",
    "    'va_rate': round(float(smoke_df['va'].mean()), 3),\n",
    "    'em_rate': round(float(smoke_df['em'].mean()), 3),\n",
    "    'ex_rate': round(float(smoke_df['ex'].mean()), 3),\n",
    "})\n",
    "\n",
    "if BATCH_DEBUG_ON_MISMATCH:\n",
    "    mismatches = smoke_df[(smoke_df['ex'] == 0) | (smoke_df['va'] == 0)]\n",
    "    for _, row in mismatches.iterrows():\n",
    "        print('\\nNLQ:', row['nlq'])\n",
    "        print('PRED:', row['pred_sql'])\n",
    "        print('GOLD:', row['gold_sql'])\n",
    "        print('ERR:', row['error'])\n",
    "        print('DECISIONS:\\n' + format_decision_log(row['decision_log'], max_items=25))\n",
    "        print('-' * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89984dc7",
   "metadata": {},
   "source": [
    "### Test Suite Accuracy (TS)\n",
    "\n",
    "TS checks semantic consistency across perturbed database replicas and reduces lucky single-DB matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6c) TS harness\n",
    "from nl2sql.eval import test_suite_accuracy_for_item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c61e8a",
   "metadata": {},
   "source": [
    "### Full Evaluation Controls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ae08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7a) Full evaluation runtime config\n",
    "\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "\n",
    "def _git_commit_short() -> str | None:\n",
    "    try:\n",
    "        return subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'], text=True).strip()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "TS_PREFIX = TS_PREFIX if 'TS_PREFIX' in globals() else 'classicmodels_ts'\n",
    "SUITE_DBS = [f\"{TS_PREFIX}_{i:02d}\" for i in range(1, TS_N + 1)]\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=32)\n",
    "def make_engine_cached(db_name: str) -> Engine:\n",
    "    return make_engine(db_name)\n",
    "\n",
    "\n",
    "def make_engine_fn(db_name: str) -> Engine:\n",
    "    return make_engine_cached(db_name)\n",
    "\n",
    "\n",
    "RUN_METADATA = {\n",
    "    'notebook': '03_agentic_eval.ipynb',\n",
    "    'model_id': MODEL_ID,\n",
    "    'adapter_path': ADAPTER_PATH,\n",
    "    'commit': _git_commit_short(),\n",
    "    'run_name': RUN_NAME,\n",
    "}\n",
    "\n",
    "print('Evaluation config:')\n",
    "print('  limit:', EVAL_LIMIT)\n",
    "print('  ts_dbs:', SUITE_DBS)\n",
    "print('  output:', EVAL_OUTPUT_PATH)\n",
    "print('  metadata:', RUN_METADATA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f9403",
   "metadata": {},
   "source": [
    "### Full Evaluation (VA/EX/EM/TS)\n",
    "\n",
    "This run is the reproducible report path used for dissertation tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b771d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7b) Full evaluation run\n",
    "\n",
    "import json\n",
    "\n",
    "cfg = _react_cfg(name=RUN_NAME)\n",
    "report = evaluate_react_ablation(\n",
    "    test_set=test_set,\n",
    "    engine=engine,\n",
    "    config=cfg,\n",
    "    limit=EVAL_LIMIT,\n",
    "    allow_extra_columns_ex=False,\n",
    "    ts_suite_db_names=SUITE_DBS,\n",
    "    ts_make_engine_fn=make_engine_fn,\n",
    "    ts_max_rows=MAX_ROWS_TS,\n",
    "    progress_every=20,\n",
    "    seed=BATCH_RANDOM_SEED,\n",
    "    run_metadata=RUN_METADATA,\n",
    ")\n",
    "\n",
    "# Backward-compatible shape for existing analysis scripts/cells.\n",
    "results = []\n",
    "for item in report['items']:\n",
    "    trace = item.get('trace') or []\n",
    "    decisions = _trace_to_decisions(trace)\n",
    "    results.append(\n",
    "        {\n",
    "            'nlq': item['nlq'],\n",
    "            'gold_sql': item['gold_sql'],\n",
    "            'pred_sql': item['pred_sql'],\n",
    "            'va': item['va'],\n",
    "            'em': item['em'],\n",
    "            'ex': item['ex'],\n",
    "            'ts': item.get('ts'),\n",
    "            'ts_debug': item.get('ts_debug'),\n",
    "            'pred_err': item.get('error'),\n",
    "            'gold_err': item.get('gold_error'),\n",
    "            'trace': trace,\n",
    "            'trace_summary': summarize_trace(trace),\n",
    "            'decision_log': decisions,\n",
    "        }\n",
    "    )\n",
    "\n",
    "out = {\n",
    "    **report,\n",
    "    'items': results,\n",
    "}\n",
    "\n",
    "out_path = Path(EVAL_OUTPUT_PATH)\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "out_path.write_text(json.dumps(out, indent=2, default=str), encoding='utf-8')\n",
    "\n",
    "ts_rate = report['ts_rate'] if report.get('ts_rate') is not None else 0.0\n",
    "print(\n",
    "    f\"{RUN_NAME} | n={report['n']} | \"\n",
    "    f\"VA={report['va_rate']:.3f} EM={report['em_rate']:.3f} \"\n",
    "    f\"EX={report['ex_rate']:.3f} TS={ts_rate:.3f}\"\n",
    ")\n",
    "print('Saved to', out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 7c) EX failure profiling (quick categories)\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _decision_reasons(decision_log: list[dict]) -> list[str]:\n",
    "    reasons = []\n",
    "    for d in decision_log or []:\n",
    "        r = d.get('reason')\n",
    "        if r and r not in ('ok', 'execution', 'stop_on_first_success'):\n",
    "            reasons.append(r)\n",
    "    return reasons\n",
    "\n",
    "\n",
    "def categorize_ex_failure(item: dict) -> str:\n",
    "    if not item.get('pred_sql'):\n",
    "        return 'no_prediction'\n",
    "    if int(item.get('va', 0)) == 0:\n",
    "        return 'invalid_sql'\n",
    "    if int(item.get('ex', 0)) == 1:\n",
    "        return 'correct'\n",
    "\n",
    "    reasons = _decision_reasons(item.get('decision_log') or [])\n",
    "    if any('missing_value_hint' in r for r in reasons):\n",
    "        return 'value_linking'\n",
    "    if any('missing_location' in r for r in reasons):\n",
    "        return 'location_linking'\n",
    "    if any('missing_agg' in r for r in reasons):\n",
    "        return 'aggregation'\n",
    "    if any('missing_group_by' in r for r in reasons):\n",
    "        return 'grouping'\n",
    "    if any('missing_order_by' in r or 'missing_limit' in r for r in reasons):\n",
    "        return 'ordering_limit'\n",
    "    if any('join' in r for r in reasons):\n",
    "        return 'join_path'\n",
    "    return 'other_semantic'\n",
    "\n",
    "\n",
    "failure_counts = Counter(categorize_ex_failure(r) for r in results)\n",
    "failure_df = pd.DataFrame(\n",
    "    [{'category': k, 'count': v} for k, v in failure_counts.items()]\n",
    ").sort_values('count', ascending=False)\n",
    "\n",
    "display(failure_df)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
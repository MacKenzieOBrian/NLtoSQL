# Decision Record (Dissertation)

This document records major research and engineering decisions for the NL→SQL reproduction study, including rationale, rejected alternatives, and implementation locations. Decision traceability is important for reproducibility, evaluation validity, and dissertation defence.

---

## 1. Research Design & Experimental Framing

| Decision | Why (Justification) | Alternatives Rejected | Where |
|---|---|---|---|
| Replicate Ojuri et al. methodology using OSS | Enables fair comparison under open constraints | Evaluate GPT-4 directly → proprietary, non-reproducible | `STATE_OF_THE_ART.md`, `METHODOLOGY.md` |
| Comparative framing (prompting vs PEFT vs agent) | Avoids one-shot system claims; creates measurable uplift ladder | Only report final system → hides where improvements come from | `notebooks/` + dissertation Chapter 5 |
| ClassicModels as domain benchmark | Matches Ojuri; business queries; multi-table joins | Spider → domain mismatch; BIRD → compute heavy | `DATA.md` |

---

## 2. Dataset & Evaluation Discipline

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| Fixed 200-item test benchmark | Enables repeated experiments with stable metrics | Dynamic or streaming evaluation → non-deterministic | `data/classicmodels_test_200.json` |
| Disjoint train/test | Prevents leakage and invalid generalisation claims | “Few-shot only” approach → hides training impact | `notebooks/04_build_training_set.ipynb` |
| Schema included in evaluation inputs | Required for column/table grounding | Retrieval-based schema → increases architectural confounds | `DATA.md`, `prompting.py` |

---

## 3. Why We Run (k=0) and (k=3) in Multiple Phases

| Phase | What Changes | What It Tests | Why It Matters |
|---|---|---|---|
| Baseline (`k=0`) | Prompting, no exemplars | intrinsic LM SQL reasoning | establishes floor |
| Baseline (`k=3`) | + exemplars | ICL conditioning uplift | measures prompt-only gains |
| QLoRA (`k=0`) | trained adapters | SFT uplift alone | isolates fine-tuning effect |
| QLoRA (`k=3`) | adapters + exemplars | coexistence effects | tests complement vs substitution |
| Agentic (planned) | + execution loop | semantic correction | expected largest EX/TS gain |

**Reflection:** This ladder allows stronger causal inference than reporting a single configuration.

---

## 4. Metric Selection (Ojuri-Aligned)

| Metric | Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|---|
| VA | executability | distinguishes syntax vs logic | static parsing → misses runtime issues | `eval.py` |
| EM | surface strict match | debugging | used alone misleading for SQL | `eval.py` |
| EX | execution accuracy | functional correctness | BLEU/ROUGE | `eval.py` |
| TS | test-suite accuracy (planned) | semantic robustness | accuracy-only metrics | documented in `ARCHITECTURE.md` |

**Meta:** Execution-based metrics are standard in NL→SQL because SQL has many surface-equivalent programs.

---

## 5. Safety, Security & Data Access

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| Read-only SQL execution | prevents destructive outputs | trust model outputs → unsafe | `query_runner.py` |
| Block DDL/DML tokens | hallucinated UPDATE/DELETE possible | evaluate full SQL grammar → unnecessary | `eval.py` |
| Cloud SQL + Connector | private IP; IAM auth | public DB endpoint → security risk | `db.py`, `CONFIG.md` |

---

## 6. Reproducibility & Experimental Integrity

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| Colab as primary runtime | open access + realistic SME hardware | multi-GPU cluster → unrealistic/examiner-unfair | `CONFIG.md` |
| Pinned dependency versions | avoids Colab binary drift | “latest pip” → breaks reproducibility | `requirements.txt` |
| Deterministic decoding | eliminates sampling noise | sampling-based decoding → introduces variance | `llm.py` |
| Gitignored results | avoid accidental large artifacts | committing artifacts → brittle | `.gitignore` |

---

## 7. PEFT Training Data (QLoRA)

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| 200 supervised examples | realistic small-scale SFT | >5k examples → infeasible on Colab | `data/train/` |
| SQL validated before training | enforces VA=True → avoids garbage learning | train on synthetic → unknown distribution drift | `04_build_training_set.ipynb` |

---

## Architecture Notes (for traceability)

- Components: schema access + prompting + strategy (prompt/QLoRA/agent) + safety/postprocess + execution/eval. Library code in `nl2sql/`; notebooks orchestrate per strategy.
- Flow: NLQ → schema text → strategy → SQL candidate → safety filter → DB execution → metrics (VA/EX/EM; TS planned).
- Design tensions: schema fidelity vs prompt length; k=0 vs k>0 (separate adapter vs exemplar effects); agent vs deterministic eval (EX/TS gains vs latency/variance).
| Mixed difficulty coverage | probes SQL behaviours | single-difficulty → misleading | `DATA.md` |

---

## 8. Compute Constraints & Deployment Reality

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| 8B scale + 4-bit quantization | fits Colab VRAM; QLoRA-compatible | full FP16 → 60–80GB | `CONFIG.md` |
| No external GPUs | aligns with SME deployment assumption | cloud GPU clusters | `COMPUTATIONAL_CONSTRAINTS.md` |
| OSS only (model + infra) | transparency + reproducibility | GPT-4 APIs | `DECISIONS.md` |

---



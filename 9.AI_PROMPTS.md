# Appendix - Use of AI Tools

This appendix explains how AI tools were used in a limited, transparent way. The goal was to reduce routine engineering friction without outsourcing research decisions.

---

## Representative Examples (Plain Language)

**Example 1 - Install and dependency issues**
Prompt: "bitsandbytes import fails on Colab. Which versions work with T4?"
Why acceptable: This is about environment setup, not methodology.

**Example 2 - Small helper**
Prompt: "Extract the first SELECT statement from a chatty LLM response."
Why acceptable: A tiny helper was added and manually reviewed.

**Example 3 - Boilerplate recall**
Prompt: "Minimal PEFT/LoRA training config with only essential params."
Why acceptable: This just recalled common boilerplate; I adjusted it to match my setup.

**Example 4 - Debugging**
Prompt: "MySQL ambiguous column error in JOIN - common causes?"
Why acceptable: Used to form a hypothesis, then verified directly.

**Example 5 - Evaluation loop sketch**
Prompt: "Pseudocode for VA/EX/EM loop given pred SQL + gold SQL + engine."
Why acceptable: Used as a starter outline and tutorial to make it, final evaluation logic was written, tested, and documented in this dissertation.

**Example 6 - Script boilerplate**
Prompt: "Minimal CLI runner with argparse for model/eval settings."
Why acceptable: Standard boilerplate; logic was then customized for this repo.

**Example 7 - Markdown cleanup**
Prompt: "Make notebook markdown concise and align with current code paths."
Why acceptable: Documentation-only, no effect on experimental logic.

## How I Validated AI Output

- Read every suggestion and compared it to official documentation or my own tests.
- Ran the code in my environment and inspected outputs.
- Adjusted or replaced any logic that did not match project needs.



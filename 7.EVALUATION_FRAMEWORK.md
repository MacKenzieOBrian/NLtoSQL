# Evaluation Framework

## At‑a‑Glance
- **Primary metrics:** VA / EX / TS (suite-based semantic check)
- **Evaluation substrate:** ClassicModels (200‑item test set)
- **Agent diagnostics:** trace‑level analysis for failure attribution
- **Goal:** compare prompting, QLoRA, and execution‑guided agent under identical harness

---

## 1) Decision Journey (Literature‑Backed)
- **Execution‑centric metrics:** SQL is non‑unique; execution is the semantic arbiter [Yu et al., 2018; Zhong et al., 2020].
- **Agent trace analysis:** necessary for interpreting VA=1, EX=0 in agentic loops.
- **Closed‑schema setting:** ClassicModels mirrors Ojuri et al. (2025) and Spider‑S/BIRD‑mini controlled‑schema evaluations.
- **Compute‑aware validation:** feasibility under Colab‑class VRAM is reported.
- **Acceptance criteria:** output control + semantic gate prevent valid‑but‑irrelevant SQL (PICARD; ValueNet/DIN‑SQL).
- **Intent constraints + canonicalization:** grouped‑aggregate checks + table‑case normalization improve EX alignment.
- **TS + error taxonomy:** TS detects accidental correctness across perturbed DB replicas; error‑type logging (projection, aggregation scope, join selection) explains EX failures beyond VA.

---

## 2) Metrics
### 2.1 Valid SQL (VA)
VA = 1 if SQL executes without syntax/schema errors. Baseline validity signal in NL→SQL pipelines.

### 2.2 Execution Accuracy (EX)
EX = 1 if result‑set matches gold execution output. Primary correctness metric.

### 2.3 Test‑Suite Accuracy (TS) — implemented (suite‑based)
TS = 1 if predicted and gold results match **across multiple perturbed DB replicas**.  
This is a **lightweight suite‑based semantic check**, not full *distilled test suites* in the Zhong et al. sense.  
It is still defensible as a robustness‑oriented semantic metric that reduces “lucky execution” on a single DB.  
Refs: [18]

### 2.4 Exact Match (EM)
Reported for compatibility; used for debugging, not correctness.

---

## 3) Dataset & Benchmarking Substrate
- **Schema:** ClassicModels (MySQL)
- **Test set:** 200 NL→SQL pairs
- **Split discipline:** no leakage; schema preserved
- **Execution:** live DB execution (semantic oracle)

---

## 4) Evaluation Pipeline
1) Generate SQL (prompt / QLoRA / agent)
2) Postprocess + safety guards
3) Execute via `QueryRunner`
4) Compute VA/EX/EM/TS
5) Aggregate across 200 samples
6) Agentic trace analysis (optional)

Relevant code:
- `nl2sql/eval.py`
- `notebooks/02_baseline_prompting_eval.ipynb`
- `notebooks/05_qlora_train_eval.ipynb`
- `notebooks/03_agentic_eval.ipynb`

---

## 5) Agentic Evaluation Notes
Agentic strategies add a second diagnostic axis: *why* EX fails when VA succeeds. Trace logging captures:
- candidate diversity
- error classification
- repair success
- fallback rate

These are reported alongside VA/EX/EM to support explainable evaluation.

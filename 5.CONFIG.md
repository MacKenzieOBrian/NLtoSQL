# Configuration & Reproducibility

This file describes the minimal configuration needed to reproduce all experiments (baseline, QLoRA, and execution-guided agentic runs) on a Colab-class GPU. It documents: project layout (code vs notebooks vs results), environment variables and secrets, dependency pinning, model + adapter configuration, and evaluation knobs (metrics, error guards, agent settings).

## Project structure (why it changed)

The repo is intentionally split into:
- `nl2sql/` (importable “experiment harness”): stable, reviewable code for DB access, safe execution, prompting, and evaluation.
- `notebooks/` (Colab runners): orchestrate runs, save outputs, and generate dissertation tables/figures without duplicating core logic.
- `data/` (benchmarks) and `results/` (run outputs): keep inputs and outputs separate; `results/` is gitignored by default to avoid committing large outputs by accident.

This makes runs easier to reproduce: the notebook becomes a thin runner, while evaluation logic lives in version-controlled modules.

## Quickstart (Colab — Baseline Prompting)

To reproduce the main baseline results on ClassicModels:

1. **Runtime**
   - Use a GPU runtime (T4/A100) in Colab.

2. **Clone + install**
   - `git clone https://github.com/MacKenzieOBrian/NLtoSQL.git /content/NLtoSQL`
   - `%cd /content/NLtoSQL`
   - `pip install -r requirements.txt`
   - Restart the runtime once (to clear preinstalled wheels).

3. **Auth & env**
   - GCP: `google.colab.auth.authenticate_user()` (or ADC locally).
   - HF: set `HF_TOKEN` (or run `notebook_login()`).
   - Set DB env vars (`INSTANCE_CONNECTION_NAME`, `DB_USER`, `DB_PASS`, `DB_NAME`) or provide interactively when prompted.

4. **Run baseline eval**
   - Open `notebooks/02_baseline_prompting_eval.ipynb`.
   - Run all cells; results are saved to `results/baseline/`.

5. **Output handling**
   - `results/` is gitignored by default. If you want to commit a particular run, copy selected JSONs to a versioned subfolder and adjust `.gitignore` as needed.

## Quickstart (QLoRA — Fine-Tuned Adapters)

1. **Prepare a non-overlapping training set**
   - Start from `data/train/classicmodels_train_200.jsonl`.
   - Run `notebooks/04_build_training_set.ipynb` to validate:
     - SQL is SELECT-only,
     - no NLQ overlap with `data/classicmodels_test_200.json`,
     - all SQL executes successfully (VA=True).
   - Fix any failing rows and re-run validation.

2. **Train adapters + evaluate**
   - Run `notebooks/05_qlora_train_eval.ipynb`.
   - Adapters are saved under `results/adapters/qlora_classicmodels/`.
   - Evaluation JSONs are saved to `results/qlora/`.

3. **Modes**
   - `k=0`: measures the effect of fine-tuning alone.
   - `k=3`: measures fine-tuning + in-context exemplars (main comparator to prompt-only baselines).

## Core Configuration Keys

Set these as environment variables in Colab or your local shell:

| Variable | Purpose | Example |
|-----------------------|--------------------------------|------------------------------------------------------------|
| `INSTANCE_CONNECTION_NAME` | Cloud SQL instance name         | `modified-enigma-476414-h9:europe-west2:classicmodels`     |
| `DB_USER`             | MySQL username                 | `root`                                                     |
| `DB_PASS`             | MySQL password                 | (prompt-only; do not commit)                               |
| `DB_NAME`             | Database name                  | `classicmodels`                                            |
| `HF_TOKEN`            | Hugging Face auth token        | `hf_…` (from HF settings)                                  |
| `ADAPTER_PATH`        | Path to QLoRA adapter folder   | `results/adapters/qlora_classicmodels` (default)           |

Notes:
- If `ADAPTER_PATH` is unset or missing, notebooks/scripts automatically fall back to the base model.
- `HF_TOKEN` is required for gated Llama-3; never commit it to the repo.

## Dependencies (Pinned for Reproducibility)

All core packages are pinned in `requirements.txt` to avoid Colab binary drift.

- **LLM stack:** `torch`, `transformers`, `accelerate`, `bitsandbytes`, `triton`, `peft`, `trl`, `datasets`
- **DB stack:** `google-cloud-sql-connector`, `sqlalchemy`, `pymysql`
- **Config/utilities:** `huggingface_hub`, `pydantic`, `python-dotenv`, `numpy`, `pandas`

Colab gotcha:
- Colab ships its own wheels; they often conflict with HF/BNB versions.
- Run the single setup cell in the notebooks (uninstalls, then installs pinned versions), then restart the runtime once and continue.

### What each dependency is for (short)
`torch`/`transformers` handle model load/generation/training; `bitsandbytes`/`triton`/`accelerate` make 4-bit work on GPUs; `peft`/`trl`/`datasets` drive the LoRA/QLoRA SFT loop; `huggingface_hub` is for gated Llama token/download; `google-cloud-sql-connector`/`sqlalchemy`/`pymysql` keep ClassicModels access secure; `pydantic`/`python-dotenv` tame env/config.

## QLoRA Adapter Configuration

- **Location:** `results/adapters/qlora_classicmodels/`
- **Base model:** `meta-llama/Meta-Llama-3-8B-Instruct`
- **LoRA config:** `r=32`, `α=64`, dropout `0.05`, targets `q_proj` and `v_proj`, `inference_mode=True`
- **Size:** `adapter_model.safetensors` ≈ 52 MB (plus `checkpoint-75/` for training provenance)
- **Usage:** Set `ADAPTER_PATH=results/adapters/qlora_classicmodels` (default in notebooks/scripts). If the path is missing, code falls back to the base model automatically.


## Evaluation metrics (Ojuri-style)

- **VA (Validity)**: predicted SQL executes successfully (syntactic/engine validity check).
- **EM (Exact Match)**: normalized SQL string match vs gold SQL (strict, conservative).
- **EX (Execution Accuracy)**: execute predicted SQL and compare its result set to the gold SQL result set.
- **TS (Test-Suite Accuracy)**: planned; compare predicted vs gold results across multiple distilled DB variants.

## NL→SQL Guardrails (Config Summary)

The following guardrails are always active during evaluation:

- **Projection & ORDER/LIMIT constraints**
  - Minimal projections: drop extra SELECT columns unless required for GROUP BY/ORDER BY.
  - ORDER BY / LIMIT only when the NLQ implies ranking (e.g., “top”, “highest”, “first”).
- **Schema-aware routing**
  - country/creditLimit → join `customers` via `orders.customerNumber`
  - productLine/vendor → join `products` (and `orderdetails` for totals)
  - order totals → derive from `orderdetails.quantityOrdered * priceEach`
- **Literal templates**
  - Status literals whitelisted: `'Shipped'`, `'Cancelled'`, `'On Hold'`, `'Disputed'`, `'In Process'`, `'Resolved'`.

Implementation pointers:
- Prompt rules: `nl2sql/prompting.py`
- Post-generation guards: `nl2sql/postprocess.py` (`guarded_postprocess`)
- Eval harness: `nl2sql/eval.py`, `scripts/run_full_pipeline.py`

## Agentic (Execution-Guided) Configuration

### Core behaviour

- Mode: execution-guided reranker over multiple SQL candidates
- Tools: `QueryRunner` (safe SELECT executor) + ClassicModels schema summary
- Candidates: main prompt + tabular prompt, plus an optional deterministic few-shot baseline
- Filtering: strict SELECT-only (`clean_candidate`), no markdown/explanations
- Reranking: lexical `semantic_score` − λ·`count_select_columns(sql)` (intent-aware + projection penalty)
- Retries: one-shot repair on common MySQL errors using `classify_error` + `error_hint`

### Where it lives

- Helpers: `nl2sql/agent_utils.py`
- Notebook harness: `notebooks/03_agentic_eval.ipynb`
- Script harness: `scripts/run_full_pipeline.py` (react mode)

### Usage

- Set mode to react (script) or run the ReAct cells in the notebook; defaults keep 200-query runs tractable on a T4.
- Tune `num_cands` and `max_steps` as needed.

### Scope

This is a ReAct-inspired execution-guided reranker, not a full Thought/Action/Observation agent. Observations (errors/hints) guide repairs and prompts, but the model does not yet emit explicit `Action: ...` lines or multi-step traces.

Rationale for dissertation write-up:
- Exact-match sensitivity means minor projection/order differences look like failures; enforcing minimal outputs aligns with EM’s definition and reduces false negatives.
- Joining the correct table follows the normalized schema design; aligning the planner with schema locality cuts unknown-column errors and improves EX without changing model weights.
- Status literal and aggregation templates anchor the model to schema-grounded constants/functions, a lightweight form of constrained decoding (cf. Scholak et al., 2021, PICARD).
- Error-aware retries leverage deterministic DB error signals as supervision-free feedback, consistent with ReAct and other iterative tool-using agents.

Dissertation hook:
- Present results in three steps: prompt-only → prompt+QLoRA → QLoRA+agentic (semantic rerank + repair). The progression illustrates how execution guidance stabilises VA, while intent-aware reranking/repairs target EX without new training—an interpretable bridge from Ojuri-style agents to open-source Llama-3.

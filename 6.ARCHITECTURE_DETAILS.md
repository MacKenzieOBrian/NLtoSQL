# Detailed System Architecture

This document expands the architectural details for the NL→SQL reproduction study, connecting implementation decisions to methodological goals and evaluation requirements. research decisions are tracked separately in `DECISIONS.md`.

---

## 1. Architectural Viewpoint

This architecture reflects a **research-oriented pipeline**, not a production service. It must satisfy three competing properties:

1. **Scientific validity** — evaluation and metrics must be meaningful.
2. **Reproducibility** — experiments must be rerunnable on standard hardware (Colab).
3. **Comparability** — results must be comparable to Ojuri et al. and related NL→SQL work.

These constraints strongly shaped system design.

---

## 2. High-Level Conceptual Flow

The conceptual pipeline can be described as:

NL Query → Schema Context → Model Strategy → SQL Candidate → Safety Filter → SQL Execution → Metrics


This aligns with Ojuri et al.’s prompting → fine-tuning → agent loop progression.

Control flow is intentionally serial at this stage to maintain metric interpretability.

---

## 3. Logical Component Architecture

The system is decomposed into five logical components:

1. **Schema Access Layer**
2. **Prompt Construction Layer**
3. **Model Strategy Layer**
4. **Postprocessing + Safety Layer**
5. **Execution & Evaluation Layer**

Each component is independent, enabling replacement experiments (e.g., swap strategy = Prompt vs QLoRA vs ReAct).

---

## 4. Physical Layout & Repository Structure

The physical organisation described in the original document :contentReference[oaicite:1]{index=1} is retained and appropriate for research reproducibility:
nl2sql/
db.py
schema.py
query_runner.py
prompting.py
llm.py
postprocess.py
eval.py
notebooks/
data/
results/


**Principle:** notebooks orchestrate, library modules implement evaluation.

This avoids the common ML anti-pattern where evaluation code hides inside notebook cells.

---

## 5. Architectural Rationale (Why This Shape)

Each design choice supports replication fidelity:

| Architectural Need | Design Expression |
|---|---|
| Execution-based metrics | Live database + QueryRunner |
| Schema grounding | Schema-to-text prompt representation |
| Strategy comparison | Pluggable Model Strategy Layer |
| Reproducibility | Pinned deps + deterministic decoding |
| Compute realism | 8B + 4-bit quantisation |
| Secure evaluation | Read-only SQL enforcement |
| Traceability | JSON logging for VA/EX/TS |

This answers the examiner’s implied question:

> “Why not just call GPT-4 with a prompt?”

---

## 6. Design Tensions & Trade-offs

Important architectural tensions observed:

### 6.1 Schema Fidelity vs Prompt Length

- High-fidelity schema improves column selection
- Long schema increases token pressure → higher latency
- ClassicModels hits a “sweet spot” for 8B models

### 6.2 Generalization vs Fair Comparison

- QLoRA may internalise schema
- Prompting uses external schema
- Must evaluate **both** `k=0` and `k>0` to disambiguate

### 6.3 ReAct Agent vs Deterministic Evaluation

- ReAct improves semantic correction
- ReAct introduces stochastic control flow
- Must isolate agent loop uplift from sampling noise

---

## 7. Extension Hooks (Designed but Not Yet Executed)

Two future mechanisms were considered during architecture design:

### 7.1 ReAct / ExCoT Agent Loop

Hook point:
SQL Candidate → Execution Feedback → Rewrite → Re-evaluate


This aligns with ExCoT (Zhai et al., 2025) and Ojuri et al.’s GPT-4 results.

**Reflection:** Agent loops are expected to provide the largest performance jump in EX/TS.

### 7.2 Test-Suite (TS) Evaluation

Requires supporting:

- multiple DB clones
- deterministic perturbations
- semantic comparison

This explains why `QueryRunner` was built to return structured result sets.

---

## 8. Compute & Latency Characteristics (Colab Constraints)

For SME-feasibility and dissertation context:

| Operation | Cost/Constraint |
|---|---|
| Model load (8B, NF4) | 5–9GB VRAM |
| Prompt inference | 0.5–1.5s/query |
| Execution validation | ~3–15ms/query |
| QLoRA fine-tuning | VRAM bound + tokenisation overhead |
| Agent loops | Multiplicative latency |

These numbers matter in the final discussion section under:

> “Is this deployable outside proprietary API ecosystems?”

---

## 9. Deployment Modes

The architecture supports three deployment modes (conceptually):

| Mode | Description |
|---|---|
| **Research** | full VA/EX/TS pipeline |
| **Interactive** | NL→SQL + schema context + execution |
| **Batch Eval** | headless evaluation w/o UI |

Only **Research Mode** is implemented, but the architecture cleanly permits the others.

---

## 10. Security & Safety Considerations

SQL safety is fundamental because evaluation involves executing untrusted model outputs.

Security stance:

| Concern | Mitigation |
|---|---|
| destructive SQL | token blocking + read-only enforcement |
| credential leakage | env vars + Cloud SQL IAM |
| DB exposure | private connector + no public IP |
| hallucinated dangerous queries | logged + rejected |

This is essential because industry deployments must comply with governance constraints.

---

## 11. Threats to Validity (Architecture-Specific)

Architecture induces specific validity threats:

| Threat | Impact | Mitigation |
|---|---|---|
| DB state sensitivity | EX/TS vary if data changes | freeze ClassicModels dump |
| schema overfitting | QLoRA may internalise schema | evaluate k=0 & k>0 |
| prompt confounds | exemplars may leak info | exemplar discipline |
| stochastic decoding | non-deterministic EX | deterministic decoding for evaluation |

---

## 12. Meta Reflection

The architecture deliberately supports:

- scientific evaluation (VA/EX/TS)
- reproducible baselines
- compute-feasible fine-tuning
- agentic extension
- SME deployment realism

Unlike many NL→SQL papers, the architecture is not optimized purely for leaderboard scores; it optimizes for **replication and interpretability**, which is the core value of this dissertation.


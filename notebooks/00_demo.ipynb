{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bbdb4d8",
   "metadata": {},
   "source": [
    "# 00 Demo: NL→SQL (prompt vs QLoRA vs ReAct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b88de",
   "metadata": {},
   "source": [
    "Imports quick guide: we load schema helpers (`nl2sql.schema`), prompt builder/postprocess (`nl2sql.prompting`, `nl2sql.postprocess`), safe executor (`nl2sql.query_runner`), and model loader (`nl2sql.llm` or direct HF). These are small utilities we wrote to keep the notebooks thin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d5e9f0",
   "metadata": {},
   "source": [
    "Docs I leaned on: HF Transformers quantization (https://huggingface.co/docs/transformers/main_classes/quantization), PEFT/TRL (https://huggingface.co/docs/peft/, https://huggingface.co/docs/trl/), Cloud SQL connector + SQLAlchemy creator (https://cloud.google.com/sql/docs/mysql/connect-run, https://docs.sqlalchemy.org/en/20/core/engines.html#custom-dbapi-connect), ReAct (https://arxiv.org/abs/2210.03629), NL→SQL prompting survey (https://arxiv.org/abs/2410.06011)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969bf5f3",
   "metadata": {},
   "source": [
    "Docs I leaned on: HF Transformers quantization (https://huggingface.co/docs/transformers/main_classes/quantization), PEFT/TRL (https://huggingface.co/docs/peft/, https://huggingface.co/docs/trl/), Cloud SQL connector + SQLAlchemy creator (https://cloud.google.com/sql/docs/mysql/connect-run, https://docs.sqlalchemy.org/en/20/core/engines.html#custom-dbapi-connect), ReAct (https://arxiv.org/abs/2210.03629)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c590fd18",
   "metadata": {},
   "source": [
    "This code just pins the Colab runtime so 4-bit Llama loads. Run once, restart, then skip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857bd6b",
   "metadata": {},
   "source": [
    "## Setup (skip if runtime already prepared)\n",
    "If you’re on a fresh Colab GPU, run this once, then restart runtime. Otherwise skip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f65cfc",
   "metadata": {},
   "source": [
    "**Docs (setup):** HF Transformers quantization + BitsAndBytes (4-bit) https://huggingface.co/docs/transformers/main_classes/quantization, bnb https://github.com/TimDettmers/bitsandbytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e039a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "set -e\n",
    "export PIP_DEFAULT_TIMEOUT=120\n",
    "pip uninstall -y torch torchvision torchaudio bitsandbytes triton transformers accelerate peft trl datasets numpy pandas fsspec requests google-auth || true\n",
    "pip install -q --no-cache-dir --force-reinstall   numpy==1.26.4 pandas==2.2.1 fsspec==2024.5.0 requests==2.31.0 google-auth==2.43.0\n",
    "pip install -q --no-cache-dir --force-reinstall   torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121   --index-url https://download.pytorch.org/whl/cu121\n",
    "pip install -q --no-cache-dir --force-reinstall   bitsandbytes==0.43.3 triton==2.3.1   transformers==4.44.2 accelerate==0.33.0 peft==0.17.0 trl==0.9.6 datasets==2.20.0\n",
    "print(\"Setup done. Restart runtime, then continue.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb570087",
   "metadata": {},
   "source": [
    "This block is just auth/DB setup: HF token for Llama 3 and Cloud SQL connector + SQLAlchemy so the ClassicModels DB stays private."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea79b34",
   "metadata": {},
   "source": [
    "## Auth and paths (explain to the audience)\n",
    "- HF token: required to load gated Llama 3.\n",
    "- DB creds: needed to hit ClassicModels via Cloud SQL Connector.\n",
    "- Adapters: if present, we show fine-tuned behavior; if not, we show base model only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a783c2",
   "metadata": {},
   "source": [
    "**Docs (auth/DB):** Cloud SQL connector pattern https://cloud.google.com/sql/docs/mysql/connect-run; SQLAlchemy creator hook https://docs.sqlalchemy.org/en/20/core/engines.html#custom-dbapi-connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a986ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "from google.cloud.sql.connector import Connector\n",
    "from google.oauth2.service_account import Credentials\n",
    "from sqlalchemy import create_engine\n",
    "from pathlib import Path\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\") or getpass(\"Enter HF_TOKEN: \").strip()\n",
    "INSTANCE_CONNECTION_NAME = os.getenv(\"INSTANCE_CONNECTION_NAME\") or input(\"INSTANCE_CONNECTION_NAME: \")\n",
    "DB_USER = os.getenv(\"DB_USER\") or input(\"DB_USER: \")\n",
    "DB_PASS = os.getenv(\"DB_PASS\") or getpass(\"DB_PASS: \")\n",
    "DB_NAME = os.getenv(\"DB_NAME\") or \"classicmodels\"\n",
    "GOOGLE_CREDS = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "creds = Credentials.from_service_account_file(GOOGLE_CREDS) if GOOGLE_CREDS else None\n",
    "connector = Connector(credentials=creds)\n",
    "\n",
    "def getconn():\n",
    "    return connector.connect(\n",
    "        INSTANCE_CONNECTION_NAME,\n",
    "        \"pymysql\",\n",
    "        user=DB_USER,\n",
    "        password=DB_PASS,\n",
    "        db=DB_NAME,\n",
    "    )\n",
    "engine = create_engine(\"mysql+pymysql://\", creator=getconn, future=True)\n",
    "print(\"Engine ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7adca75",
   "metadata": {},
   "source": [
    "Here we grab the schema summary (tables/columns) and a tiny demo slice (5 items). The schema text keeps the model from hallucinating tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698afc8d",
   "metadata": {},
   "source": [
    "## Load schema + small demo slice\n",
    "We use the schema summary to ground prompts. We pick 5–10 items from the 200 test set for a fast demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b206a9",
   "metadata": {},
   "source": [
    "**Docs (schema prompts):** NL→SQL schema-grounded prompting survey https://arxiv.org/abs/2410.06011; Spider-style listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from nl2sql.schema import build_schema_summary\n",
    "\n",
    "SCHEMA_SUMMARY = build_schema_summary(engine, db_name=DB_NAME)\n",
    "test_path = Path(\"data/classicmodels_test_200.json\")\n",
    "test_set = json.loads(test_path.read_text(encoding=\"utf-8\"))\n",
    "demo_set = test_set[:5]\n",
    "print(\"Demo items:\", len(demo_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f66029e",
   "metadata": {},
   "source": [
    "Model load: HF 4-bit NF4 + BitsAndBytes. If adapters exist, we use them; otherwise base model. Deterministic decoding for repeatability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec91735d",
   "metadata": {},
   "source": [
    "## Load model (base or QLoRA adapters)\n",
    "Refs: HF Transformers 4-bit NF4 + BitsAndBytes, PEFT QLoRA. Adapters are optional; if missing we use the base model and explain the difference live.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da0763b",
   "metadata": {},
   "source": [
    "**Docs (model load):** HF 4-bit NF4 quantization https://huggingface.co/docs/transformers/main_classes/quantization; PEFT/QLoRA https://huggingface.co/docs/peft/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c55562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "ADAPTER_PATH = os.getenv(\"ADAPTER_PATH\") or \"results/adapters/qlora_classicmodels\"\n",
    "\n",
    "cc_major, _ = torch.cuda.get_device_capability(0) if torch.cuda.is_available() else (0,0)\n",
    "compute_dtype = torch.bfloat16 if cc_major >= 8 else torch.float16\n",
    "\n",
    "# Tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map={\"\": 0} if torch.cuda.is_available() else None,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "base_model.generation_config.do_sample = False\n",
    "base_model.generation_config.temperature = 1.0\n",
    "base_model.generation_config.top_p = 1.0\n",
    "\n",
    "from pathlib import Path\n",
    "adapter_dir = Path(ADAPTER_PATH)\n",
    "if adapter_dir.exists():\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_dir, token=HF_TOKEN)\n",
    "    print(\"Loaded adapters from\", adapter_dir)\n",
    "else:\n",
    "    model = base_model\n",
    "    print(\"No adapters found; using base model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84653d95",
   "metadata": {},
   "source": [
    "Prompt-only demo: k=0 and k=3 on a few questions. We show prompt → SQL → execute, then VA/EX. Good for a quick before/after with adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df30ae65",
   "metadata": {},
   "source": [
    "## Few examples: prompt-only vs QLoRA (k=0 and k=3)\n",
    "We’ll run a handful of items with k=0 (no exemplars) and k=3 (few-shot). We show the prompt, the SQL, and execute it to prove it works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189848a7",
   "metadata": {},
   "source": [
    "**Docs (prompt/eval):** ICL patterns https://arxiv.org/abs/2005.14165; execution-based metrics (VA/EX) https://aclanthology.org/2020.emnlp-main.29/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1142c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nl2sql.prompting import make_few_shot_messages\n",
    "from nl2sql.postprocess import normalize_sql\n",
    "from nl2sql.query_runner import QueryRunner\n",
    "from nl2sql.eval import execution_accuracy\n",
    "\n",
    "runner = QueryRunner(engine)\n",
    "\n",
    "exemplars = demo_set[:3]\n",
    "\n",
    "for k in [0,3]:\n",
    "    print(f\"== Demo k={k} ==\")\n",
    "    for item in demo_set:\n",
    "        msgs = make_few_shot_messages(\n",
    "            nlq=item['nlq'],\n",
    "            schema_summary=SCHEMA_SUMMARY,\n",
    "            exemplars=exemplars if k>0 else [],\n",
    "            k=k,\n",
    "        )\n",
    "        prompt_preview = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tok(prompt_preview, return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=256)\n",
    "        text = tok.decode(out[0], skip_special_tokens=True)\n",
    "        sql = normalize_sql(text)\n",
    "        try:\n",
    "            runner.run(sql)\n",
    "            va=True\n",
    "            ex = execution_accuracy(engine, sql, item['sql'])\n",
    "        except Exception as e:\n",
    "            va=False; ex=False; \n",
    "        print(f\"Q: {item['nlq']}SQL: {sql}VA: {va} EX: {ex}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6062e7",
   "metadata": {},
   "source": [
    "Mini ReAct: propose SQL, run it via QueryRunner (SELECT-only), see error/result, refine once. Just to illustrate the idea without long runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247982fb",
   "metadata": {},
   "source": [
    "## Tiny ReAct demo (1–2 steps)\n",
    "We show how the agent proposes SQL, runs it, sees errors/results, and refines once. This is a toy loop to illustrate the idea.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd252aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nl2sql.query_runner import QueryRunner\n",
    "runner = QueryRunner(engine)\n",
    "\n",
    "PROMPT_INSTR = \"You are an expert SQL agent. Think briefly, propose one SELECT query, then adjust if there is an error.\"\n",
    "\n",
    "def react_once(nlq, schema_text):\n",
    "    history=[]\n",
    "    observation=\"\"\n",
    "    for step in range(2):\n",
    "        prompt = f\"{PROMPT_INSTR}Schema:{schema_text}Previous observation: {observation}Question: {nlq}SQL:\"\n",
    "        inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=192)\n",
    "        text = tok.decode(out[0], skip_special_tokens=True)\n",
    "        sql = normalize_sql(text)\n",
    "        try:\n",
    "            rows = runner.run(sql)\n",
    "            observation=f\"SUCCESS ({len(rows)} rows)\"\n",
    "            return sql, observation\n",
    "        except Exception as e:\n",
    "            observation=f\"ERROR: {e}\"\n",
    "            history.append((sql, observation))\n",
    "    return sql, observation\n",
    "\n",
    "sample = demo_set[0]\n",
    "sql, obs = react_once(sample['nlq'], SCHEMA_SUMMARY)\n",
    "print(f\"Q: {sample['nlq']}SQL: {sql}Obs: {obs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf357a25",
   "metadata": {},
   "source": [
    "Recap: cite the full 200-item results from saved JSONs instead of rerunning heavy evals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8cf9cd",
   "metadata": {},
   "source": [
    "## Cite full results without rerunning\n",
    "We already ran the full 200-item evaluations. Headline numbers (from saved JSONs in `results/qlora/`):\n",
    "- Prompt few-shot EX: ~0.25–0.33\n",
    "- QLoRA k=3 EX: ~0.38 (VA ~0.87)\n",
    "- QLoRA k=0 EX: ~0.065 (needs agentic/TS)\n",
    "Use these in the demo; no need to recompute live.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

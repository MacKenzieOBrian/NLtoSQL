1. Proprietary LLM-Based NL→SQL Systems

Recent NL→SQL research shows that proprietary LLMs such as GPT-4 achieve high execution accuracy on multi-table queries when paired with agentic reasoning frameworks (Yao et al., 2023; Xi et al., 2025). Ojuri et al. (2025) specifically demonstrate that GPT-4, operating within a ReAct-style loop, can iteratively refine candidate SQL using execution feedback, achieving strong alignment to the ClassicModels schema.

These systems currently define the upper performance bound, but they have clear drawbacks for reproducibility:

closed model weights and training data

opaque internal reasoning mechanisms

per-query API and rate-limit constraints

vendor lock-in for enterprise deployment

Surveys of next-generation database interfaces (Hong et al., 2025; Zhu et al., 2024) note that these constraints motivate open-source alternatives for research, compliance, and on-prem operation.

2. Open-Source LLMs for NL→SQL

Instruction-tuned open models (e.g., Llama-3-8B-Instruct) can produce syntactically valid SQL when given schema context, but typically underperform proprietary models on semantic grounding tasks such as join reasoning, aggregation, and filtering (Li et al., 2023). This gap has led to interest in parameter-efficient fine-tuning (PEFT) to encode domain-specific behavior without full weight updates (Ding et al., 2023; Goswami et al., 2024).

Ojuri et al. (2025) report that QLoRA adapters trained on the ClassicModels domain substantially close the gap to GPT-4 on execution accuracy. Notebook results in this project replicate the literature trend: zero-shot performance was syntactically valid but semantically weak on join-heavy queries, while QLoRA training improved execution validity and consistency.

3. Prompting vs. Model Adaptation

In-context learning (Brown et al., 2020) enables adaptation without modifying weights, but survey evidence shows sensitivity to example selection and schema complexity (Zhu et al., 2024). Experiments confirmed this behavior: few-shot prompting improved over zero-shot baselines, but remained unstable across reorderings and did not reliably handle multi-table aggregation.

Comparative studies (Mosbach et al., 2023) show that supervised fine-tuning typically outperforms ICL for domain tasks. This aligns with the decision to incorporate QLoRA fine-tuning as a middle-ground between zero-shot prompting and full fine-tuning.

4. Execution-Feedback and Agentic Reasoning

Agentic reasoning frameworks such as ReAct (Yao et al., 2023) enable LLMs to interleave reasoning and acting, using external tools (e.g., SQL execution engines) to iteratively refine outputs. Surveys (Xi et al., 2025; Hong et al., 2025) identify these tool-augmented pipelines as a major trend in NL→SQL, as they reduce both syntactic errors (invalid SQL) and logical errors (incorrect joins or filters).

Extensions such as ExCoT (Zhai et al., 2025) explicitly propagate execution traces into the reasoning loop, improving alignment for complex queries. Ojuri et al. (2025) attribute part of their performance gain on ClassicModels to execution feedback, consistent with execution-guided decoding work (Zhong et al., 2017).

However, multiple surveys (Zhu et al., 2024; Rajkumar et al., 2022) note a persistent gap: execution validity (VA) is necessary but not sufficient for semantic correctness (EX). Models frequently emit executable but semantically misaligned queries—motivating hybrid approaches combining execution feedback, schema linking, constrained decoding (Scholak et al., 2021), and semantic reranking.

This informs the pipeline evolution in this project: integrating execution feedback, repair, reranking, and fallback to improve semantic correctness without additional model parameters.

5. Evaluation Benchmarks and Metrics

Early NL→SQL evaluations relied on string match metrics, which penalized logically equivalent SQL. Modern work uses:

Execution accuracy (EX) — output equivalence on real DB (Zhong et al., 2020)

Validity accuracy (VA) — executable vs. failing queries

Test-suite accuracy (TS) — robustness across data perturbations

Large-scale benchmarks (Li et al., 2023; Hong et al., 2025) adopt execution-centric metrics, reflecting the field’s emphasis on semantic correctness. Ojuri et al. (2025) used EX/TS metrics on ClassicModels, allowing direct comparison in this reproduction.

Notebook usage of EX/VA mirrored this trend: string-match metrics underestimated correctness, while execution-based metrics correctly scored semantically equivalent SQL.

6. Resource-Aware Model Adaptation and Deployment

Full fine-tuning of 7B+ models typically requires >60GB VRAM, limiting experimentation. QLoRA enables fine-tuning on 4-bit weights using adapter matrices, reducing VRAM to ~8–12GB and making training feasible on consumer GPUs (Ding et al., 2023). Notebook findings confirmed that QLoRA training completed on Colab T4 GPUs, validating literature claims.

Survey evidence (Hong et al., 2025) notes that enterprise database interfaces increasingly require on-prem inference, cost control, and no external API dependency, aligning with the deployment motivations of this project.

7. Practical SOTA Takeaways Relevant to Reproduction

From this literature, five conclusions emerge:

Proprietary GPT-4 agents currently lead NL→SQL performance but are closed and non-reproducible.

Open-source models benefit from PEFT, especially in domain tasks.

Agentic loops and execution feedback materially improve semantic correctness.

Execution-based metrics are required to measure true query equivalence.

Resource constraints shape methodological choices, making QLoRA a viable compromise for reproducibility.

These findings motivate evaluating whether open-source NL→SQL pipelines enhanced with PEFT and agentic refinement can approximate proprietary GPT-4 performance on ClassicModels under realistic hardware and deployment conditions.
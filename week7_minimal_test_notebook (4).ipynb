{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b52ec60",
   "metadata": {},
   "source": [
    "\n",
    "# NL-to-SQL scaffold (classicmodels)\n",
    "\n",
    "What this notebook does:\n",
    "- Auth to GCP\n",
    "- Safe Cloud SQL connection via connector + SQLAlchemy\n",
    "- Schema helpers + QueryRunner tool\n",
    "- Smoke tests and dataset validator\n",
    "- Base Llama-3-8B load (pre-QLoRA placeholder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0388a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auth to Google Cloud\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14c054",
   "metadata": {},
   "source": [
    "## Project context\n",
    "Swap to env var in production if you don't want to hardcode project_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae103c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "project_id = \"modified-enigma-476414-h9\"  # replace with env var in production\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04453d28",
   "metadata": {},
   "source": [
    "## Installs\n",
    "Pin these in a requirements cell/file for real runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8572e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install \"cloud-sql-python-connector[pymysql]\" SQLAlchemy==2.0.7 pymysql cryptography==41.0.0 --force-reinstall --no-cache-dir\n",
    "!{sys.executable} -m pip install accelerate\n",
    "!{sys.executable} -m pip install bitsandbytes\n",
    "!{sys.executable} -m pip install peft\n",
    "!{sys.executable} -m pip install transformers\n",
    "!{sys.executable} -m pip install datasets\n",
    "!{sys.executable} -m pip install trl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2469e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and logger\n",
    "import os\n",
    "import logging\n",
    "from google.cloud.sql.connector import Connector\n",
    "import sqlalchemy\n",
    "from sqlalchemy import text\n",
    "import pymysql\n",
    "from typing import Optional\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"nl2sql_db\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2adc2a",
   "metadata": {},
   "source": [
    "## Connection params\n",
    "Env first, prompt fallback during dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dbf201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "# Set these environment variables in Colab using:\n",
    "#   %env DB_USER=... %env DB_NAME=... etc (or use secrets manager)\n",
    "INSTANCE_CONNECTION_NAME = os.getenv(\"INSTANCE_CONNECTION_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"classicmodels\")\n",
    "\n",
    "# Fallback interactive prompt if variables missing\n",
    "if not INSTANCE_CONNECTION_NAME:\n",
    "    INSTANCE_CONNECTION_NAME = input(\"Enter INSTANCE_CONNECTION_NAME: \").strip()\n",
    "if not DB_USER:\n",
    "    DB_USER = input(\"Enter DB_USER: \").strip()\n",
    "if not DB_PASS:\n",
    "    DB_PASS = getpass(\"Enter DB_PASS: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ecf44d",
   "metadata": {},
   "source": [
    "## Connector + engine setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308caa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "from sqlalchemy.engine import Engine\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "connector = Connector()\n",
    "\n",
    "def getconn():\n",
    "    \"\"\"SQLAlchemy creator hook using the Cloud SQL connector.\"\"\"\n",
    "    return connector.connect(\n",
    "        INSTANCE_CONNECTION_NAME,\n",
    "        \"pymysql\",\n",
    "        user=DB_USER,\n",
    "        password=DB_PASS,\n",
    "        db=DB_NAME,\n",
    "    )\n",
    "\n",
    "engine: Engine = sqlalchemy.create_engine(\"mysql+pymysql://\", creator=getconn, future=True)\n",
    "\n",
    "@contextmanager\n",
    "def safe_connection(engine):\n",
    "    \"\"\"Yield a connection and clean up after use.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = engine.connect()\n",
    "        yield conn\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55497d9c",
   "metadata": {},
   "source": [
    "## Schema exploration helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def list_tables(engine) -> list:\n",
    "    \"\"\"Return a list of table names.\"\"\"\n",
    "    with safe_connection(engine) as conn:\n",
    "        result = conn.execute(text(\"SHOW TABLES;\")).fetchall()\n",
    "    return [r[0] for r in result]\n",
    "\n",
    "def get_table_columns(engine, table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Return a DataFrame of columns.\"\"\"\n",
    "    query = text(\"\"\"\n",
    "        SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE, COLUMN_KEY\n",
    "        FROM INFORMATION_SCHEMA.COLUMNS\n",
    "        WHERE TABLE_SCHEMA = :db AND TABLE_NAME = :table\n",
    "        ORDER BY ORDINAL_POSITION\n",
    "    \"\"\")\n",
    "    with safe_connection(engine) as conn:\n",
    "        df = pd.read_sql(query, conn, params={\"db\": DB_NAME, \"table\": table_name})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfaf57e",
   "metadata": {},
   "source": [
    "## Smoke tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4416e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sample_customers(limit: int = 10):\n",
    "    q = text(\"SELECT customerNumber, customerName, country FROM customers LIMIT :limit;\")\n",
    "    with safe_connection(engine) as conn:\n",
    "        df = pd.read_sql(q, conn, params={\"limit\": limit})\n",
    "    return df\n",
    "\n",
    "try:\n",
    "    tables = list_tables(engine)\n",
    "    logger.info(\"Tables in classicmodels: %s\", tables)\n",
    "    sample_df = fetch_sample_customers(5)\n",
    "    display(sample_df)\n",
    "except Exception as e:\n",
    "    logger.exception(\"Smoke test failed: %s\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57550152",
   "metadata": {},
   "source": [
    "## QueryRunner (read-only tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from typing import Any, Dict\n",
    "\n",
    "class QueryExecutionError(Exception):\n",
    "    pass\n",
    "\n",
    "def now_utc_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "class QueryRunner:\n",
    "    \"\"\"\n",
    "    Execute generated SQL safely against the engine, capture results and metadata,\n",
    "    and keep a history suitable for evaluation and error analysis.\n",
    "    \"\"\"\n",
    "    def __init__(self, engine, max_rows: int = 1000, forbidden_tokens=None):\n",
    "        self.engine = engine\n",
    "        self.max_rows = max_rows\n",
    "        self.history = []\n",
    "        self.forbidden_tokens = forbidden_tokens or [\"drop \", \"delete \", \"truncate \", \"alter \", \"create \", \"update \", \"insert \"]\n",
    "\n",
    "    def _safety_check(self, sql: str) -> None:\n",
    "        lowered = (sql or \"\").strip().lower()\n",
    "        if not lowered:\n",
    "            raise QueryExecutionError(\"Empty SQL string\")\n",
    "        for token in self.forbidden_tokens:\n",
    "            if token in lowered:\n",
    "                raise QueryExecutionError(f\"Destructive SQL token detected: {token.strip()}\")\n",
    "\n",
    "    def run(self, sql: str, params: Optional[Dict[str, Any]] = None, capture_df: bool = True) -> Dict[str, Any]:\n",
    "        entry = {\n",
    "            \"sql\": sql,\n",
    "            \"params\": params,\n",
    "            \"timestamp\": now_utc_iso(),\n",
    "            \"success\": False,\n",
    "            \"rowcount\": 0,\n",
    "            \"exec_time_s\": None,\n",
    "            \"error\": None,\n",
    "            \"columns\": None,\n",
    "            \"result_preview\": None,\n",
    "        }\n",
    "        try:\n",
    "            self._safety_check(sql)\n",
    "            start = datetime.now(timezone.utc)\n",
    "            with safe_connection(self.engine) as conn:\n",
    "                result = conn.execute(sqlalchemy.text(sql), params or {})\n",
    "                rows = result.fetchall()\n",
    "                cols = list(result.keys())\n",
    "            end = datetime.now(timezone.utc)\n",
    "            exec_time = (end - start).total_seconds()\n",
    "            df = None\n",
    "            if capture_df:\n",
    "                df = pd.DataFrame(rows, columns=cols)\n",
    "                if len(df) > self.max_rows:\n",
    "                    df = df.iloc[: self.max_rows]\n",
    "            entry.update({\n",
    "                \"success\": True,\n",
    "                \"rowcount\": min(len(rows), self.max_rows),\n",
    "                \"exec_time_s\": exec_time,\n",
    "                \"columns\": cols,\n",
    "                \"result_preview\": df\n",
    "            })\n",
    "        except Exception as e:\n",
    "            entry.update({\n",
    "                \"error\": str(e),\n",
    "                \"success\": False\n",
    "            })\n",
    "        finally:\n",
    "            self.history.append(entry)\n",
    "        return entry\n",
    "\n",
    "    def last(self):\n",
    "        return self.history[-1] if self.history else None\n",
    "\n",
    "    def save_history(self, path: str):\n",
    "        serializable = []\n",
    "        for h in self.history:\n",
    "            s = {k: v for k, v in h.items() if k != \"result_preview\"}\n",
    "            serializable.append(s)\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(serializable, f, indent=2, default=str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db1cbe",
   "metadata": {},
   "source": [
    "## QueryRunner quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qr = QueryRunner(engine, max_rows=200)\n",
    "test_sql = \"SELECT customerNumber, customerName, country FROM customers LIMIT 10;\"\n",
    "meta = qr.run(test_sql)\n",
    "print(\"Success:\", meta[\"success\"])\n",
    "if meta[\"success\"]:\n",
    "    display(meta[\"result_preview\"])\n",
    "else:\n",
    "    print(\"Error:\", meta[\"error\"])\n",
    "\n",
    "# List and display schema\n",
    "for table_name in list_tables(engine):\n",
    "    print(f\"\n",
    "Schema for table: {table_name}\")\n",
    "    df_columns = get_table_columns(engine, table_name)\n",
    "    display(df_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c3121",
   "metadata": {},
   "source": [
    "## Dataset validation helper\n",
    "Run the static classicmodels test set against the live DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e170002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_test_set(path: str = \"data/classicmodels_test_200.json\", limit: Optional[int] = None):\n",
    "    import json\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        items = json.load(f)\n",
    "    if limit:\n",
    "        items = items[:limit]\n",
    "\n",
    "    qr = QueryRunner(engine, max_rows=200)\n",
    "    successes = []\n",
    "    failures = []\n",
    "    for idx, item in enumerate(items):\n",
    "        meta = qr.run(item[\"sql\"], capture_df=False)\n",
    "        if meta[\"success\"]:\n",
    "            successes.append(idx)\n",
    "        else:\n",
    "            failures.append({\n",
    "                \"index\": idx,\n",
    "                \"nlq\": item.get(\"nlq\"),\n",
    "                \"sql\": item.get(\"sql\"),\n",
    "                \"error\": meta[\"error\"],\n",
    "            })\n",
    "    print(f\"Ran {len(items)} queries. Success: {len(successes)}. Failures: {len(failures)}.\")\n",
    "    if failures:\n",
    "        print(\"Failures (first 5):\")\n",
    "        for f in failures[:5]:\n",
    "            print(f)\n",
    "    return successes, failures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e92472",
   "metadata": {},
   "source": [
    "## Tiny starter NLQ-SQL set (for quick checks)\n",
    "Main test set lives in data/classicmodels_test_200.json."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db697297",
   "metadata": {},
   "source": [
    "## Load static test set\n",
    "Use the fixed 200-sample NLQ-SQL pairs from data/classicmodels_test_200.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3797e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/classicmodels_test_200.json', 'r', encoding='utf-8') as f:\n",
    "    test_set = json.load(f)\n",
    "print(f\"Loaded {len(test_set)} test items from data/classicmodels_test_200.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8975b6b8",
   "metadata": {},
   "source": [
    "## Load base model/tokenizer (pre-QLoRA placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16084aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token=True\n",
    ")\n",
    "\n",
    "print(f\"Tokenizer and model '{model_id}' loaded successfully.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

# Configuration & Reproducibility

How to run this without surprises: pinned deps, tokens, DB access, and what each library is for, in plain terms.

## Project structure (why it changed)

The repo is intentionally split into:
- `nl2sql/` (importable “experiment harness”): stable, reviewable code for DB access, safe execution, prompting, and evaluation.
- `notebooks/` (Colab runners): orchestrate runs, save outputs, and generate dissertation tables/figures without duplicating core logic.
- `data/` (benchmarks) and `results/` (run outputs): keep inputs and outputs separate; `results/` is gitignored by default to avoid committing large outputs by accident.

This makes runs easier to reproduce: the notebook becomes a thin runner, while evaluation logic lives in version-controlled modules.

## Quickstart (Colab baseline)

1. Use a GPU runtime (T4/A100).
2. Fresh clone the repo into `/content`
3. Install pinned deps from `requirements.txt`, then restart the runtime.
4. Authenticate:
   - GCP: `google.colab.auth.authenticate_user()` (or ADC locally)
   - Hugging Face: `notebook_login()` or `HF_TOKEN`
5. Run: `notebooks/02_baseline_prompting_eval.ipynb`
6. Outputs are saved under `results/baseline/` (gitignored by default; see `results/README.md`).
   - If you want these outputs on GitHub, remove/adjust the `results/` rule in `.gitignore` and commit curated JSONs.

## Quickstart (QLoRA)

1. Build a training set that does not overlap the 200-item benchmark:
   - The repo includes a starter training file at `data/train/classicmodels_train_200.jsonl`.
   - Run `notebooks/04_build_training_set.ipynb` to validate it strictly:
     - SELECT-only output
     - no overlap with `data/classicmodels_test_200.json` (exact NLQ match check)
     - SQL must execute on the live ClassicModels DB (VA=True)
   - If any rows fail, edit `data/train/classicmodels_train_200.jsonl` and re-run validation.
2. Fine-tune and evaluate adapters:
   - Run `notebooks/05_qlora_train_eval.ipynb` (saves adapters to `results/adapters/` and eval JSONs to `results/qlora/`).

Evaluation note:
- The QLoRA notebook evaluates the fine-tuned adapters in two inference modes:
  - `k=0` (no exemplars): measures what fine-tuning achieves on its own.
  - `k=3` (few-shot exemplars): measures whether prompt conditioning still adds uplift on top of fine-tuning.
This mirrors the dissertation question: prompt-only vs fine-tuning, and their interaction.

## Environment variables

Set these before running notebooks (or enter when prompted):

| Variable | Purpose | Example |
|----------|---------|---------|
| `INSTANCE_CONNECTION_NAME` | Cloud SQL instance identifier | `modified-enigma-476414-h9:europe-west2:classicmodels` |
| `DB_USER` | MySQL username | `root` |
| `DB_PASS` | MySQL password | — |
| `DB_NAME` | Database name | `classicmodels` |

## Dependencies

Pinned in `requirements.txt` to avoid Colab binary drift. The most failure-prone pins are `numpy`, `pandas`, `torch`, `triton`, and `bitsandbytes`.

### What each dependency is for (short)
`torch`/`transformers` handle model load/generation/training; `bitsandbytes`/`triton`/`accelerate` make 4-bit work on GPUs; `peft`/`trl`/`datasets` drive the LoRA/QLoRA SFT loop; `huggingface_hub` is for gated Llama token/download; `google-cloud-sql-connector`/`sqlalchemy`/`pymysql` keep ClassicModels access secure; `pydantic`/`python-dotenv` tame env/config.

**Colab gotcha:** Colab ships random wheels. Run the single setup cell (cleans + pins torch/cu121 + bnb/triton + HF stack), restart once, then carry on. If you add packages, pin them to play nice with NumPy 1.26.x.

## QLoRA adapters on disk (for eval)
- Path: `results/adapters/qlora_classicmodels`
- Base: `meta-llama/Meta-Llama-3-8B-Instruct`
- LoRA config: r=32, α=64, dropout=0.05, targets `q_proj`/`v_proj`, `inference_mode=True`
- Size: `adapter_model.safetensors` ~52 MB; checkpoint folder `checkpoint-75/` retained
- How to use: set `ADAPTER_PATH=results/adapters/qlora_classicmodels` (already the default in notebooks/scripts). Falls back to base model if missing.
- Agent tweaks in notebooks: prompt-stripping decode, a tiny projection guard for recurring patterns (minimal SELECT, no extra columns/ORDER unless asked), and a result-aware retry in ReAct. Default to a small slice for debugging; switch to full set for final runs.


## Evaluation metrics (Ojuri-style)

- **VA (Validity)**: predicted SQL executes successfully (syntactic/engine validity check).
- **EM (Exact Match)**: normalized SQL string match vs gold SQL (strict, conservative).
- **EX (Execution Accuracy)**: execute predicted SQL and compare its result set to the gold SQL result set.
- **TS (Test-Suite Accuracy)**: planned; compare predicted vs gold results across multiple distilled DB variants.

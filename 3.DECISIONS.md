# Decisions (traceable and comparative)

This section records key methodological decisions, their motivations, rejected alternatives, and traceability. The goal is to make the experimental framing auditable, theoretically justified, and reproducible. These decisions complement the Methodology and Evaluation chapters and reflect established practice in NL→SQL research, where ablation-style comparisons and disclosure of evaluation conditions are standard.

## 1. Research Design

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| 200 supervised examples | realistic small-scale SFT | >5k examples → infeasible on Colab | `data/train/` |
| SQL validated before training | enforces VA=True → avoids garbage learning | train on synthetic → unknown distribution drift | `04_build_training_set.ipynb` |

**Justification:** Comparative framing aligns with contemporary NL→SQL evaluation practice, where prompting, fine-tuning, and agentic refinement are treated as separable adaptation mechanisms for structured tasks. This avoids single-configuration claims and enables attribution of performance gains to specific mechanisms.

## 2. Dataset & Evaluation

Leakage and schema omission are known threats to validity; we validate datasets against the live ClassicModels schema and de-duplicate NLQs across train/test.

This follows evaluation discipline used in Spider, BIRD, and Ojuri et al., where schema grounding and leakage control are treated as basic validity requirements for NL→SQL benchmarks.

## 3. Prompt Hygiene

Controls: fixed system message, ordered schema summary, k∈{0,3} exemplars, deterministic decoding, guarded first-SELECT extraction.

**Interpretation:** Prompt hygiene isolates intrinsic model SQL competence from confounding prompt engineering effects. Few-shot prompting improves structural patterns (syntax, projection) but cannot reliably recover semantic mappings (joins, aggregates) without weight adaptation.

## 4. Adaptation Ladder (k=0 vs k=3; PEFT; Agent)

| Stage | Mechanism | Expected gain |
|---|---|---|
| Prompt-only | in-context examples | syntax/schema stability |
| QLoRA | adapter fine-tuning | semantic mappings (joins/aggregates) |
| Execution-guided agent | candidate execution + rerank/repair | semantic robustness, self-correction |

**Methodological Value:** The ladder operationalises an ablation used in applied NLP: it helps infer where semantic competence is acquired (weights vs context vs feedback). Similar staged comparisons appear in Ojuri et al. (prompt → adapter → agent) and broader LLM reasoning studies that treat multi-step refinement as a distinct capability layer.

## 5. Metrics

- VA (validity): executable SQL
- EX (execution accuracy): result-set match vs gold
- EM (exact match): normalized string match
- TS (planned): suite-based robustness

**Justification:** Execution-based metrics (VA/EX/TS) are preferred because SQL has high surface variability. String match alone underestimates correctness and misclassifies alternate-but-valid programs. Test-suite robustness (TS) measures semantic stability under data perturbations.

## 6. Prompt/Adapter/Agent Choices

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| 8B scale + 4-bit NF4 | fits Colab VRAM; QLoRA-compatible | full FP16 7B/13B → 60–80GB | `CONFIG.md` |
| Few-shot + deterministic decode | reproducible, leakage-safe | aggressive sampling → instability | notebooks/02_baseline_prompting_eval.ipynb |
| agent_utils guardrails | SELECT-only, semantic rerank, error-classified repair | full ReAct grammar (out of scope for 8B) | `nl2sql/agent_utils.py` |

## 7. Safety, Security & Data Access

Read-only execution, DDL/DML blocked, role-constrained DB access via Cloud SQL connector and SQLAlchemy creator.

**Relevance:** Executing model-generated SQL introduces safety concerns absent in typical NLP tasks. Read-only execution and DB roles are standard governance in enterprise deployments and agentic systems involving tool calling.

## 8. Reproducibility & Integrity

Pinned dependencies, deterministic decoding, declared env vars, and git-tracked configs.

**Rationale:** Reproducibility is non-trivial in text-to-SQL due to dependency drift (CUDA/BNB/Transformers), schema coupling, and execution variance. Pinned deps, deterministic decode, and declarative config align with reproducibility recommendations in recent LLM systems work.

## 9. PEFT (QLoRA) Feasibility

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| QLoRA adapters (r=32, α=64) | domain adaptation under ≤10GB VRAM | full FT or larger models | `CONFIG.md`, `05_qlora_train_eval.ipynb` |

**Justification:** PEFT/QLoRA enable domain adaptation under strict VRAM budgets; literature reports similar feasibility windows (8–12GB VRAM) for 7B–8B models with 4-bit quantization and LoRA heads.

## 10. Architecture Notes (for traceability)

- Components: schema access + prompting + strategy (prompt/QLoRA/agent) + safety/postprocess + execution/eval. Library code in `nl2sql/`; notebooks orchestrate per strategy.
- Flow: NLQ → schema text → strategy → SQL candidate → safety filter → DB execution → metrics (VA/EX/EM; TS planned).

**Design Considerations:** Architecture balances: (1) schema fidelity vs prompt length, (2) zero-shot vs few-shot conditioning, (3) fine-tuning vs agentic correction, and (4) execution robustness vs evaluation determinism. The execution-guided agent implements minimal semantic self-correction without full ReAct Thought/Action grammars; execution feedback improves VA but EX gains remain limited without multi-step reasoning traces.

## 11. Compute Constraints & Deployment Reality

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| Colab/T4-scale GPU | matches SME/teaching constraint, reproducible | dedicated clusters | `CONFIG.md` |
| OSS-only stack | transparency/compliance | proprietary APIs (GPT-4, etc.) | repo-wide |

**Motivation:** These constraints reflect realistic SME/regulated settings where proprietary APIs may be disallowed and GPU clusters unavailable—matching motivations in Ojuri et al. and recent open-source NL→SQL studies.

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f2b652",
   "metadata": {},
   "source": [
    "# Agentic Evaluation (ReAct-style)\n",
    "\n",
    "Purpose: run a **ReAct\u2011inspired, execution\u2011guided NL\u2192SQL pipeline** on ClassicModels,\n",
    "report **VA / EX / EM / TS**, and keep everything reproducible.\n",
    "\n",
    "Run order:\n",
    "1) Environment + DB connection\n",
    "2) Schema + test set\n",
    "3) Model load\n",
    "4) ReAct helpers + prompts\n",
    "5) Quick sanity check\n",
    "6) Full eval (VA/EX/EM/TS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c44350",
   "metadata": {},
   "source": [
    "Docs used:\n",
    "- HF Transformers quantization (4\u2011bit NF4)\n",
    "- PEFT / QLoRA\n",
    "- Cloud SQL Connector + SQLAlchemy creator\n",
    "- ReAct (reason \u2192 act \u2192 observe \u2192 revise)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faade3c2",
   "metadata": {},
   "source": [
    "## Setup (run first, then restart)\n",
    "In a fresh Colab GPU runtime, run this one cell to clean preinstalls and pin the CUDA 12.1 torch/bitsandbytes/triton stack. When it finishes, **Runtime \u2192 Restart runtime**, then run the rest of the notebook from the clone cell onward without more restarts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79779a6",
   "metadata": {},
   "source": [
    "**Docs (setup):** HF Transformers quantization + BitsAndBytes (4-bit) https://huggingface.co/docs/transformers/main_classes/quantization, bnb https://github.com/TimDettmers/bitsandbytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b144d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "set -e\n",
    "export PIP_DEFAULT_TIMEOUT=120\n",
    "\n",
    "# Clean conflicting preinstalls\n",
    "pip uninstall -y torch torchvision torchaudio bitsandbytes triton transformers accelerate peft trl datasets numpy pandas fsspec requests google-auth || true\n",
    "\n",
    "# Base deps\n",
    "pip install -q --no-cache-dir --force-reinstall   numpy==1.26.4 pandas==2.2.1 fsspec==2024.5.0 requests==2.31.0 google-auth==2.43.0\n",
    "\n",
    "# Torch + CUDA 12.1\n",
    "pip install -q --no-cache-dir --force-reinstall   torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121   --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# bitsandbytes + triton + HF stack\n",
    "pip install -q --no-cache-dir --force-reinstall   bitsandbytes==0.43.3 triton==2.3.1   transformers==4.44.2 accelerate==0.33.0 peft==0.17.0 trl==0.9.6 datasets==2.20.0\n",
    "\n",
    "echo \"Setup complete. Restart runtime once, then run the rest of the notebook top-to-bottom.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b0b11",
   "metadata": {},
   "source": [
    "Model load: HF 4-bit NF4 + BitsAndBytes; deterministic decoding. If adapters exist, we load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47936fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Clone repo (Colab) + install deps\n",
    "import os\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not os.path.exists('/content/NLtoSQL'):\n",
    "        !git clone https://github.com/MacKenzieOBrian/NLtoSQL.git /content/NLtoSQL\n",
    "    %cd /content/NLtoSQL\n",
    "    !pip -q install -r requirements.txt\n",
    "    import torch, transformers, accelerate, peft\n",
    "    print('torch', torch.__version__, 'cuda', torch.cuda.is_available())\n",
    "else:\n",
    "    print('Not in Colab; using existing workspace')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24721f1",
   "metadata": {},
   "source": [
    "Prompt/eval: build prompts (system+schema+k exemplars), generate SQL, postprocess, and compute VA/EX/EM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b7e43",
   "metadata": {},
   "source": [
    "**Ref:** Colab clone/install pattern; keeps notebooks thin and code in `nl2sql/`. Hugging Face/Colab standard workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20974344",
   "metadata": {},
   "source": [
    "### Reference notes (what this builds on)\n",
    "- DB access: Cloud SQL Connector + SQLAlchemy creator\n",
    "- Schema/prompting: schema\u2011grounded prompting surveys\n",
    "- Model load: HF 4\u2011bit NF4 + PEFT\n",
    "- Agent loop: ReAct\u2011style execution feedback\n",
    "- Eval: VA/EX/EM + TS harness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f75fce",
   "metadata": {},
   "source": [
    "## Optional: use gcloud ADC (without a key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ff993",
   "metadata": {},
   "source": [
    "**Ref:** GCP ADC flow (docs: https://cloud.google.com/docs/authentication/provide-credentials-adc). Optional fallback if no service account JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this only if you prefer gcloud-based ADC (no JSON key)\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    %pip install -q --upgrade google-auth google-auth-oauthlib\n",
    "    !gcloud auth application-default login\n",
    "else:\n",
    "    print(\"Not in Colab; skip gcloud auth.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21847187",
   "metadata": {},
   "source": [
    "**Ref:** Pinned CUDA12.1 torch/bitsandbytes/triton stack per HF/BnB guidance for 4-bit loads on Colab GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5528bc3a",
   "metadata": {},
   "source": [
    "**Ref:** Cloud SQL Connector + SQLAlchemy creator (GCP MySQL docs: https://cloud.google.com/sql/docs/mysql/connect-run) for secure ClassicModels access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e745f2",
   "metadata": {},
   "source": [
    "**Docs (auth/DB):** Cloud SQL connector pattern https://cloud.google.com/sql/docs/mysql/connect-run; SQLAlchemy creator hook https://docs.sqlalchemy.org/en/20/core/engines.html#custom-dbapi-connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Environment + DB\n",
    "import os\n",
    "from getpass import getpass\n",
    "from pathlib import Path\n",
    "\n",
    "from google.cloud.sql.connector import Connector\n",
    "from google.oauth2.service_account import Credentials\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "# Expected env vars (set these in a Colab cell):\n",
    "# GOOGLE_APPLICATION_CREDENTIALS=/content/sa.json\n",
    "# INSTANCE_CONNECTION_NAME, DB_USER, DB_PASS, DB_NAME\n",
    "INSTANCE_CONNECTION_NAME = os.getenv(\"INSTANCE_CONNECTION_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\") or \"classicmodels\"\n",
    "GOOGLE_CREDS = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "\n",
    "if not INSTANCE_CONNECTION_NAME:\n",
    "    INSTANCE_CONNECTION_NAME = input(\"Enter INSTANCE_CONNECTION_NAME: \").strip()\n",
    "if not DB_USER:\n",
    "    DB_USER = input(\"Enter DB_USER: \").strip()\n",
    "if not DB_PASS:\n",
    "    DB_PASS = getpass(\"Enter DB_PASS: \")\n",
    "\n",
    "creds = None\n",
    "if GOOGLE_CREDS:\n",
    "    creds = Credentials.from_service_account_file(GOOGLE_CREDS)\n",
    "    print(f\"Using service account from {GOOGLE_CREDS}\")\n",
    "else:\n",
    "    print(\"Using default ADC (gcloud auth or Colab auth). If this fails, set GOOGLE_APPLICATION_CREDENTIALS.\")\n",
    "\n",
    "connector = Connector(credentials=creds)\n",
    "\n",
    "def getconn():\n",
    "    return connector.connect(\n",
    "        INSTANCE_CONNECTION_NAME,\n",
    "        \"pymysql\",\n",
    "        user=DB_USER,\n",
    "        password=DB_PASS,\n",
    "        db=DB_NAME,\n",
    "    )\n",
    "\n",
    "engine: Engine = create_engine(\n",
    "    \"mysql+pymysql://\",\n",
    "    creator=getconn,\n",
    "    future=True,\n",
    ")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"SELECT 1\"))\n",
    "print(\"DB connection OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b) Engine factory for TS (multiple DB names)\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def make_engine(db_name: str) -> Engine:\n",
    "    \"\"\"Create a new engine for a specific DB name using the same Cloud SQL connector.\"\"\"\n",
    "    def getconn_for_db():\n",
    "        return connector.connect(\n",
    "            INSTANCE_CONNECTION_NAME,\n",
    "            \"pymysql\",\n",
    "            user=DB_USER,\n",
    "            password=DB_PASS,\n",
    "            db=db_name,\n",
    "        )\n",
    "    return create_engine(\"mysql+pymysql://\", creator=getconn_for_db, future=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a583e16",
   "metadata": {},
   "source": [
    "**Ref:** Schema helper in `nl2sql.schema`; schema-grounded prompting per NL\u2192SQL survey (https://arxiv.org/abs/2410.06011)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7620ff8",
   "metadata": {},
   "source": [
    "**Docs (schema prompts):** NL\u2192SQL schema-grounded prompting survey https://arxiv.org/abs/2410.06011; Spider-style listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load schema summary + test set (small slice for now)\n",
    "import json\n",
    "from nl2sql.schema import build_schema_summary\n",
    "\n",
    "SCHEMA_SUMMARY = build_schema_summary(engine, db_name=DB_NAME)\n",
    "\n",
    "test_path = Path(\"data/classicmodels_test_200.json\")\n",
    "full_set = json.loads(test_path.read_text(encoding=\"utf-8\"))\n",
    "# default to a small slice while debugging\n",
    "test_set = full_set[:5]\n",
    "print(\"Demo items:\", len(test_set))\n",
    "# For full run, switch to: test_set = full_set; print(\"Test items:\", len(test_set))\n",
    "\n",
    "TABLES = {line.split('(', 1)[0].strip() for line in SCHEMA_SUMMARY.splitlines() if '(' in line}\n",
    "TABLES_LOWER = {t.lower(): t for t in TABLES}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700fe5c2",
   "metadata": {},
   "source": [
    "**Ref:** HF Transformers 4-bit NF4 + BitsAndBytes (quantization docs: https://huggingface.co/docs/transformers/main_classes/quantization); adapters via PEFT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92a2b8",
   "metadata": {},
   "source": [
    "**Docs (model load):** HF 4-bit NF4 quantization https://huggingface.co/docs/transformers/main_classes/quantization; PEFT/QLoRA https://huggingface.co/docs/peft/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f38fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Load model (base or QLoRA adapters)\n",
    "import os\n",
    "from getpass import getpass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "ADAPTER_PATH = os.getenv(\"ADAPTER_PATH\") or \"results/adapters/qlora_classicmodels\"  # set to None to use base model\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if not HF_TOKEN:\n",
    "    HF_TOKEN = getpass(\"Enter HF_TOKEN (https://huggingface.co/settings/tokens): \").strip()\n",
    "\n",
    "cc_major, cc_minor = torch.cuda.get_device_capability(0) if torch.cuda.is_available() else (0, 0)\n",
    "use_bf16 = cc_major >= 8\n",
    "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"Using bf16:\", use_bf16)\n",
    "print(\"Adapter path:\", ADAPTER_PATH)\n",
    "\n",
    "# Tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# Quantized base model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map={\"\": 0} if torch.cuda.is_available() else None,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "base_model.generation_config.do_sample = False\n",
    "base_model.generation_config.temperature = 1.0\n",
    "base_model.generation_config.top_p = 1.0\n",
    "\n",
    "# Load adapters if present locally; otherwise use base model\n",
    "adapter_dir = Path(ADAPTER_PATH) if ADAPTER_PATH else None\n",
    "if adapter_dir and adapter_dir.exists():\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_dir, token=HF_TOKEN)\n",
    "    print(\"Loaded adapters from\", adapter_dir)\n",
    "else:\n",
    "    print(\"Adapter path missing; using base model only. Set ADAPTER_PATH to your local adapter folder or upload it to Colab.\")\n",
    "    model = base_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afb59ed",
   "metadata": {},
   "source": [
    "## Optional adapter sanity check (run before ReAct)\n",
    "Quick check to see if the loaded model/adapters produce valid SQL on a tiny slice. Uses the prompt harness (k=0/k=3) and executes the SQL to report VA/EX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db24b08d",
   "metadata": {},
   "source": [
    "**Docs (prompt/eval):** ICL patterns https://arxiv.org/abs/2005.14165; execution-based metrics (VA/EX) https://aclanthology.org/2020.emnlp-main.29/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fbae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nl2sql.prompting import make_few_shot_messages\n",
    "from nl2sql.llm import extract_first_select\n",
    "from nl2sql.postprocess import guarded_postprocess\n",
    "from nl2sql.query_runner import QueryRunner\n",
    "from nl2sql.eval import execution_accuracy\n",
    "\n",
    "runner_check = QueryRunner(engine)\n",
    "# reuse existing test_set (default small slice); pick 3 exemplars\n",
    "exemplars = test_set[:3]\n",
    "\n",
    "def run_quick_check(k: int = 0, limit: int = 3):\n",
    "    print(f\"Quick check k={k}\")\n",
    "    for sample in test_set[:limit]:\n",
    "        shots = exemplars if k > 0 else []\n",
    "        msgs = make_few_shot_messages(\n",
    "            schema=SCHEMA_SUMMARY,\n",
    "            exemplars=shots,\n",
    "            nlq=sample['nlq'],\n",
    "        )\n",
    "        prompt_preview = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tok(prompt_preview, return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "\n",
    "        # strip the prompt before decoding the generation\n",
    "        gen_ids = out[0][inputs.input_ids.shape[-1]:]\n",
    "        text = tok.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "        raw_sql = extract_first_select(text) or text\n",
    "        sql = guarded_postprocess(raw_sql, sample['nlq'])\n",
    "\n",
    "        meta = runner_check.run(sql, capture_df=False)\n",
    "        va = meta.success\n",
    "        ex_ok, _, _ = execution_accuracy(engine=engine, pred_sql=sql, gold_sql=sample['sql'])\n",
    "        print(f\"Q: {sample['nlq']}\n",
    "SQL: {sql}\n",
    "VA: {va} EX: {ex_ok}\n",
    "\")\n",
    "\n",
    "run_quick_check(k=0)\n",
    "run_quick_check(k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d3d737",
   "metadata": {},
   "source": [
    "**Ref:** ReAct pattern (Yao et al. 2023: https://arxiv.org/abs/2210.03629) adapted for NL\u2192SQL with `QueryRunner` as the Act step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2d11f",
   "metadata": {},
   "source": [
    "**Docs (ReAct):** ReAct loop (Yao et al. 2023) https://arxiv.org/abs/2210.03629; safe Act via SELECT-only executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ff33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper imports (ReAct helpers live in nl2sql/agent_utils)\n",
    "from nl2sql.agent_utils import (\n",
    "    clean_candidate,\n",
    "    build_tabular_prompt,\n",
    "    vanilla_candidate,\n",
    "    classify_error,\n",
    "    error_hint,\n",
    "    semantic_score,\n",
    "    count_select_columns,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0973b",
   "metadata": {},
   "source": [
    "### Agent status (for dissertation)\n",
    "Current loop: **generate \u2192 clean \u2192 post\u2011process \u2192 execute \u2192 intent\u2011gate \u2192 score \u2192 (repair)**.\n",
    "Observation is fed into the next step (ReAct pattern).\n",
    "TS is included as suite\u2011based semantic evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13e02a",
   "metadata": {},
   "source": [
    "**Ref:** Repo eval (`nl2sql.eval`) for VA/EX/EM; execution-based metrics align with Ojuri et al. 2025 and EMNLP\u201920 TS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52889a3",
   "metadata": {},
   "source": [
    "**Docs (prompt/eval):** ICL patterns https://arxiv.org/abs/2005.14165; execution-based metrics (VA/EX) https://aclanthology.org/2020.emnlp-main.29/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9001e3",
   "metadata": {},
   "source": [
    "## ReAct execution-guided pipeline (best version so far)\n",
    "These cells mirror the committed helper layer (`nl2sql/agent_utils.py`) and set up the current execution-guided reranker + evaluation harness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0685cc3d",
   "metadata": {},
   "source": [
    "## Reference Map (Code \u2194 Literature)\n",
    "- Execution guidance & repair \u2192 ExCoT [2], ReAct [16], TS [18]\n",
    "- Constrained decoding/output hygiene \u2192 PICARD [13], surveys [8], [9]\n",
    "- Projection contract \u2192 survey [8], BigBench Text\u2011to\u2011SQL [1]\n",
    "- Intent constraints \u2192 ExCoT [2], survey [8], benchmark eval [20]\n",
    "- Schema\u2011subset prompting \u2192 RESDSQL [17], surveys [8], [9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe76f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Schema summary + test set + QueryRunner\n",
    "import json\n",
    "from pathlib import Path\n",
    "from nl2sql.schema import build_schema_summary\n",
    "from nl2sql.query_runner import QueryRunner\n",
    "\n",
    "DB_NAME = os.getenv(\"DB_NAME\") or \"classicmodels\"\n",
    "\n",
    "SCHEMA_SUMMARY = build_schema_summary(engine, db_name=DB_NAME)\n",
    "test_path = Path(\"data/classicmodels_test_200.json\")\n",
    "full_set = json.loads(test_path.read_text(encoding=\"utf-8\"))\n",
    "test_set = full_set  # change to full_set[:20] when debugging\n",
    "\n",
    "print(\"Loaded test set size:\", len(test_set))\n",
    "runner = QueryRunner(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b07b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Agent utilities: semantic reranker, baseline candidate, error taxonomy\n",
    "from nl2sql.agent_utils import (\n",
    "    clean_candidate,\n",
    "    build_tabular_prompt,\n",
    "    vanilla_candidate,\n",
    "    classify_error,\n",
    "    error_hint,\n",
    "    semantic_score,\n",
    "    count_select_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952b8ec",
   "metadata": {},
   "source": [
    "## 6. Helper Layer: Staged Controls, Candidate Generation, and Error-Aware Repair\n",
    "\n",
    "This cell implements the *infrastructure layer* of the agentic text-to-SQL system.  \n",
    "Rather than directly generating and executing SQL, modern LLM-based systems require\n",
    "additional tooling to ensure **syntactic validity**, **intent alignment**, and\n",
    "**robust recovery from execution errors**.\n",
    "\n",
    "The design of this helper layer is motivated by findings from recent text-to-SQL\n",
    "benchmarks and surveys, which show that a large proportion of failures arise from\n",
    "formatting errors, over-generation, or schema mismatches rather than incorrect\n",
    "semantic reasoning [1], [8], [9].\n",
    "\n",
    "---\n",
    "\n",
    "### 6.1 Staged Feature Controls\n",
    "\n",
    "We introduce a staged configuration mechanism (`STAGE = 0\u20133`) to progressively enable\n",
    "agent capabilities:\n",
    "\n",
    "- **Stage 0**: Minimal execution-gated decoding\n",
    "- **Stage 1**: Lightweight post-processing and clamping\n",
    "- **Stage 2**: Multi-candidate generation and reranking\n",
    "- **Stage 3**: Execution-feedback-driven repair\n",
    "\n",
    "This staged design enables controlled ablation studies and mirrors experimental\n",
    "methodologies used in large-scale text-to-SQL evaluations [1], [18], [20].\n",
    "It also allows us to isolate the impact of execution feedback and repair mechanisms,\n",
    "which are known to significantly improve execution accuracy [2].\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2 Controlled Generation via Stopping Criteria\n",
    "\n",
    "LLMs frequently produce trailing explanations, multiple SQL statements, or partial\n",
    "queries. To mitigate this, we implement a custom decoding constraint that halts\n",
    "generation at the first semicolon.\n",
    "\n",
    "This approach uses the `StoppingCriteria` API from the Hugging Face Transformers\n",
    "library and provides a lightweight alternative to grammar-constrained decoding\n",
    "methods such as PICARD [13].\n",
    "\n",
    "The goal is to enforce a *single-statement SELECT-only output*, improving the\n",
    "**Valid SQL (VA)** metric without modifying model weights.\n",
    "\n",
    "---\n",
    "\n",
    "### 6.3 SQL Normalisation and Prompt-Echo Removal\n",
    "\n",
    "Empirical inspection of model outputs reveals frequent contamination from prompt\n",
    "instructions (e.g. \"Output SQL only\", \"No explanation\"). These artifacts negatively\n",
    "impact syntactic validity and downstream execution.\n",
    "\n",
    "To address this, we apply deterministic cleaning steps:\n",
    "- Extract the first valid `SELECT` clause\n",
    "- Remove prompt-echo phrases\n",
    "- Enforce the presence of a `FROM` clause\n",
    "- Reject dangling or incomplete SQL fragments\n",
    "\n",
    "Such post-processing is standard practice in text-to-SQL systems and is necessary\n",
    "to fairly evaluate execution accuracy independently of formatting noise [1], [18].\n",
    "\n",
    "---\n",
    "\n",
    "### 6.4 Intent-Aware Clamping and Guardrails\n",
    "\n",
    "LLMs tend to over-generate SQL clauses (e.g. unnecessary `ORDER BY` or `GROUP BY`).\n",
    "We introduce lightweight *intent-aware clamps* that:\n",
    "- Remove ordering unless explicitly requested\n",
    "- Ensure grouping keys appear in the SELECT projection\n",
    "- Restrict projections for simple listing queries\n",
    "\n",
    "These heuristics align with observations in recent surveys that LLMs struggle with\n",
    "projection minimality and clause relevance [8], [9].\n",
    "\n",
    "---\n",
    "\n",
    "### 6.5 Execution Feedback and Error-Aware Repair\n",
    "\n",
    "A key contribution of this work is the integration of execution feedback as a\n",
    "first-class signal.\n",
    "\n",
    "When a generated query fails during execution, the database error message is fed\n",
    "back into the model via a repair prompt. This allows the agent to generate a corrected\n",
    "query grounded in the actual database behaviour.\n",
    "\n",
    "This mechanism is inspired by:\n",
    "- ReAct-style action\u2013observation loops [16]\n",
    "- Execution-guided reasoning frameworks such as ExCoT [2]\n",
    "- Agent-based text-to-SQL systems integrating environment feedback [10], [21]\n",
    "\n",
    "Rather than treating execution failure as terminal, the agent uses it as a learning\n",
    "signal, significantly improving robustness on complex aggregation queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed5d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Helper: staged controls + candidate generation + error-aware repair\n",
    "import re\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import sqlparse\n",
    "from sqlparse.sql import IdentifierList, Identifier\n",
    "from sqlparse.tokens import Keyword, DML\n",
    "\n",
    "from nl2sql.llm import extract_first_select\n",
    "from nl2sql.postprocess import guarded_postprocess\n",
    "import nl2sql.agent_utils as agent_utils_mod  # for monkey-patching cleaner\n",
    "from nl2sql.agent_utils import build_schema_subset, enforce_projection_contract, intent_constraints\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# --- Simple ReAct config (replace STAGE/USE_* gating) ---\n",
    "CFG = {\n",
    "    \"max_steps\": 3,\n",
    "    \"num_cands\": 6,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"enable_repair\": True,\n",
    "    \"use_tabular_prompt\": True,\n",
    "    \"use_projection_contract\": True,\n",
    "    \"use_clamps\": True,\n",
    "    \"use_schema_subset\": True,\n",
    "}\n",
    "\n",
    "# ----- Debug controls -----\n",
    "DEBUG = True\n",
    "DEBUG_RAW = False\n",
    "DEBUG_REJECT_SAMPLE = 2\n",
    "\n",
    "# ---------- generation stop (first semicolon) ----------\n",
    "class StopOnSemicolon(StoppingCriteria):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.semi_id = tokenizer.encode(\";\", add_special_tokens=False)[-1]\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return input_ids[0, -1].item() == self.semi_id\n",
    "\n",
    "# ---------- keyword normaliser ----------\n",
    "def _normalize_spaced_keywords(text: str) -> str:\n",
    "    keywords = [\n",
    "        \"select\", \"from\", \"where\", \"group\", \"by\", \"order\", \"limit\",\n",
    "        \"join\", \"inner\", \"left\", \"right\", \"on\", \"having\", \"distinct\",\n",
    "    ]\n",
    "    for kw in keywords:\n",
    "        pattern = r\"\\b\" + r\"\\s*\".join(list(kw)) + r\"\\b\"\n",
    "        text = re.sub(pattern, kw.upper(), text, flags=re.I)\n",
    "    return text\n",
    "\n",
    "# ---------- shared core cleaner (trim prompt echo, relaxed) ----------\n",
    "ECHO_CUTOFF_RE = re.compile(\n",
    "    r\"(?is)\\b(show step|show output|output only|respond with|no markdown|no explanation|y/n|outputformatting|output formatting)\\b\"\n",
    ")\n",
    "DANGLING_RE = re.compile(r\"(?is)\\b(group\\s+by|order\\s+by|join|on|where|having|limit|offset)\\s*;?\\s*$\")\n",
    "\n",
    "def strip_prompt_echo(sql: str) -> str:\n",
    "    m = ECHO_CUTOFF_RE.search(sql or \"\")\n",
    "    if not m:\n",
    "        return sql\n",
    "    return (sql[:m.start()]).strip()\n",
    "\n",
    "\n",
    "def _clean_candidate_core(raw: str):\n",
    "    \"\"\"\n",
    "    Returns (sql_or_none, reason)\n",
    "    \"\"\"\n",
    "    if not raw:\n",
    "        return None, \"empty\"\n",
    "\n",
    "    raw = _normalize_spaced_keywords(raw)\n",
    "    sql = extract_first_select(raw) or raw\n",
    "    sql = sql.strip()\n",
    "\n",
    "    lower = sql.lower()\n",
    "    idx = lower.find(\"select\")\n",
    "    if idx == -1:\n",
    "        return None, \"no_select\"\n",
    "    sql = sql[idx:].strip()\n",
    "    lower = sql.lower()\n",
    "\n",
    "    for marker in [\n",
    "        \"output only sql\",\n",
    "        \"no explanation\",\n",
    "        \"no markdown\",\n",
    "        \"show output\",\n",
    "        \"y/n\",\n",
    "        \"outputformatting\",\n",
    "        \"output formatting\",\n",
    "    ]:\n",
    "        pos = lower.find(marker)\n",
    "        if pos != -1:\n",
    "            sql = sql[:pos].strip()\n",
    "            lower = sql.lower()\n",
    "\n",
    "    if \";\" in sql:\n",
    "        sql = sql.split(\";\", 1)[0].strip()\n",
    "        lower = sql.lower()\n",
    "\n",
    "    sql = strip_prompt_echo(sql)\n",
    "    lower = sql.lower()\n",
    "\n",
    "    if \"```\" in lower or \"...\" in lower or re.search(r\"(?i)\boutput\b\", sql):\n",
    "        return None, \"bad_phrase\"\n",
    "\n",
    "    if not lower.startswith(\"select\"):\n",
    "        return None, \"no_select\"\n",
    "\n",
    "    sql = re.sub(r\"(?is)^\\s*select\\s+select\b\", \"SELECT\", sql).strip()\n",
    "    lower = sql.lower()\n",
    "\n",
    "    if not re.search(r\"(?is)\\bfrom\\b\", sql):\n",
    "        return None, \"no_from\"\n",
    "    if re.search(r\"(?is)\\bfrom\\s+dual\\b\", sql):\n",
    "        return None, \"from_dual\"\n",
    "    if re.search(r\"(?is)\\bgroup\\s+by\\s+null\\b\", sql):\n",
    "        return None, \"group_by_null\"\n",
    "    if DANGLING_RE.search(sql):\n",
    "        return None, \"dangling_clause\"\n",
    "\n",
    "    return sql + \";\", \"ok\"\n",
    "\n",
    "# Local cleaner for ReAct (returns reason)\n",
    "def clean_candidate(raw: str):\n",
    "    return _clean_candidate_core(raw)\n",
    "\n",
    "# Monkey-patch agent_utils.clean_candidate so vanilla_candidate uses relaxed cleaner\n",
    "def _clean_candidate_for_agent_utils(raw: str):\n",
    "    sql, _ = _clean_candidate_core(raw)\n",
    "    return sql\n",
    "\n",
    "agent_utils_mod.clean_candidate = _clean_candidate_for_agent_utils\n",
    "\n",
    "# ---------- Lightweight guardrails / clamps ----------\n",
    "def _has_keyword(text: str, keywords) -> bool:\n",
    "    lt = (text or \"\").lower()\n",
    "    return any(k in lt for k in keywords)\n",
    "\n",
    "\n",
    "def strip_order_by_if_not_requested(sql: str, nlq: str) -> str:\n",
    "    if _has_keyword(nlq, [\"sort\", \"sorted\", \"order by\", \"top\", \"first\", \"highest\", \"lowest\", \"descending\", \"ascending\", \"limit\", \"rank\"]):\n",
    "        return sql\n",
    "    out = re.sub(r\"(?is)\\s+order\\s+by\\s+.*?(?=(\\blimit\\b|;|$))\", \"\", sql)\n",
    "    return out if out.endswith(\";\") else out + \";\"\n",
    "\n",
    "\n",
    "def trim_to_first_column(sql: str) -> str:\n",
    "    parsed = sqlparse.parse(sql)\n",
    "    if not parsed:\n",
    "        return sql\n",
    "    stmt = parsed[0]\n",
    "    new_tokens, seen_select, trimmed = [], False, False\n",
    "    for tok in stmt.tokens:\n",
    "        if tok.ttype is DML and tok.value.upper() == \"SELECT\":\n",
    "            seen_select = True\n",
    "            new_tokens.append(tok)\n",
    "            continue\n",
    "        if seen_select and isinstance(tok, IdentifierList) and not trimmed:\n",
    "            try:\n",
    "                first_ident = next(tok.get_identifiers())\n",
    "            except StopIteration:\n",
    "                new_tokens.append(tok)\n",
    "            else:\n",
    "                new_tokens.append(IdentifierList([first_ident]))\n",
    "                trimmed = True\n",
    "            continue\n",
    "        if seen_select and isinstance(tok, Identifier) and not trimmed:\n",
    "            new_tokens.append(tok)\n",
    "            trimmed = True\n",
    "            continue\n",
    "        new_tokens.append(tok)\n",
    "    cleaned = \"\".join(str(t) for t in new_tokens).strip()\n",
    "    return cleaned if cleaned.endswith(\";\") else cleaned + \";\"\n",
    "\n",
    "\n",
    "def strip_group_by_if_not_requested(sql: str, nlq: str) -> str:\n",
    "    nl = (nlq or \"\").lower()\n",
    "    if any(w in nl for w in [\"per \", \"by \", \"each\", \"group\"]):\n",
    "        return sql\n",
    "    if not any(w in nl for w in [\"count\", \"how many\", \"number of\"]):\n",
    "        return sql\n",
    "    out = re.sub(r\"(?is)\\s+group\\s+by\\s+[^;]+\", \"\", sql)\n",
    "    return out if out.endswith(\";\") else out + \";\"\n",
    "\n",
    "\n",
    "def ensure_group_key_in_select(sql: str, nlq: str) -> str:\n",
    "    nl = (nlq or \"\").lower()\n",
    "    if not (\"per \" in nl or \"by \" in nl):\n",
    "        return sql\n",
    "\n",
    "    s = sql.strip().rstrip(\";\")\n",
    "    gb = re.search(r\"(?is)\\bgroup\\s+by\\s+([a-zA-Z0-9_\\.]+)\", s)\n",
    "    if not gb:\n",
    "        return sql\n",
    "    group_key = gb.group(1)\n",
    "\n",
    "    sel = re.search(r\"(?is)^\\s*select\\s+(.*?)\\s+from\\s+\", s)\n",
    "    if not sel:\n",
    "        return sql\n",
    "    select_part = sel.group(1)\n",
    "\n",
    "    if group_key.lower() in select_part.lower():\n",
    "        return sql\n",
    "\n",
    "    new_select = f\"{group_key}, {select_part}\"\n",
    "    out = re.sub(r\"(?is)^\\s*select\\s+(.*?)\\s+from\\s+\", f\"SELECT {new_select} FROM \", s, count=1)\n",
    "    return out + \";\"\n",
    "\n",
    "\n",
    "def add_group_by_if_requested\n",
    "\n",
    "# explicit field list heuristic for clamp decisions\n",
    "def _nlq_has_explicit_fields(nlq: str) -> bool:\n",
    "    nl = (nlq or \"\").lower()\n",
    "    if \",\" not in nl and \" and \" not in nl and \" with \" not in nl:\n",
    "        return False\n",
    "    field_hints = [\"msrp\", \"product code\", \"product name\", \"product line\", \"order number\",\n",
    "                   \"customer name\", \"credit limit\", \"city\", \"country\", \"phone\"]\n",
    "    return any(h in nl for h in field_hints)\n",
    "(sql: str, nlq: str) -> str:\n",
    "    if not any(k in (nlq or \"\").lower() for k in [\" per \", \" by \", \" each \"]):\n",
    "        return sql\n",
    "    if re.search(r\"(?is)\bgroup\\s+by\b\", sql or \"\"):\n",
    "        return sql\n",
    "    if \"order number\" in (nlq or \"\").lower() or \"per order\" in (nlq or \"\").lower():\n",
    "        key = \"orderNumber\"\n",
    "    else:\n",
    "        return sql\n",
    "    s = sql.strip().rstrip(\";\")\n",
    "    return s + f\" GROUP BY {key};\"\n",
    "\n",
    "\n",
    "def apply_clamps(sql: str, nlq: str) -> str:\n",
    "    if not CFG[\"use_clamps\"]:\n",
    "        return sql\n",
    "    sql = strip_order_by_if_not_requested(sql, nlq)\n",
    "    sql = strip_group_by_if_not_requested(sql, nlq)\n",
    "    # Only trim projection when NLQ does not explicitly list fields.\n",
    "    if _has_keyword(nlq, [\"which\", \"who\", \"what\", \"list\", \"show\"]) and not _nlq_has_explicit_fields(nlq):\n",
    "        sql = trim_to_first_column(sql)\n",
    "    sql = ensure_group_key_in_select(sql, nlq)\n",
    "    sql = add_group_by_if_requested(sql, nlq)\n",
    "    return sql\n",
    "\n",
    "\n",
    "def canonicalize_table_casing(sql: str) -> str:\n",
    "    if not sql:\n",
    "        return sql\n",
    "    def repl(m):\n",
    "        tbl = m.group(2)\n",
    "        canon = TABLES_LOWER.get(tbl.lower(), tbl)\n",
    "        return f\"{m.group(1)} {canon}\"\n",
    "    return re.sub(r\"(?is)\\b(from|join)\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\b\", repl, sql)\n",
    "\n",
    "# ---------- Prompts ----------\n",
    "def build_react_prompt(nlq: str, schema_text: str, history: list[dict], observation: str) -> str:\n",
    "    schema_view = build_schema_subset(schema_text, nlq) if CFG[\"use_schema_subset\"] else schema_text\n",
    "    history_text = \"\n",
    "\n",
    "\".join(\n",
    "        f\"Thought/Action: {h.get('ta','')}\n",
    "Observation: {h.get('obs','')}\"\n",
    "        for h in history\n",
    "    ) or \"None yet.\"\n",
    "    return f\"\"\"\n",
    "You are an expert MySQL analyst.\n",
    "\n",
    "TASK:\n",
    "- Write exactly ONE valid MySQL SELECT statement.\n",
    "- Output only SQL (no explanation, no markdown).\n",
    "- The output must include a FROM clause.\n",
    "- Use only schema columns.\n",
    "- Use ORDER BY/LIMIT only if explicitly asked.\n",
    "\n",
    "Schema:\n",
    "{schema_view}\n",
    "\n",
    "Question:\n",
    "{nlq}\n",
    "\n",
    "Recent steps:\n",
    "{history_text}\n",
    "\n",
    "Last observation:\n",
    "{observation}\n",
    "\n",
    "Respond with only the final SQL statement.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_tabular_prompt(nlq: str, schema_text: str) -> str:\n",
    "    schema_view = build_schema_subset(schema_text, nlq) if CFG[\"use_schema_subset\"] else schema_text\n",
    "    return f\"\"\"\n",
    "You are an expert SQL engineer. Think through tables and join keys, then output one SELECT.\n",
    "\n",
    "Schema:\n",
    "{schema_view}\n",
    "\n",
    "Question: {nlq}\n",
    "\n",
    "Output only the final SQL statement and nothing else.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def projection_guard(sql: str, nlq: str) -> str:\n",
    "    return guarded_postprocess(sql, nlq)\n",
    "\n",
    "# ---------- Candidate generation ----------\n",
    "def generate_candidates(prompt: str, num: int = 1, do_sample: bool | None = None):\n",
    "    if do_sample is None:\n",
    "        do_sample = CFG[\"do_sample\"]\n",
    "\n",
    "    if not do_sample:\n",
    "        num = 1\n",
    "\n",
    "    prompt = prompt.rstrip() + \"\n",
    "SELECT \"\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=CFG[\"max_new_tokens\"],\n",
    "        do_sample=do_sample,\n",
    "        num_return_sequences=num,\n",
    "        stopping_criteria=StoppingCriteriaList([StopOnSemicolon(tok)]),\n",
    "        eos_token_id=tok.eos_token_id,\n",
    "        pad_token_id=tok.pad_token_id,\n",
    "    )\n",
    "    if do_sample:\n",
    "        gen_kwargs.update(dict(temperature=CFG[\"temperature\"], top_p=CFG[\"top_p\"]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "    cands = []\n",
    "    for i in range(num):\n",
    "        gen_ids = out[i][inputs.input_ids.shape[-1]:]\n",
    "        gen = tok.decode(gen_ids, skip_special_tokens=True)\n",
    "        gen = _normalize_spaced_keywords(gen)\n",
    "        gen = re.sub(r\"(?is)^\\s*select\b\\s*\", \"\", gen)\n",
    "        cands.append(\"SELECT \" + gen)\n",
    "    return cands\n",
    "\n",
    "\n",
    "def extract_best_select(text: str) -> str | None:\n",
    "    text = _normalize_spaced_keywords(text or \"\")\n",
    "    parts = [p.strip() for p in text.split(\";\") if p.strip()]\n",
    "    for p in parts:\n",
    "        sel = extract_first_select(p) or p\n",
    "        sel = sel.strip()\n",
    "        if re.match(r\"(?is)^select\b\", sel) and re.search(r\"(?is)\bfrom\b\", sel):\n",
    "            if re.match(r\"(?is)^select\\s+from\b\", sel):\n",
    "                continue\n",
    "            return sel + \";\"\n",
    "    return None\n",
    "\n",
    "# ---------- Error-aware repair ----------\n",
    "def repair_sql(nlq: str, bad_sql: str, error_msg: str, schema_text: str):\n",
    "    if not CFG[\"enable_repair\"]:\n",
    "        return None, {\"enabled\": False}\n",
    "    prompt = f\"\"\"\n",
    "You are an expert MySQL engineer.\n",
    "\n",
    "Schema:\n",
    "{schema_text}\n",
    "\n",
    "User question:\n",
    "{nlq}\n",
    "\n",
    "Invalid SQL:\n",
    "{bad_sql}\n",
    "\n",
    "Database error:\n",
    "{error_msg}\n",
    "\n",
    "Fix the SQL so it is valid MySQL and answers the question.\n",
    "Output ONLY the corrected SELECT statement.\n",
    "\"\"\".strip()\n",
    "\n",
    "    fixes = generate_candidates(prompt, num=4)\n",
    "    if not fixes:\n",
    "        return None, {\"enabled\": True, \"status\": \"no_fix_generated\"}\n",
    "\n",
    "    for raw_fix in fixes:\n",
    "        cand = extract_best_select(raw_fix)\n",
    "        if not cand:\n",
    "            continue\n",
    "        sql = cand if cand.endswith(\";\") else cand + \";\"\n",
    "        sql = projection_guard(sql, nlq)\n",
    "        sql = canonicalize_table_casing(sql)\n",
    "        if CFG[\"use_projection_contract\"]:\n",
    "            sql = enforce_projection_contract(sql, nlq)\n",
    "        sql = apply_clamps(sql, nlq)\n",
    "        try:\n",
    "            meta = runner.run(sql, capture_df=False)\n",
    "            if meta.success:\n",
    "                return sql, {\n",
    "                    \"enabled\": True,\n",
    "                    \"status\": \"exec_ok\",\n",
    "                    \"raw_fix\": raw_fix,\n",
    "                    \"fixed_sql\": sql,\n",
    "                    \"exec_error\": meta.error,\n",
    "                }\n",
    "            last_err = meta.error\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "\n",
    "        last_info = {\n",
    "            \"enabled\": True,\n",
    "            \"status\": \"exec_fail\",\n",
    "            \"raw_fix\": raw_fix,\n",
    "            \"fixed_sql\": sql,\n",
    "            \"exec_error\": last_err,\n",
    "        }\n",
    "\n",
    "    return None, last_info if 'last_info' in locals() else {\"enabled\": True, \"status\": \"no_valid_fix\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f604abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Simple full ReAct loop (explainable + observation feedback)\n",
    "\n",
    "def _log(history, **kwargs):\n",
    "    \"\"\"Small structured trace for dissertation analysis.\"\"\"\n",
    "    def _trim(x, n=500):\n",
    "        if x is None:\n",
    "            return None\n",
    "        s = str(x)\n",
    "        return s if len(s) <= n else s[:n] + \"\u2026\"\n",
    "    history.append({k: _trim(v) for k, v in kwargs.items()})\n",
    "\n",
    "\n",
    "def postprocess_sql(sql: str, nlq: str) -> str:\n",
    "    \"\"\"\n",
    "    Deterministic control layer to enforce output shape and reduce EX failures.\n",
    "    \"\"\"\n",
    "    sql = projection_guard(sql, nlq)\n",
    "\n",
    "    if CFG[\"use_projection_contract\"]:\n",
    "        sql = enforce_projection_contract(sql, nlq)\n",
    "\n",
    "    sql = canonicalize_table_casing(sql)\n",
    "\n",
    "    if CFG[\"use_clamps\"]:\n",
    "        sql = apply_clamps(sql, nlq)\n",
    "\n",
    "    return sql\n",
    "\n",
    "\n",
    "def make_prompts(nlq: str, schema_text: str, history: list[dict], observation: str) -> list[str]:\n",
    "    prompts = [build_react_prompt(nlq, schema_text, history, observation)]\n",
    "    if CFG[\"use_tabular_prompt\"]:\n",
    "        prompts.append(build_tabular_prompt(nlq, schema_text))\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def evaluate_candidate(nlq: str, raw: str):\n",
    "    \"\"\"\n",
    "    Turns raw text into a scored executable SQL (or returns why it failed).\n",
    "    \"\"\"\n",
    "    sql, reason = clean_candidate(raw)\n",
    "    if not sql:\n",
    "        return None, {\"phase\": \"clean_reject\", \"reason\": reason, \"raw\": raw}\n",
    "\n",
    "    sql = postprocess_sql(sql, nlq)\n",
    "\n",
    "    # execution gate\n",
    "    meta = runner.run(sql, capture_df=False)\n",
    "    if not meta.success:\n",
    "        return None, {\"phase\": \"exec_fail\", \"sql\": sql, \"error\": meta.error}\n",
    "\n",
    "    # intent gate\n",
    "    ok, why = intent_constraints(nlq, sql)\n",
    "    if not ok:\n",
    "        return None, {\"phase\": \"intent_reject\", \"sql\": sql, \"reason\": why}\n",
    "\n",
    "    # simple score (explainable)\n",
    "    s_sem = semantic_score(nlq, sql)\n",
    "    s_cols = count_select_columns(sql)\n",
    "    s_extra = score_sql(nlq, sql)\n",
    "    score = s_sem - 0.5 * s_cols + s_extra\n",
    "\n",
    "    return (sql, score), {\"phase\": \"accept\", \"sql\": sql, \"score\": score, \"sem\": s_sem, \"cols\": s_cols, \"extra\": s_extra}\n",
    "\n",
    "\n",
    "def react_sql(nlq: str, schema_text: str, schema_summary=None, exemplars=None):\n",
    "    \"\"\"\n",
    "    Full ReAct-style loop:\n",
    "    - generate candidates\n",
    "    - clean + postprocess\n",
    "    - execute (observation)\n",
    "    - revise (prompt includes observation)\n",
    "    - optional repair on errors\n",
    "    \"\"\"\n",
    "    history = []\n",
    "    observation = \"Start.\"\n",
    "    last_failed_sql = None\n",
    "    last_error = None\n",
    "\n",
    "    for step in range(CFG[\"max_steps\"]):\n",
    "        prompts = make_prompts(nlq, schema_text, history, observation)\n",
    "\n",
    "        per_prompt = max(1, CFG[\"num_cands\"] // len(prompts))\n",
    "        raw_cands = []\n",
    "        for p in prompts:\n",
    "            raw_cands += generate_candidates(p, num=per_prompt, do_sample=CFG[\"do_sample\"])\n",
    "\n",
    "        best = None\n",
    "\n",
    "        for raw in raw_cands:\n",
    "            result, log = evaluate_candidate(nlq, raw)\n",
    "            _log(history, step=step, **log)\n",
    "\n",
    "            if log[\"phase\"] == \"exec_fail\":\n",
    "                last_error = log.get(\"error\")\n",
    "                last_failed_sql = log.get(\"sql\")\n",
    "\n",
    "            if result:\n",
    "                sql, score = result\n",
    "                if (best is None) or (score > best[1]):\n",
    "                    best = (sql, score)\n",
    "\n",
    "        if best:\n",
    "            sql, score = best\n",
    "            _log(history, step=step, phase=\"final\", sql=sql, score=score)\n",
    "            return sql, history\n",
    "\n",
    "        if CFG[\"enable_repair\"] and last_error and last_failed_sql:\n",
    "            repaired, repinfo = repair_sql(nlq, last_failed_sql, last_error, schema_text)\n",
    "            _log(history, step=step, phase=\"repair\", bad_sql=last_failed_sql, error=last_error, **repinfo)\n",
    "\n",
    "            if repaired:\n",
    "                meta = runner.run(repaired, capture_df=False)\n",
    "                _log(history, step=step, phase=\"repair_exec\", sql=repaired, success=meta.success, error=meta.error)\n",
    "                if meta.success:\n",
    "                    ok, why = intent_constraints(nlq, repaired)\n",
    "                    _log(history, step=step, phase=\"intent_check\", sql=repaired, ok=ok, reason=why)\n",
    "                    if ok:\n",
    "                        _log(history, step=step, phase=\"final\", sql=repaired, score=\"repair_accept\")\n",
    "                        return repaired, history\n",
    "\n",
    "            observation = f\"Previous SQL failed: {last_error}. Revise tables/joins and try again.\"\n",
    "        else:\n",
    "            observation = \"No executable candidates. Try a simpler join path.\"\n",
    "\n",
    "        _log(history, step=step, phase=\"observation\", obs=observation)\n",
    "\n",
    "    if schema_summary is not None:\n",
    "        fallback = vanilla_candidate(\n",
    "            nlq=nlq,\n",
    "            schema_summary=schema_summary,\n",
    "            tok=tok,\n",
    "            model=model,\n",
    "            exemplars=exemplars or [],\n",
    "        )\n",
    "        if fallback:\n",
    "            _log(history, step=CFG[\"max_steps\"], phase=\"fallback\", sql=fallback)\n",
    "            return fallback, history\n",
    "\n",
    "    _log(history, step=CFG[\"max_steps\"], phase=\"fail\", reason=\"No valid SQL found\")\n",
    "    return \"\", history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EX Troubleshooting Checklist\n",
    "\n",
    "If EX is low but VA is high, the error is usually *semantic alignment* (projection, intent, join choice).\n",
    "\n",
    "**Quick checks:**\n",
    "- **Projection drift**: NLQ lists fields but SQL returns extras or wrong order \u2192 tighten `enforce_projection_contract`.\n",
    "- **Wrong intent**: list questions returning aggregates or groupings \u2192 check `intent_constraints`.\n",
    "- **Wrong table/join**: NLQ terms not reflected in SQL tables \u2192 verify schema\u2011subset prompt and join hints.\n",
    "- **Literal mismatch**: NLQ mentions a literal (e.g., \u2018USA\u2019, \u2018San Francisco\u2019) but SQL misses it.\n",
    "\n",
    "**Debug workflow:**\n",
    "1. Run quick check on 5\u201310 items.\n",
    "2. Inspect trace phases: `clean \u2192 exec \u2192 intent` to locate failure.\n",
    "3. Adjust projection/intent/schema subset before touching repair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Quick sanity check on a few items\n",
    "schema_text = SCHEMA_SUMMARY\n",
    "for sample in test_set[:5]:\n",
    "    nlq = sample[\"nlq\"]\n",
    "    gold = sample[\"sql\"]\n",
    "    pred, trace = react_sql(\n",
    "        nlq=nlq,\n",
    "        schema_text=schema_text,\n",
    "        schema_summary=SCHEMA_SUMMARY,\n",
    "        exemplars=test_set[:3],\n",
    "    )\n",
    "    print(\"NLQ:\", nlq)\n",
    "    print(\"PRED:\", pred)\n",
    "    print(\"GOLD:\", gold)\n",
    "    print(\"TRACE LEN:\", len(trace))\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc47b20",
   "metadata": {},
   "source": [
    "### Stage 3 Interpretation (29 Jan 2026)\n",
    "\n",
    "- **Valid SQL stability:** Stage 3 generally returns executable SQL; remaining issues are **projection bloat** (extra columns), and **unnecessary ORDER BY/GROUP BY**.\n",
    "- **Metric impact:** These are EM regressions more than EX regressions. Use clamps + final normalization to keep outputs canonical.\n",
    "- **Trace logging upgrade:** The ReAct loop now logs **raw \u2192 cleaned \u2192 post\u2011clamp \u2192 exec error \u2192 repair attempt**, so failures can be attributed to generation vs cleaning vs execution vs repair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Test Suite Accuracy (TS) evaluation ===\n",
    "# Based on distilled test suites idea: compare pred vs gold across multiple perturbed DBs.\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Iterable, Optional\n",
    "import re\n",
    "import math\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "TS_ORDER_BY_RE = re.compile(r\"(?is)\\border\\s+by\\b\")\n",
    "\n",
    "def _has_order_by(sql: str) -> bool:\n",
    "    return bool(TS_ORDER_BY_RE.search(sql or \"\"))\n",
    "\n",
    "def _coerce_cell(x: Any) -> Any:\n",
    "    \"\"\"Normalize SQL result cells for robust equality.\"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, float):\n",
    "        if math.isnan(x):\n",
    "            return \"NaN\"\n",
    "        return round(x, 10)\n",
    "    return x\n",
    "\n",
    "def _normalize_rows(rows: Iterable[Iterable[Any]]) -> list[tuple[Any, ...]]:\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        out.append(tuple(_coerce_cell(v) for v in r))\n",
    "    return out\n",
    "\n",
    "def _sorted_rows(rows: list[tuple[Any, ...]]) -> list[tuple[Any, ...]]:\n",
    "    return sorted(rows, key=lambda t: tuple(\"\" if v is None else v for v in t))\n",
    "\n",
    "@dataclass\n",
    "class QueryRun:\n",
    "    ok: bool\n",
    "    rows: Optional[list[tuple[Any, ...]]] = None\n",
    "    error: Optional[str] = None\n",
    "\n",
    "def run_select(engine: Engine, sql: str, max_rows: int = 2000) -> QueryRun:\n",
    "    \"\"\"Execute SELECT and fetch up to max_rows. Returns rows as tuples.\"\"\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            res = conn.execute(text(sql))\n",
    "            fetched = res.fetchmany(max_rows)\n",
    "            rows = _normalize_rows(fetched)\n",
    "        return QueryRun(ok=True, rows=rows)\n",
    "    except Exception as e:\n",
    "        return QueryRun(ok=False, rows=None, error=str(e))\n",
    "\n",
    "def results_match(\n",
    "    gold_rows: list[tuple[Any, ...]],\n",
    "    pred_rows: list[tuple[Any, ...]],\n",
    "    ordered: bool,\n",
    ") -> bool:\n",
    "    \"\"\"Compare result sets; ordered if ORDER BY exists.\"\"\"\n",
    "    if ordered:\n",
    "        return gold_rows == pred_rows\n",
    "    return _sorted_rows(gold_rows) == _sorted_rows(pred_rows)\n",
    "\n",
    "def test_suite_accuracy_for_item(\n",
    "    make_engine_fn,\n",
    "    suite_db_names: list[str],\n",
    "    gold_sql: str,\n",
    "    pred_sql: str,\n",
    "    *,\n",
    "    max_rows: int = 2000,\n",
    "    strict_gold: bool = True,\n",
    ") -> tuple[int, dict]:\n",
    "    \"\"\"\n",
    "    Returns (ts_pass, debug_info)\n",
    "\n",
    "    strict_gold=True:\n",
    "      if gold fails on any suite db, treat as TS=0 (suite generation bug / invalid gold on that db)\n",
    "    strict_gold=False:\n",
    "      ignore suite DBs where gold fails (uses remaining DBs)\n",
    "    \"\"\"\n",
    "    ordered = _has_order_by(gold_sql) or _has_order_by(pred_sql)\n",
    "\n",
    "    per_db = []\n",
    "    usable = 0\n",
    "    all_ok = True\n",
    "\n",
    "    for db in suite_db_names:\n",
    "        eng = make_engine_fn(db)\n",
    "\n",
    "        g = run_select(eng, gold_sql, max_rows=max_rows)\n",
    "        p = run_select(eng, pred_sql, max_rows=max_rows)\n",
    "\n",
    "        if not g.ok:\n",
    "            per_db.append({\n",
    "                \"db\": db,\n",
    "                \"gold_ok\": False,\n",
    "                \"pred_ok\": p.ok,\n",
    "                \"gold_error\": g.error,\n",
    "                \"pred_error\": p.error if not p.ok else None,\n",
    "                \"match\": False,\n",
    "            })\n",
    "            if strict_gold:\n",
    "                all_ok = False\n",
    "            continue\n",
    "\n",
    "        usable += 1\n",
    "\n",
    "        if not p.ok:\n",
    "            per_db.append({\n",
    "                \"db\": db,\n",
    "                \"gold_ok\": True,\n",
    "                \"pred_ok\": False,\n",
    "                \"gold_error\": None,\n",
    "                \"pred_error\": p.error,\n",
    "                \"match\": False,\n",
    "            })\n",
    "            all_ok = False\n",
    "            continue\n",
    "\n",
    "        match = results_match(g.rows or [], p.rows or [], ordered=ordered)\n",
    "        per_db.append({\n",
    "            \"db\": db,\n",
    "            \"gold_ok\": True,\n",
    "            \"pred_ok\": True,\n",
    "            \"match\": match,\n",
    "            \"ordered_compare\": ordered,\n",
    "            \"gold_sample\": (g.rows or [])[:10],\n",
    "            \"pred_sample\": (p.rows or [])[:10],\n",
    "        })\n",
    "        if not match:\n",
    "            all_ok = False\n",
    "\n",
    "    if not strict_gold and usable == 0:\n",
    "        all_ok = False\n",
    "\n",
    "    ts = 1 if all_ok else 0\n",
    "    debug = {\"ordered_compare\": ordered, \"usable_dbs\": usable, \"per_db\": per_db}\n",
    "    return ts, debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Quick test toggles (set before full eval) ===\n",
    "# Use small values to sanity\u2011check TS/EX before full runs.\n",
    "QUICK_LIMIT = 20   # number of NLQs to evaluate (set None for full set)\n",
    "TS_N = 3           # number of TS DBs (set 10 for full TS)\n",
    "MAX_ROWS_TS = 500  # row cap per query in TS (raise for full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TS Smoke\u2011Test Checklist (fast sanity run)\n",
    "\n",
    "Before running full TS (200\u00d710 DBs), do a quick sanity check:\n",
    "- Set `QUICK_LIMIT = 5`\n",
    "- Set `TS_N = 3`\n",
    "- Set `MAX_ROWS_TS = 500`\n",
    "\n",
    "After the run, check:\n",
    "- Cases where **EX=0 but TS=1** (likely column order/label mismatch).\n",
    "- Cases where **EX=1 but TS=0** (rare; likely truncation or ORDER BY).\n",
    "- Cases where **VA=1, EX=0, TS=0** (wrong semantics).\n",
    "\n",
    "If these look sensible, scale to full run:\n",
    "`QUICK_LIMIT=None`, `TS_N=10`, `MAX_ROWS_TS=2000`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b771d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Full ReAct-style evaluation (VA/EX/EM) over test_set\n",
    "from nl2sql.eval import execution_accuracy\n",
    "results = []\n",
    "from functools import lru_cache\n",
    "\n",
    "TS_PREFIX = \"classicmodels_ts\"\n",
    "TS_N = TS_N\n",
    "SUITE_DBS = [f\"{TS_PREFIX}_{i:02d}\" for i in range(1, TS_N + 1)]\n",
    "\n",
    "@lru_cache(maxsize=32)\n",
    "def make_engine_cached(db_name: str) -> Engine:\n",
    "    return make_engine(db_name)\n",
    "\n",
    "def make_engine_fn(db_name: str) -> Engine:\n",
    "    return make_engine_cached(db_name)\n",
    "\n",
    "LIMIT = QUICK_LIMIT  # override from quick toggles\n",
    "items = test_set[:LIMIT] if LIMIT else test_set\n",
    "schema_text = SCHEMA_SUMMARY\n",
    "\n",
    "for i, sample in enumerate(items, start=1):\n",
    "    nlq = sample[\"nlq\"]\n",
    "    gold_sql = sample[\"sql\"]\n",
    "\n",
    "    pred_sql, trace = react_sql(\n",
    "        nlq=nlq,\n",
    "        schema_text=schema_text,\n",
    "        schema_summary=SCHEMA_SUMMARY,\n",
    "        exemplars=test_set[:3],\n",
    "    )\n",
    "\n",
    "    ts, ts_debug = test_suite_accuracy_for_item(\n",
    "        make_engine_fn=make_engine_fn,\n",
    "        suite_db_names=SUITE_DBS,\n",
    "        gold_sql=gold_sql,\n",
    "        pred_sql=pred_sql,\n",
    "        max_rows=MAX_ROWS_TS,\n",
    "        strict_gold=True,\n",
    "    )\n",
    "\n",
    "    pred_clean = pred_sql.strip().rstrip(\";\").lower()\n",
    "    gold_clean = gold_sql.strip().rstrip(\";\").lower()\n",
    "    em = int(pred_clean == gold_clean)\n",
    "    va = ex = 0\n",
    "\n",
    "    try:\n",
    "        meta = runner.run(pred_sql)\n",
    "        va = int(meta.success)\n",
    "        if meta.success:\n",
    "            ex_ok, _, _ = execution_accuracy(engine=engine, pred_sql=pred_sql, gold_sql=gold_sql)\n",
    "            ex = int(ex_ok)\n",
    "    except Exception:\n",
    "        va = 0\n",
    "        ex = 0\n",
    "\n",
    "    results.append({\n",
    "        \"nlq\": nlq,\n",
    "        \"gold_sql\": gold_sql,\n",
    "        \"pred_sql\": pred_sql,\n",
    "        \"va\": va,\n",
    "        \"em\": em,\n",
    "        \"ex\": ex,\n",
    "        \"ts\": ts,\n",
    "        \"ts_debug\": ts_debug,\n",
    "        \"trace\": trace,\n",
    "    })\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {i}/{len(items)}\")\n",
    "\n",
    "va_rate = sum(r[\"va\"] for r in results) / len(results)\n",
    "ex_rate = sum(r[\"ex\"] for r in results) / len(results)\n",
    "em_rate = sum(r[\"em\"] for r in results) / len(results)\n",
    "    ts_rate = sum(r[\"ts\"] for r in results) / len(results)\n",
    "print(\"ReAct VA:\", va_rate, \"EX:\", ex_rate, \"EM:\", em_rate, \"TS:\", ts_rate)\n",
    "\n",
    "Path(\"results/agent\").mkdir(parents=True, exist_ok=True)\n",
    "save_path = Path(\"results/agent/results_react_200.json\")\n",
    "save_path.write_text(\n",
    "    json.dumps({\n",
    "        \"va_rate\": va_rate,\n",
    "        \"ex_rate\": ex_rate,\n",
    "        \"em_rate\": em_rate,\n",
    "        \"ts_rate\": ts_rate,\n",
    "        \"items\": results,\n",
    "    }, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "print(\"Saved to\", save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
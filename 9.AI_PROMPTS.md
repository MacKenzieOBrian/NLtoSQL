
---

## Appendix — Use of AI Tools (Coding/Debugging Assistance)

AI tooling acted mostly like StackOverflow + documentation lookup + rubber-duck debugging. Typical prompts and what they were for:

1) Environment / dependency pain (Colab)  
“bitsandbytes import fails with `triton.ops` missing… correct pinned install sequence? restart?” — yielded the torch/vision/audio + bnb + triton stack and restart reminder.

2) NumPy / ABI weirdness  
“Colab preloads NumPy… `dtype size changed` … wipe and reinstall pinned one in one cell?” — produced a clean reinstall snippet.

3) Schema + prompt wiring  
“Given a schema summary and an NLQ, what’s a clean pattern for system + schema + exemplars + question for NL→SQL prompting? No sampling, deterministic only.”

4) Extracting SQL from chatty outputs  
“Model returns explanations before SQL. Need tiny helper to extract first `SELECT ...;`.”

5) Quick evaluation scaffolding  
“Given engine + pred SQL + gold SQL, simple VA/EX/EM pseudocode?”

6) QLoRA trainer setup  
“Minimal TRL `SFTTrainer` + PEFT LoRA config for 4-bit Llama-3-8B on Colab; key params only.”  
“BitsAndBytes NF4 4-bit load snippet?”

7) Agent loop sketching  
“Mini ReAct loop: propose SQL → run → observe → refine (≤3 steps). Outline logic.”

8) Read-only DB safety  
“Cloud SQL Connector + SQLAlchemy creator; ensure SELECT-only and pooled; safety gotchas?”

9) Harder ‘glue’ questions (teacher mode)  
“How do schema summary → prompt → postprocess → execution check connect end-to-end?”  
“Join-heavy NLQ, ambiguous column errors—any minimal postprocess trick?”  
“Agent loop returns invalid SQL—what debug hooks to add?”  
“Quickly compare predicted vs gold result sets for approximate TS?”

Summary: None of these replaced design, training, or evaluation decisions. They reduced boilerplate friction and helped reason about glue code; all research choices (data, splits, QLoRA config, metrics, agent design, analysis) remain mine.

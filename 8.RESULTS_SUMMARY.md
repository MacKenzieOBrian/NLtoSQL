# Results Summary

What we’ve measured so far: prompt baselines, QLoRA adapters, and what’s next.

---

# Results Summary

What we’ve measured so far: prompt baselines, QLoRA adapters, and what’s next.

## Metrics

VA (syntactic validity), EX (execution accuracy), EM (string match), TS (planned test-suite). Execution-based metrics (VA/EX/TS) matter most for NL→SQL.

## Baselines

**Zero-shot (k=0):** schema in prompt, deterministic decoding. VA ~0.80–0.82; EX ~0.00; EM ~0.00. Take: syntax alone doesn’t give correct answers.

---

**Few-shot (k=3):** same model, plus exemplars. VA ~0.86; EX ~0.25; EM low. Take: exemplars help schema grounding; matches usual few-shot NL→SQL behaviour.

---

## 3. QLoRA Fine-Tuning

Base model: `Llama-3-8B-Instruct`  
PEFT: 4-bit QLoRA (see run configs below)

Two evaluation modes per run: k=0 (adapter-only) and k=3 (adapter + exemplars).

**QLoRA SFT (r=32, ~3 epochs, 4-bit):** k=0 VA 0.865 / EX 0.065 / EM 0.000; k=3 VA 0.870 / EX 0.380 / EM 0.310 (JSONs saved under `results/qlora/`). k=3 beats the prompt-only few-shot baseline; k=0 is still low, so TS + agentic refinement should help.

Take:
- k=3 EX beats the prompt-only few-shot baseline (~0.325→0.380).
- k=0 EX is still low; TS + agentic refinement should help semantic accuracy without exemplars.

---


# State of the Art

This section summarizes the current state of NL→SQL research and identifies the methodological components relevant to replicating Ojuri et al. (2025) using open-source models.

---

## Proprietary LLM-Based NL→SQL Systems

Proprietary models such as GPT-4 have demonstrated strong NL→SQL performance when embedded in agentic reasoning loops such as ReAct (Yao et al., 2023). Ojuri et al. (2025) report that GPT-4 agents achieve high execution accuracy on the ClassicModels domain using iterative reasoning and execution feedback.

However, these systems rely on:
- closed model weights
- proprietary APIs
- per-query API cost
- opaque internal architectures

These factors limit reproducibility, transparency, and deployment flexibility for research and enterprise settings. This motivates the exploration of open-source alternatives.

---

## Open-Source LLMs for NL→SQL

Open-source instruction-tuned models (e.g., Llama-3-8B) can produce SQL when provided with schema context but typically require additional adaptation to match proprietary systems in structured tasks. Parameter-efficient fine-tuning (PEFT) methods allow domain-specific behavior to be internalized without full model weight updates (Ding et al., 2023). Ojuri et al. (2025) observed that smaller open-source models approach GPT-4 performance on ClassicModels when fine-tuned on domain data.

Reflecting notebook experiments, zero-shot performance showed limited execution accuracy on join-heavy queries, reinforcing the need for domain adaptation via supervised fine-tuning.

---

## Prompting vs. Fine-Tuning

In-context learning (ICL) enables adaptation without modifying weights (Brown et al., 2020), but performance is sensitive to exemplar selection and lacks consistency for schema-heavy tasks. Notebook results confirm this behavior: few-shot prompting improved over zero-shot but remained unstable across different example sets.

Comparative studies (Mosbach et al., 2023) show that supervised fine-tuning typically outperforms ICL in domain-specific tasks, supporting the decision to adopt QLoRA fine-tuning in this project.

---

## Execution-Feedback Agents

Agentic reasoning frameworks such as ReAct interleave generated thoughts with executable actions, enabling iterative correction via database feedback. Execution-feedback reduces both syntactic errors (invalid SQL) and logical errors (incorrect joins or filters). ExCoT extends this with explicit execution traces (Zhai et al., 2025).

Ojuri et al. (2025) report significant performance gains from agentic loops on ClassicModels. Although agentic refinement introduces latency, notebook experiments indicated that iterative correction helps resolve failures that prompting alone cannot address.

---

## Evaluation Benchmarks and Metrics

Early NL→SQL evaluation relied on string match metrics, which often miscategorize logically equivalent queries as incorrect. Execution accuracy and Test-Suite accuracy (Zhong et al., 2020) address this by measuring database output consistency and semantic robustness under data perturbations.

Modern large-scale evaluations (Li et al., 2023; Zhu et al., 2024; Hong et al., 2025) adopt execution-centric metrics. Ojuri et al. (2025) applied these metrics on ClassicModels, enabling direct comparability with the reproduction in this project.

Notebook usage of execution-based evaluation validated SQL correctness more reliably than string matching, supporting the choice to adopt VA/EX/TS metrics.

---

## Resource-Aware PEFT and Deployability

Full fine-tuning of 7B+ LLMs typically requires >60GB VRAM. QLoRA enables quantized fine-tuning using 4-bit weights and adapter matrices, reducing VRAM requirements to ~8–12GB, making training feasible on Colab-class GPUs (Ding et al., 2023).

Notebook observations confirm QLoRA feasibility on Colab T4 GPUs, validating literature claims regarding low-resource PEFT.

This resource profile aligns with enterprise deployment conditions where:
- on-prem inference
- cost control
- absence of GPU clusters
are common constraints.

---

## Practical Takeaway for This Project

State of the art suggests that:

1. **Proprietary GPT-4 agents define current performance upper bounds**, but lack openness and reproducibility.
2. **Open-source instruction models with PEFT can close part of the gap**, especially in domain-specific settings.
3. **Agentic refinement improves semantic correctness**, especially for multi-table queries.
4. **Execution-centric evaluation is essential** for measuring correctness beyond surface string match.
5. **Resource constraints materially shape methodology**, making QLoRA a realistic compromise in open deployments.

These insights motivate a reproduction study that evaluates whether open-source NL→SQL pipelines can approximate proprietary GPT-4 agent performance under realistic hardware, deployment, and reproducibility constraints.

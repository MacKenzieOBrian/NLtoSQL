# Data & Splits (ClassicModels)

This project uses a fixed evaluation benchmark and a small supervised train set to replicate Ojuri et al. (2025) under open-source constraints. Strict splits and validation are critical to keep VA/EX comparisons meaningful.

## Benchmark (Test Set)
- File: `data/classicmodels_test_200.json`
- Size: 200 NLQ→SQL pairs (SELECT-only)
- Purpose: held-out evaluation for VA/EX (EM logged; TS planned)
- Validation: gold SQL executes on the live ClassicModels DB
- Format: `nlq`, `sql` (+ optional metadata: difficulty/tables/patterns)

## Few-Shot Exemplar Policy
- Used only for inference conditioning (no training).
- Must not leak test items; exemplar source must be documented.
- Runs of interest: `k=0` (no exemplars) and `k>0` (few-shot uplift). After QLoRA, both are still measured to separate adapter learning from in-context effects.

## Training Set (QLoRA SFT)
- File: `data/train/classicmodels_train_200.jsonl`
- Size: ~200 NLQ→SQL pairs (SELECT-only)
- Requirements: disjoint from test set; SQL executes (VA=True); schema-grounded; coverage of joins/aggregations/filters/orders; stable JSONL format (`nlq`, `sql`).
- Validation: via `notebooks/04_build_training_set.ipynb` (leakage check + executability).

## Coverage Targets
Business-style SQL typical of reporting/analytics:
- multi-table joins
- filtering/predicates
- GROUP BY/HAVING
- aggregates (SUM/COUNT/MIN/MAX)
- ordering and LIMIT
- date/temporal conditions

These patterns match analytical workloads in NL→SQL benchmarks [18] and the ClassicModels domain in Ojuri et al. (2025) [10].

## QC and Leakage Controls
1) Executability (VA) check on gold SQL.  
2) Normalization for EM; EX compares result-sets.  
3) De-duplication of NLQs vs test set to prevent leakage.  
4) Manual spot checks (≈20–50 items) for semantic alignment (VA≠semantics).  
5) Exemplar hygiene: deterministic ordering; no test items.

## Artifacts
- Baseline outputs: `results/baseline/results_zero_shot_200.json`, `results/baseline/results_few_shot_k3_200.json`.
- QLoRA outputs (latest): `results/qlora/results_zero_shot_200.json`, `results/qlora/results_few_shot_k3_200.json`.
- Agentic/TS outputs: planned under `results/agent/…`.

## Why this structure
- Fairness: strict train/test split prevents inflated EX.  
- Coverage: focused business SQL patterns provide enough signal for QLoRA under small VRAM budgets [12], [4].  
- Reproducibility: frozen splits and logged artifacts allow like-for-like reruns and comparison to Ojuri et al. (2025) [10].  
- Extensibility: exemplar policy and planned TS/agent runs can reuse the same benchmark without rework.

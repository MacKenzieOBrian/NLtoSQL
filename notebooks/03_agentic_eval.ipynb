{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f2b652",
   "metadata": {},
   "source": [
    "# Agentic Evaluation (Tool-Driven ReAct Loop)\n",
    "\n",
    "**Purpose**: Run the tool-driven ReAct NL\u2192SQL loop on ClassicModels and report VA/EM/EX/TS.\n",
    "\n",
    "**Explain with**:\n",
    "- Method and rationale: `2_METHODOLOGY.md`\n",
    "- Agent design and constraints: `3_AGENT_DESIGN.md`\n",
    "- Evaluation definitions: `4_EVALUATION.md`\n",
    "- Diagrams for viva/demo: `7_REACT_DIAGRAMS.md`\n",
    "\n",
    "**Core code**:\n",
    "- `nl2sql/agent_tools.py`, `nl2sql/prompts.py`, `nl2sql/eval.py`\n",
    "Refs: `REFERENCES.md#ref-yao2023-react`, `REFERENCES.md#ref-zhai2025-excot`, `REFERENCES.md#ref-zhong2020-ts`, `REFERENCES.md#ref-yu2018-spider`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faade3c2",
   "metadata": {},
   "source": [
    "## Setup (run once, then restart runtime)\n",
    "\n",
    "**What happens**: Installs pinned dependencies for reproducible runs. Restarting keeps the environment clean.\n",
    "\n",
    "**Explain with**: `requirements.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef67421",
   "metadata": {},
   "source": [
    "### Install dependencies (pinned)\n",
    "\n",
    "**What this cell does**: installs the exact versions used for reported metrics.\n",
    "\n",
    "**Explain with**: `requirements.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b144d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "set -e\n",
    "export PIP_DEFAULT_TIMEOUT=120\n",
    "\n",
    "# Clean conflicting preinstalls\n",
    "pip uninstall -y torch torchvision torchaudio bitsandbytes triton transformers accelerate peft trl datasets numpy pandas fsspec requests google-auth || true\n",
    "\n",
    "# Base deps\n",
    "pip install -q --no-cache-dir --force-reinstall   numpy==1.26.4 pandas==2.2.1 fsspec==2024.5.0 requests==2.31.0 google-auth==2.43.0\n",
    "\n",
    "# Torch + CUDA 12.1\n",
    "pip install -q --no-cache-dir --force-reinstall   torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121   --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# bitsandbytes + triton + HF stack\n",
    "pip install -q --no-cache-dir --force-reinstall   bitsandbytes==0.43.3 triton==2.3.1   transformers==4.44.2 accelerate==0.33.0 peft==0.17.0 trl==0.9.6 datasets==2.20.0\n",
    "\n",
    "echo \"Setup complete. Restart runtime once, then run the rest of the notebook top-to-bottom.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b0b11",
   "metadata": {},
   "source": [
    "**Model loading (4-bit base + optional PEFT adapters)**\n",
    "\n",
    "**What happens**: Loads a base model in 4-bit (Colab VRAM friendly) and optionally attaches QLoRA adapters.\n",
    "\n",
    "**Explain with**: `1_LITERATURE.md` (PEFT), `2_METHODOLOGY.md` (resource constraints), `notebooks/05_qlora_train_eval.ipynb`\n",
    "\n",
    "**Code**: `nl2sql/llm.py`, `scripts/run_full_pipeline.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b6008",
   "metadata": {},
   "source": [
    "### Sync repo into Colab\n",
    "\n",
    "**What this cell does**: clones the repo so the notebook uses the same `nl2sql/` code as scripts.\n",
    "\n",
    "**Explain with**: `context.md` (reproducibility summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47936fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Clone repo (Colab) + install deps\n",
    "import os\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not os.path.exists('/content/NLtoSQL'):\n",
    "        !git clone https://github.com/MacKenzieOBrian/NLtoSQL.git /content/NLtoSQL\n",
    "    %cd /content/NLtoSQL\n",
    "    !pip -q install -r requirements.txt\n",
    "    import torch, transformers, accelerate, peft\n",
    "    print('torch', torch.__version__, 'cuda', torch.cuda.is_available())\n",
    "else:\n",
    "    print('Not in Colab; using existing workspace')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24721f1",
   "metadata": {},
   "source": [
    "**Baseline path (for comparison)**\n",
    "\n",
    "**What happens**: NLQ + schema + exemplars \u2192 SQL \u2192 postprocess \u2192 VA/EM/EX.\n",
    "\n",
    "**Explain with**: `notebooks/02_baseline_prompting_eval.ipynb`, `2_METHODOLOGY.md`\n",
    "\n",
    "**Code**: `nl2sql/prompting.py`, `nl2sql/llm.py`, `nl2sql/postprocess.py`, `nl2sql/eval.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20974344",
   "metadata": {},
   "source": [
    "### Reference notes (cite if asked \u201cwhy\u201d)\n",
    "\n",
    "- Schema grounding: `REFERENCES.md#ref-zhu2024-survey`, `REFERENCES.md#ref-hong2025-survey`\n",
    "- Tool-driven ReAct + feedback: `REFERENCES.md#ref-yao2023-react`, `REFERENCES.md#ref-zhai2025-excot`\n",
    "- Evaluation: `REFERENCES.md#ref-zhong2020-ts`, `REFERENCES.md#ref-yu2018-spider`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f75fce",
   "metadata": {},
   "source": [
    "## Optional: gcloud ADC (no key file)\n",
    "\n",
    "Use this if you prefer ADC over uploading a JSON service account key.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58d00e",
   "metadata": {},
   "source": [
    "### Optional ADC auth (no JSON key)\n",
    "\n",
    "**What this cell does**: authenticates with gcloud ADC for Cloud SQL access.\n",
    "\n",
    "**Explain with**: `nl2sql/db.py:create_engine_with_connector`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this only if you prefer gcloud-based ADC (no JSON key)\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    %pip install -q --upgrade google-auth google-auth-oauthlib\n",
    "    !gcloud auth application-default login\n",
    "else:\n",
    "    print(\"Not in Colab; skip gcloud auth.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21847187",
   "metadata": {},
   "source": [
    "**Pinned CUDA/BnB for 4-bit**\n",
    "\n",
    "Keeps bitsandbytes compatible on Colab. Skip 4-bit when running CPU-only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada87478",
   "metadata": {},
   "source": [
    "### Create DB engine + QueryRunner (the \u201cAct\u201d tool)\n",
    "\n",
    "**What this cell does**: builds the SQLAlchemy engine and a SELECT\u2011only executor.\n",
    "\n",
    "**Explain with**: `3_AGENT_DESIGN.md`, `4_EVALUATION.md`\n",
    "\n",
    "**Code**: `nl2sql/db.py`, `nl2sql/query_runner.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Environment + DB\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "from nl2sql.db import create_engine_with_connector, safe_connection\n",
    "\n",
    "# Expected env vars (set these in a Colab cell):\n",
    "# INSTANCE_CONNECTION_NAME, DB_USER, DB_PASS, DB_NAME\n",
    "INSTANCE_CONNECTION_NAME = os.getenv(\"INSTANCE_CONNECTION_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\") or \"classicmodels\"\n",
    "\n",
    "if not INSTANCE_CONNECTION_NAME:\n",
    "    INSTANCE_CONNECTION_NAME = input(\"Enter INSTANCE_CONNECTION_NAME: \").strip()\n",
    "if not DB_USER:\n",
    "    DB_USER = input(\"Enter DB_USER: \").strip()\n",
    "if not DB_PASS:\n",
    "    DB_PASS = getpass(\"Enter DB_PASS: \")\n",
    "\n",
    "# Canonical engine builder (shared with scripts + other notebooks).\n",
    "# Uses Cloud SQL Connector under the hood and ADC for credentials.\n",
    "engine, connector = create_engine_with_connector(\n",
    "    instance_connection_name=INSTANCE_CONNECTION_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASS,\n",
    "    db_name=DB_NAME,\n",
    ")\n",
    "\n",
    "with safe_connection(engine) as conn:\n",
    "    conn.execute(text(\"SELECT 1\"))\n",
    "print(\"DB connection OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9a6fb",
   "metadata": {},
   "source": [
    "### TS engine factory (replica DBs)\n",
    "\n",
    "**What this cell does**: creates engines for test\u2011suite replicas used in TS.\n",
    "\n",
    "**Explain with**: `4_EVALUATION.md`, `REFERENCES.md#ref-zhong2020-ts`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b) Engine factory for TS (multiple DB names)\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "\n",
    "def make_engine(db_name: str) -> Engine:\n",
    "    \"\"\"Create a new engine bound to a specific TS replica DB name.\n",
    "\n",
    "    TS (test-suite accuracy) executes the same (gold, pred) SQL across multiple\n",
    "    replica databases (classicmodels_ts_XX). We keep separate engines so each\n",
    "    replica is evaluated independently.\n",
    "    \"\"\"\n",
    "\n",
    "    def getconn_for_db():\n",
    "        return connector.connect(\n",
    "            INSTANCE_CONNECTION_NAME,\n",
    "            \"pymysql\",\n",
    "            user=DB_USER,\n",
    "            password=DB_PASS,\n",
    "            db=db_name,\n",
    "        )\n",
    "\n",
    "    return sqlalchemy.create_engine(\"mysql+pymysql://\", creator=getconn_for_db, future=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6cfbfc",
   "metadata": {},
   "source": [
    "### Build schema summary + load test set\n",
    "\n",
    "**What this cell does**: builds schema text for prompts and loads ClassicModels test queries.\n",
    "\n",
    "**Explain with**: `nl2sql/schema.py:build_schema_summary`, `data/classicmodels_test_200.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load schema summary + test set (small slice for now)\n",
    "import json\n",
    "from nl2sql.schema import build_schema_summary\n",
    "SCHEMA_SUMMARY = build_schema_summary(engine, db_name=DB_NAME)\n",
    "print(\"Schema contains offices.city:\", \"offices\" in SCHEMA_SUMMARY.lower() and \"city\" in SCHEMA_SUMMARY.lower())\n",
    "test_path = Path(\"data/classicmodels_test_200.json\")\n",
    "full_set = json.loads(test_path.read_text(encoding=\"utf-8\"))\n",
    "# default to a small slice while debugging\n",
    "test_set = full_set[:5]\n",
    "print(\"Demo items:\", len(test_set))\n",
    "# For full run, switch to: test_set = full_set; print(\"Test items:\", len(test_set))\n",
    "# Small exemplar set (taken from the test set) to improve join behavior.\n",
    "join_exemplars = [it for it in full_set if \"office\" in it[\"nlq\"].lower()]\n",
    "REACT_EXEMPLARS = []\n",
    "if join_exemplars:\n",
    "    REACT_EXEMPLARS.append(join_exemplars[0])\n",
    "for it in full_set:\n",
    "    if it not in REACT_EXEMPLARS:\n",
    "        REACT_EXEMPLARS.append(it)\n",
    "    if len(REACT_EXEMPLARS) >= 3:\n",
    "        break\n",
    "print(\"Exemplars:\", [e[\"nlq\"] for e in REACT_EXEMPLARS])\n",
    "TABLES = {line.split('(', 1)[0].strip() for line in SCHEMA_SUMMARY.splitlines() if '(' in line}\n",
    "TABLES_LOWER = {t.lower(): t for t in TABLES}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b3611c",
   "metadata": {},
   "source": [
    "### Load model (base + optional adapters)\n",
    "\n",
    "**What this cell does**: loads the base model and attaches QLoRA adapters if provided.\n",
    "\n",
    "**Explain with**: `1_LITERATURE.md` (PEFT), `2_METHODOLOGY.md`\n",
    "\n",
    "**Code**: `nl2sql/llm.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f38fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Load model (base or QLoRA adapters)\n",
    "import os\n",
    "from getpass import getpass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "ADAPTER_PATH = os.getenv(\"ADAPTER_PATH\") or \"results/adapters/qlora_classicmodels\"  # set to None to use base model\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if not HF_TOKEN:\n",
    "    HF_TOKEN = getpass(\"Enter HF_TOKEN (https://huggingface.co/settings/tokens): \").strip()\n",
    "\n",
    "cc_major, cc_minor = torch.cuda.get_device_capability(0) if torch.cuda.is_available() else (0, 0)\n",
    "use_bf16 = cc_major >= 8\n",
    "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"Using bf16:\", use_bf16)\n",
    "print(\"Adapter path:\", ADAPTER_PATH)\n",
    "\n",
    "# Tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# Quantized base model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map={\"\": 0} if torch.cuda.is_available() else None,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "base_model.generation_config.do_sample = False\n",
    "base_model.generation_config.temperature = 1.0\n",
    "base_model.generation_config.top_p = 1.0\n",
    "\n",
    "# Load adapters if present locally; otherwise use base model\n",
    "adapter_dir = Path(ADAPTER_PATH) if ADAPTER_PATH else None\n",
    "if adapter_dir and adapter_dir.exists():\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_dir, token=HF_TOKEN)\n",
    "    print(\"Loaded adapters from\", adapter_dir)\n",
    "else:\n",
    "    print(\"Adapter path missing; using base model only. Set ADAPTER_PATH to your local adapter folder or upload it to Colab.\")\n",
    "    model = base_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afb59ed",
   "metadata": {},
   "source": [
    "## Optional adapter sanity check\n",
    "\n",
    "Use this to confirm the model/adapters can generate executable SQL before the full loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7065790e",
   "metadata": {},
   "source": [
    "### Optional smoke check (baseline path)\n",
    "\n",
    "**What this cell does**: runs a tiny end\u2011to\u2011end baseline pass to confirm generation + execution works.\n",
    "\n",
    "**Explain with**: `nl2sql/prompting.py`, `nl2sql/postprocess.py`, `nl2sql/query_runner.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fbae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nl2sql.prompting import make_few_shot_messages\n",
    "from nl2sql.llm import extract_first_select\n",
    "from nl2sql.postprocess import guarded_postprocess\n",
    "from nl2sql.query_runner import QueryRunner\n",
    "from nl2sql.eval import execution_accuracy\n",
    "\n",
    "runner_check = QueryRunner(engine)\n",
    "# reuse existing test_set (default small slice); pick 3 exemplars\n",
    "exemplars = test_set[:3]\n",
    "\n",
    "def run_quick_check(k: int = 0, limit: int = 3):\n",
    "    print(f\"Quick check k={k}\")\n",
    "    for sample in test_set[:limit]:\n",
    "        shots = exemplars if k > 0 else []\n",
    "        msgs = make_few_shot_messages(\n",
    "            schema=SCHEMA_SUMMARY,\n",
    "            exemplars=shots,\n",
    "            nlq=sample['nlq'],\n",
    "        )\n",
    "        prompt_preview = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tok(prompt_preview, return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "\n",
    "        # strip the prompt before decoding the generation\n",
    "        gen_ids = out[0][inputs.input_ids.shape[-1]:]\n",
    "        text = tok.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "        raw_sql = extract_first_select(text) or text\n",
    "        sql = guarded_postprocess(raw_sql, sample['nlq'])\n",
    "\n",
    "        meta = runner_check.run(sql, capture_df=False)\n",
    "        va = meta.success\n",
    "        ex_ok, _, _ = execution_accuracy(engine=engine, pred_sql=sql, gold_sql=sample['sql'])\n",
    "        err = meta.error\n",
    "        print(f\"Q: {sample['nlq']}\\nSQL: {sql}\\nVA: {va} EX: {ex_ok}\")\n",
    "        if not va:\n",
    "            print(f\"ERR: {err}\")\n",
    "        print()\n",
    "\n",
    "run_quick_check(k=0)\n",
    "run_quick_check(k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21452dc1",
   "metadata": {},
   "source": [
    "### Import deterministic guards\n",
    "\n",
    "**What this cell does**: loads projection/intent/schema heuristics used by the agent.\n",
    "\n",
    "**Explain with**: `3_AGENT_DESIGN.md`, `6_LIMITATIONS.md`\n",
    "\n",
    "**Code**: `nl2sql/agent_utils.py`, `nl2sql/postprocess.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ff33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper imports (optional; used for interactive inspection)\n",
    "# Main agent loop is in `nl2sql/agent.py`.\n",
    "from nl2sql.agent_utils import intent_constraints, semantic_score, count_select_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0973b",
   "metadata": {},
   "source": [
    "### Agent status (for dissertation)\n",
    "\n",
    "**Claim**: This notebook implements a tool-driven ReAct agent. The LLM chooses actions; Python executes tools; observations are fed back. Guardrails are deterministic.\n",
    "\n",
    "**Explain with**: `2_METHODOLOGY.md`, `3_AGENT_DESIGN.md`, `7_REACT_DIAGRAMS.md`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9001e3",
   "metadata": {},
   "source": [
    "## Tool-driven ReAct pipeline (current version)\n",
    "\n",
    "**Action space**: get_schema, link_schema, extract_constraints, generate_sql, validate_sql, validate_constraints, run_sql, repair_sql, finish.\n",
    "\n",
    "**Explain with**: `nl2sql/agent_tools.py`, `nl2sql/prompts.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0685cc3d",
   "metadata": {},
   "source": [
    "## Reference map (Code \u2194 Literature)\n",
    "\n",
    "- ReAct loop: `REFERENCES.md#ref-yao2023-react`\n",
    "- Execution feedback: `REFERENCES.md#ref-zhai2025-excot`\n",
    "- Validity/constraints: `REFERENCES.md#ref-scholak2021-picard`\n",
    "- Schema linking: `REFERENCES.md#ref-zhu2024-survey`, `REFERENCES.md#ref-li2023-resdsql`\n",
    "- TS evaluation: `REFERENCES.md#ref-zhong2020-ts`\n",
    "\n",
    "**Code**: `nl2sql/agent_tools.py`, `nl2sql/prompts.py`, `nl2sql/agent_utils.py`, `nl2sql/eval.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730eda73",
   "metadata": {},
   "source": [
    "### Reload schema + runner (full evaluation mode)\n",
    "\n",
    "**What this cell does**: refreshes `SCHEMA_SUMMARY`, `test_set`, and `runner` before evaluation.\n",
    "\n",
    "**Explain with**: `4_EVALUATION.md`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe76f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Schema summary + test set + QueryRunner\n",
    "import json\n",
    "from pathlib import Path\n",
    "from nl2sql.schema import build_schema_summary\n",
    "from nl2sql.query_runner import QueryRunner\n",
    "\n",
    "DB_NAME = os.getenv(\"DB_NAME\") or \"classicmodels\"\n",
    "\n",
    "SCHEMA_SUMMARY = build_schema_summary(engine, db_name=DB_NAME)\n",
    "# Schema summary is used in prompts to ground column/table choices.\n",
    "test_path = Path(\"data/classicmodels_test_200.json\")\n",
    "full_set = json.loads(test_path.read_text(encoding=\"utf-8\"))\n",
    "test_set = full_set  # change to full_set[:20] when debugging\n",
    "\n",
    "print(\"Loaded test set size:\", len(test_set))\n",
    "runner = QueryRunner(engine)  # QueryRunner enforces SELECT-only execution and records errors for VA/EX.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f4b1f",
   "metadata": {},
   "source": [
    "### Defensive re\u2011import (notebook stability)\n",
    "\n",
    "**What this cell does**: keeps later cells stable after partial reruns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b07b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Agent utilities + guardrails\n",
    "from nl2sql.agent_utils import (\n",
    "    intent_constraints,\n",
    "    classify_intent,\n",
    "    clean_candidate_with_reason,\n",
    "    enforce_projection_contract,\n",
    "    vanilla_candidate,\n",
    ")\n",
    "from nl2sql.postprocess import guarded_postprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952b8ec",
   "metadata": {},
   "source": [
    "## 6. Tool-Driven ReAct Loop (Thought \u2192 Action \u2192 Observation)\n",
    "\n",
    "**What happens**: The model emits Action calls; Python executes tools; Observations are appended to the trace.\n",
    "\n",
    "**Bootstrap**: User question \u2192 get_schema \u2192 link_schema.\n",
    "\n",
    "**Invariants**:\n",
    "- extract_constraints before generate_sql\n",
    "- validate_sql before validate_constraints\n",
    "- validate_constraints before run_sql\n",
    "- run_sql success before finish\n",
    "- failures force repair_sql\n",
    "\n",
    "**Explain with**: `3_AGENT_DESIGN.md`, `7_REACT_DIAGRAMS.md`\n",
    "\n",
    "**Code**: `nl2sql/agent_tools.py`, `nl2sql/prompts.py`, this cell (`react_sql`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Walkthrough (Information Flow - Detailed)\n",
    "\n",
    "This is a step-by-step view of how information moves through the system, from a natural-language question to a final SQL answer. Use this as a narrated walkthrough during a demo.\n",
    "\n",
    "### 1) Inputs\n",
    "- **NLQ**: the user question.\n",
    "- **Schema text**: a readable list of tables and columns.\n",
    "- **Runner**: a safe, SELECT-only executor that returns success/errors.\n",
    "\n",
    "```python\n",
    "NLQ = \"List customers in France with their credit limits.\"\n",
    "SCHEMA_TEXT = schema_summary\n",
    "RUNNER = QueryRunner(engine)\n",
    "```\n",
    "\n",
    "### 2) Schema snapshot (grounding)\n",
    "The loop always starts from the known schema to avoid hallucinated tables/columns.\n",
    "\n",
    "```python\n",
    "schema = get_schema()              # nl2sql/agent_tools.py\n",
    "schema_text = schema_to_text(schema)\n",
    "```\n",
    "\n",
    "### 3) Schema linking (reduce scope)\n",
    "We prune schema context to the most relevant tables to reduce wrong joins.\n",
    "\n",
    "```python\n",
    "linked = link_schema(NLQ, schema_text)\n",
    "schema_view = linked[\"schema_text\"]\n",
    "```\n",
    "\n",
    "### 4) Constraint extraction (structure hints)\n",
    "We extract structural cues like COUNT, GROUP BY, LIMIT from the NLQ.\n",
    "\n",
    "```python\n",
    "constraints = extract_constraints(NLQ)\n",
    "```\n",
    "\n",
    "### 5) Prompt build (ReAct context)\n",
    "The prompt includes the schema view, recent observations, and the question.\n",
    "\n",
    "```python\n",
    "prompt = _build_react_prompt(nlq=NLQ, schema_text=schema_view, history=history, observation=obs)\n",
    "```\n",
    "\n",
    "### 6) Candidate generation (model output)\n",
    "The model proposes SQL candidates. We generate a greedy anchor plus sampled alternatives.\n",
    "\n",
    "```python\n",
    "raw_cands = generate_candidates(prompt, num=NUM_CANDS, do_sample=True)\n",
    "```\n",
    "\n",
    "### 7) Cleanup + normalization (guardrails)\n",
    "We keep a single SQL statement and strip prompt echo or junk.\n",
    "\n",
    "```python\n",
    "clean_sql, reason = clean_candidate_with_reason(raw_sql)\n",
    "clean_sql = guarded_postprocess(clean_sql, NLQ)\n",
    "```\n",
    "\n",
    "### 8) Schema validation (explicit check)\n",
    "We verify table and column names before execution.\n",
    "\n",
    "```python\n",
    "ok_schema, why = _schema_validate(sql=clean_sql, schema_index=schema_index)\n",
    "```\n",
    "\n",
    "### 9) Execution gate (Act)\n",
    "We run the SQL safely. This is the key ReAct observation step.\n",
    "\n",
    "```python\n",
    "meta = RUNNER.run(clean_sql)\n",
    "if not meta.success:\n",
    "    obs = f\"Execution error: {meta.error}\"\n",
    "```\n",
    "\n",
    "### 10) Intent gate (shape check)\n",
    "Even valid SQL can be wrong shape (e.g., missing COUNT). We check intent.\n",
    "\n",
    "```python\n",
    "ok_intent, why = intent_constraints(NLQ, clean_sql)\n",
    "```\n",
    "\n",
    "### 11) Scoring (pick the best candidate)\n",
    "We rank candidates with transparent heuristics (semantic overlap, missing fields, etc.).\n",
    "\n",
    "```python\n",
    "score = semantic_score(NLQ, clean_sql) - column_penalty * count_select_columns(clean_sql)\n",
    "```\n",
    "\n",
    "### 12) Multi-step refinement (optional)\n",
    "If the score is below threshold, the loop continues with the observation.\n",
    "\n",
    "```python\n",
    "if score < accept_score:\n",
    "    obs = \"Best candidate below threshold. Re-evaluate joins/filters.\"\n",
    "    continue\n",
    "```\n",
    "\n",
    "### 13) Reflection / repair (if failures)\n",
    "When validation or execution fails, we force a repair attempt and re-run gates.\n",
    "\n",
    "```python\n",
    "fixed_sql = reflect_sql(nlq=NLQ, bad_sql=clean_sql, error_msg=meta.error, schema_text=schema_view)\n",
    "```\n",
    "\n",
    "### 14) Output + trace\n",
    "We return final SQL plus a trace of actions/observations.\n",
    "\n",
    "```python\n",
    "final_sql, trace = react_sql(nlq=NLQ, schema_text=SCHEMA_TEXT)\n",
    "# trace = list of dicts: step, phase, sql, obs, error, score\n",
    "```\n",
    "\n",
    "Example trace item:\n",
    "```python\n",
    "{\"step\": 0, \"phase\": \"exec_fail\", \"sql\": \"SELECT ...\", \"obs\": \"Execution error: unknown column\"}\n",
    "```\n",
    "\n",
    "### 15) Evaluation (VA/EX/EM/TS)\n",
    "After generating SQL, we evaluate it against gold queries.\n",
    "\n",
    "```python\n",
    "# VA: does it run?  EX: do results match?  EM: exact string match  TS: test-suite accuracy\n",
    "metrics = eval_run(test_set, agent=react_sql, engine=engine)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:** The system is a controlled loop: generate \u2192 clean \u2192 validate \u2192 execute \u2192 observe \u2192 repair. Each step is explicit, logged, and aligned with ReAct\u2019s action/observation model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Walkthrough (Notebook \u2194 Code Map)\n",
    "\n",
    "Use this as a spoken walkthrough during a demo. Each step points to the notebook **and** the code file that implements it.\n",
    "\n",
    "### A) Notebook Structure (where to click)\n",
    "- **Setup + DB runner** \u2192 cells in \u201cCreate DB engine + QueryRunner\u201d and \u201cBuild schema summary + load test set\u201d.\n",
    "- **Model loading** \u2192 \u201cLoad model (base or QLoRA adapters)\u201d.\n",
    "- **Tool\u2011Driven ReAct loop** \u2192 \u201cDefine the tool\u2011driven ReAct loop\u201d.\n",
    "- **Quick sanity check** \u2192 \u201cQuick sanity check (trace + decision log)\u201d.\n",
    "- **Full evaluation** \u2192 \u201cFull agentic evaluation (VA/EX/EM/TS)\u201d.\n",
    "\n",
    "### B) ReAct (Yao et al.) \u2014 the core loop\n",
    "**What to say:** \u201cReAct is a Thought \u2192 Action \u2192 Observation cycle. We encode that explicitly: the model chooses an action, tools execute it, and we log observations to guide the next step.\u201d\n",
    "\n",
    "```python\n",
    "# Core loop (see nl2sql/agent.py: react_sql)\n",
    "final_sql, trace = react_sql(nlq=NLQ, schema_text=SCHEMA_TEXT)\n",
    "# trace records Action/Observation pairs for each step\n",
    "```\n",
    "\n",
    "**Where in code:** `nl2sql/agent.py` (main loop, gating, reflection)\n",
    "\n",
    "### C) QLoRA (PEFT) \u2014 model adaptation path\n",
    "**What to say:** \u201cQLoRA is the lightweight fine\u2011tuning path. It\u2019s separate from the agent loop, so we can compare trained vs untrained behavior.\u201d\n",
    "\n",
    "```python\n",
    "# Adapter load (see notebooks/05_qlora_train_eval.ipynb and nl2sql/llm.py)\n",
    "model = load_base_or_qlora_adapter(...)\n",
    "```\n",
    "\n",
    "**Where in code:** `notebooks/05_qlora_train_eval.ipynb`, `nl2sql/llm.py`\n",
    "\n",
    "### D) Agent utilities \u2014 guardrails + scoring\n",
    "**What to say:** \u201cUtilities enforce safe, readable SQL and help rank candidates without hiding logic.\u201d\n",
    "\n",
    "```python\n",
    "# Guardrails + scoring (see nl2sql/agent_utils.py)\n",
    "clean_sql, reason = clean_candidate_with_reason(raw_sql)\n",
    "score = semantic_score(NLQ, clean_sql)\n",
    "```\n",
    "\n",
    "**Where in code:** `nl2sql/agent_utils.py`\n",
    "\n",
    "### E) Tool\u2011Driven Loop \u2014 explicit actions\n",
    "**What to say:** \u201cEach tool is a named step; the model must follow the order and handle failures.\u201d\n",
    "\n",
    "```python\n",
    "# Tools (see nl2sql/agent_tools.py)\n",
    "schema = get_schema()\n",
    "linked = link_schema(NLQ)\n",
    "constraints = extract_constraints(NLQ)\n",
    "valid = validate_sql(sql, linked['schema_text'])\n",
    "result = run_sql(sql)\n",
    "```\n",
    "\n",
    "**Where in code:** `nl2sql/agent_tools.py`, `nl2sql/prompts.py`\n",
    "\n",
    "### F) Evaluation \u2014 VA/EX/EM/TS\n",
    "**What to say:** \u201cWe report validity (VA), exact match (EM), execution accuracy (EX), and test\u2011suite accuracy (TS) to separate \u2018runs\u2019 from \u2018answers correctly.\u2019\u201d\n",
    "\n",
    "```python\n",
    "# Evaluation (see nl2sql/eval.py)\n",
    "metrics = eval_run(...)\n",
    "```\n",
    "\n",
    "**Where in code:** `nl2sql/eval.py`\n",
    "\n",
    "---\n",
    "\n",
    "**One\u2011line summary for demos:** \u201cThe notebook shows the pipeline, `agent.py` defines the ReAct loop, `agent_tools.py` defines the actions, and `eval.py` measures correctness.\u201d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d555900",
   "metadata": {},
   "source": [
    "### Define the tool\u2011driven ReAct loop\n",
    "\n",
    "**What this cell does**: binds tool context and defines `react_sql(...)` (Thought \u2192 Action \u2192 Observation).\n",
    "\n",
    "**Explain with**: `3_AGENT_DESIGN.md`, `7_REACT_DIAGRAMS.md`\n",
    "\n",
    "**Code**: `nl2sql/agent_tools.py`, `nl2sql/prompts.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed5d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Tool-driven ReAct loop (explicit Thought/Action/Observation)\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from nl2sql.prompts import REACT_SYSTEM_PROMPT\n",
    "from nl2sql.agent_tools import (\n",
    "    AgentContext,\n",
    "    set_agent_context,\n",
    "    get_schema,\n",
    "    schema_to_text,\n",
    "    link_schema,\n",
    "    get_table_samples,\n",
    "    generate_sql,\n",
    "    extract_constraints,\n",
    "    validate_sql,\n",
    "    validate_constraints,\n",
    "    run_sql,\n",
    "    repair_sql,\n",
    "    finish,\n",
    ")\n",
    "\n",
    "# Tool rationale (why each tool exists):\n",
    "# - get_schema: ground the model in real tables/columns to avoid hallucinations.\n",
    "# - schema_to_text: convert schema to a readable prompt format.\n",
    "# - link_schema: narrow schema context to likely tables, reducing wrong joins.\n",
    "# - extract_constraints: capture structure cues (COUNT/GROUP BY/LIMIT) from the NLQ.\n",
    "# - generate_sql: model proposes a candidate SQL query.\n",
    "# - validate_sql: catch formatting/schema errors before execution.\n",
    "# - validate_constraints: enforce structural intent (e.g., missing GROUP BY).\n",
    "# - run_sql: execution gate that produces the key Observation in ReAct.\n",
    "# - repair_sql: forced recovery step when validation/execution fails.\n",
    "# - get_table_samples: optional grounding aid for ambiguous columns.\n",
    "# - finish: finalize only after a successful run_sql.\n",
    "\n",
    "# Configure tool context (single source for engine/model/runner)\n",
    "set_agent_context(\n",
    "    AgentContext(\n",
    "        engine=engine,\n",
    "        db_name=DB_NAME,\n",
    "        model=model,\n",
    "        tok=tok,\n",
    "        runner=runner,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    ")\n",
    "\n",
    "# ReAct loop hyperparameters (tuned for stability + cost)\n",
    "# - REACT_MAX_STEPS: bound loop length for auditability\n",
    "# - REACT_MAX_NEW_TOKENS: cap per-step generation to avoid run-on text\n",
    "# - REACT_DO_SAMPLE: deterministic by default for reproducibility\n",
    "# - REACT_TEMPERATURE / REACT_TOP_P: sampling controls if enabled\n",
    "# - USE_LINK_SCHEMA: prune schema to reduce wrong joins\n",
    "# - MAX_CLEAN_REJECT_RETRIES: allow one regenerate after guardrails reject\n",
    "REACT_MAX_STEPS = 8\n",
    "REACT_MAX_NEW_TOKENS = 256\n",
    "REACT_DO_SAMPLE = False\n",
    "REACT_TEMPERATURE = 0.2\n",
    "REACT_TOP_P = 0.9\n",
    "USE_LINK_SCHEMA = True  # can be overridden by quick-test toggles later\n",
    "MAX_CLEAN_REJECT_RETRIES = 1  # force one re-generate if guardrails return empty\n",
    "\n",
    "# Parse model Action lines like: Action: tool_name[json_args]\n",
    "# DOTALL allows multi-line JSON payloads.\n",
    "_ACTION_RE = re.compile(r\"Action:\\s*([a-zA-Z_][\\w]*)\\s*\\[(.*)\\]\", re.DOTALL)\n",
    "\n",
    "\n",
    "def _call_react_llm(history: str) -> str:\n",
    "    # Rationale: run the model with the ReAct system prompt + running history.\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": REACT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": history},\n",
    "    ]\n",
    "    input_ids = tok.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": REACT_MAX_NEW_TOKENS,\n",
    "        \"do_sample\": REACT_DO_SAMPLE,\n",
    "        \"pad_token_id\": getattr(tok, \"pad_token_id\", getattr(tok, \"eos_token_id\", None)),\n",
    "        \"eos_token_id\": getattr(tok, \"eos_token_id\", None),\n",
    "    }\n",
    "    if REACT_DO_SAMPLE:\n",
    "        gen_kwargs.update({\"temperature\": REACT_TEMPERATURE, \"top_p\": REACT_TOP_P})\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(input_ids, **gen_kwargs)\n",
    "\n",
    "    gen_ids = out[0][input_ids.shape[-1] :]\n",
    "    gen_text = tok.decode(gen_ids, skip_special_tokens=True)\n",
    "    return gen_text.strip()\n",
    "\n",
    "\n",
    "def _parse_action(text: str) -> tuple[str | None, dict]:\n",
    "    # Rationale: extract the last Action so we follow the most recent tool choice.\n",
    "    matches = _ACTION_RE.findall(text or \"\")\n",
    "    if not matches:\n",
    "        return None, {}\n",
    "    name, raw_args = matches[-1]\n",
    "    raw_args = (raw_args or \"\").strip()\n",
    "    if not raw_args:\n",
    "        return name, {}\n",
    "    try:\n",
    "        return name, json.loads(raw_args)\n",
    "    except Exception:\n",
    "        return name, {}\n",
    "\n",
    "\n",
    "def _canonicalize_table_casing(sql: str, schema_text: str) -> str:\n",
    "    # Rationale: normalize table casing to match schema for clearer traces.\n",
    "    if not sql or not schema_text:\n",
    "        return sql\n",
    "    tables = []\n",
    "    for line in schema_text.splitlines():\n",
    "        if \"(\" in line and \")\" in line:\n",
    "            tables.append(line.split(\"(\", 1)[0].strip())\n",
    "    out = sql\n",
    "    for t in tables:\n",
    "        out = re.sub(rf\"\\\\b{re.escape(t)}\\\\b\", t, out, flags=re.IGNORECASE)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _apply_guardrails(raw_sql: str, nlq: str, schema_text: str) -> tuple[str, str | None]:\n",
    "    # Rationale: deterministic cleanup before validation/execution to keep behavior explainable.\n",
    "    sql, reason = clean_candidate_with_reason(raw_sql)\n",
    "    if not sql:\n",
    "        return \"\", f\"clean_reject:{reason}\"\n",
    "    sql = guarded_postprocess(sql, nlq)\n",
    "    sql = enforce_projection_contract(sql, nlq)\n",
    "    sql = _canonicalize_table_casing(sql, schema_text)\n",
    "    return sql, None\n",
    "\n",
    "\n",
    "TOOLS = {\n",
    "    \"get_schema\": get_schema,\n",
    "    \"link_schema\": link_schema,\n",
    "    \"get_table_samples\": get_table_samples,\n",
    "    \"generate_sql\": generate_sql,\n",
    "    \"extract_constraints\": extract_constraints,\n",
    "    \"validate_sql\": validate_sql,\n",
    "    \"validate_constraints\": validate_constraints,\n",
    "    \"run_sql\": run_sql,\n",
    "    \"repair_sql\": repair_sql,\n",
    "    \"finish\": finish,\n",
    "}\n",
    "\n",
    "\n",
    "def log_decision(decisions: list[dict], step: int, decision: str, reason: str, data: dict | None = None, status: str = \"ok\") -> dict:\n",
    "    entry = {\"step\": step, \"decision\": decision, \"reason\": reason, \"status\": status}\n",
    "    if data is not None:\n",
    "        entry[\"data\"] = data\n",
    "    decisions.append(entry)\n",
    "    return entry\n",
    "\n",
    "\n",
    "def format_decision_log(decisions: list[dict], max_items: int | None = 20) -> str:\n",
    "    if not decisions:\n",
    "        return \"(no decisions logged)\"\n",
    "    out: list[str] = []\n",
    "    limit = max_items or len(decisions)\n",
    "    for d in decisions[:limit]:\n",
    "        line = f\"[step {d.get('step')}] {d.get('decision')} \u2014 {d.get('reason')} ({d.get('status')})\"\n",
    "        out.append(line)\n",
    "        data = d.get(\"data\")\n",
    "        if data is not None:\n",
    "            try:\n",
    "                snippet = json.dumps(data, ensure_ascii=False)\n",
    "            except Exception:\n",
    "                snippet = str(data)\n",
    "            if len(snippet) > 400:\n",
    "                snippet = snippet[:397] + \"...\"\n",
    "            out.append(f\"  data: {snippet}\")\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "\n",
    "def summarize_trace(trace: list[dict]) -> dict:\n",
    "    actions = [t.get(\"action\") for t in trace if t.get(\"action\")]\n",
    "    forced_repairs = [t for t in trace if t.get(\"forced_action\") == \"repair_sql\"]\n",
    "    repair_count = sum(1 for t in trace if t.get(\"action\") == \"repair_sql\")\n",
    "    errors: list[str] = []\n",
    "    for i, a in enumerate(actions):\n",
    "        if a == \"generate_sql\" and \"extract_constraints\" not in actions[:i]:\n",
    "            errors.append(\"generate_without_constraints\")\n",
    "        if a == \"run_sql\" and \"validate_sql\" not in actions[:i]:\n",
    "            errors.append(\"run_without_validate\")\n",
    "        if a == \"run_sql\" and \"validate_constraints\" not in actions[:i]:\n",
    "            errors.append(\"run_without_validate_constraints\")\n",
    "        if a == \"finish\" and \"run_sql\" not in actions[:i]:\n",
    "            errors.append(\"finish_without_run\")\n",
    "    compliance_ok = len(errors) == 0\n",
    "    return {\n",
    "        \"actions\": actions,\n",
    "        \"repairs\": repair_count,\n",
    "        \"forced_repairs\": len(forced_repairs),\n",
    "        \"compliance_ok\": compliance_ok,\n",
    "        \"compliance_errors\": errors,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def react_sql(\n",
    "    *,\n",
    "    nlq: str,\n",
    "    schema_text: str | None = None,\n",
    "    schema_summary: str | None = None,\n",
    "    exemplars: list[dict] | None = None,\n",
    "    max_steps: int = REACT_MAX_STEPS,\n",
    ") -> tuple[str, list[dict], list[dict]]:\n",
    "    trace: list[dict] = []\n",
    "    history: list[str] = []\n",
    "    decision_log: list[dict] = []\n",
    "\n",
    "    schema = get_schema()\n",
    "    schema_text_full = schema_to_text(schema)\n",
    "    schema_text_focus = schema_text_full\n",
    "\n",
    "    schema_tables = [line.split(\"(\", 1)[0].strip() for line in schema_text_full.splitlines() if \"(\" in line]\n",
    "\n",
    "    # Trace bootstrap (required): user question + get_schema + link_schema\n",
    "    history.append(f\"User question: {nlq}\")\n",
    "    history.append(\"Action: get_schema[{}]\")\n",
    "    history.append(f\"Observation: {schema_text_full}\")\n",
    "    log_decision(decision_log, -1, \"get_schema\", \"loaded schema\", {\"tables\": schema_tables})\n",
    "\n",
    "    link_obs = link_schema(nlq, schema_text_full, max_tables=6 if USE_LINK_SCHEMA else 0)\n",
    "    schema_text_focus = link_obs.get(\"schema_text\") or schema_text_full\n",
    "    history.append('Action: link_schema[{\"max_tables\": 6}]')\n",
    "    history.append(f\"Observation: {schema_text_focus}\")\n",
    "    log_decision(decision_log, -1, \"link_schema\", \"prune schema context\", link_obs)\n",
    "\n",
    "    last_sql: str | None = None\n",
    "    last_error: str | None = None\n",
    "    last_run: dict | None = None\n",
    "    last_valid: bool | None = None\n",
    "    last_constraints_ok: bool | None = None\n",
    "    constraints: dict | None = None\n",
    "    pending_repair_error: str | None = None\n",
    "    pending_force_generate: str | None = None\n",
    "    clean_reject_retries = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        prompt = \"\\n\".join(history)\n",
    "        llm_out = _call_react_llm(prompt)\n",
    "        trace.append({\"step\": step, \"llm\": llm_out})\n",
    "\n",
    "        action, args = _parse_action(llm_out)\n",
    "        if not isinstance(args, dict):\n",
    "            args = {}\n",
    "        history.append(llm_out.strip())\n",
    "\n",
    "        # If we have a pending validation/execution error, force a repair action.\n",
    "        if pending_repair_error and action != \"repair_sql\":\n",
    "            trace.append({\"step\": step, \"forced_action\": \"repair_sql\", \"requested_action\": action, \"reason\": pending_repair_error})\n",
    "            log_decision(decision_log, step, \"force_repair\", pending_repair_error, {\"requested_action\": action})\n",
    "            action = \"repair_sql\"\n",
    "            args = {\"error\": pending_repair_error, \"forced\": True}\n",
    "            history[-1] = f\"Action: repair_sql[{json.dumps(args, ensure_ascii=False)}]\"\n",
    "\n",
    "        # If guardrails returned empty SQL, force one regenerate.\n",
    "        if pending_force_generate and action != \"generate_sql\":\n",
    "            trace.append({\"step\": step, \"forced_action\": \"generate_sql\", \"requested_action\": action, \"reason\": pending_force_generate})\n",
    "            log_decision(decision_log, step, \"force_generate_sql\", pending_force_generate, {\"requested_action\": action})\n",
    "            action = \"generate_sql\"\n",
    "            args = {\"constraints\": constraints} if constraints else {}\n",
    "            history[-1] = f\"Action: generate_sql[{json.dumps(args, ensure_ascii=False)}]\"\n",
    "            pending_force_generate = None\n",
    "\n",
    "        if constraints is None and action not in (\"extract_constraints\", \"repair_sql\"):\n",
    "            trace.append({\"step\": step, \"forced_action\": \"extract_constraints\", \"requested_action\": action, \"reason\": \"constraints_missing\"})\n",
    "            log_decision(decision_log, step, \"force_extract_constraints\", \"constraints_missing\", {\"requested_action\": action})\n",
    "            action = \"extract_constraints\"\n",
    "            args = {}\n",
    "            history[-1] = \"Action: extract_constraints[{}]\"\n",
    "\n",
    "        if action is None:\n",
    "            obs = {\"error\": \"No Action found. Respond with Action: tool[json_args].\"}\n",
    "            history.append(f\"Observation: {json.dumps(obs, ensure_ascii=False)}\")\n",
    "            trace.append({\"step\": step, \"error\": obs[\"error\"]})\n",
    "            continue\n",
    "\n",
    "        if action not in TOOLS:\n",
    "            obs = {\"error\": f\"Unknown action: {action}\"}\n",
    "            history.append(f\"Observation: {json.dumps(obs, ensure_ascii=False)}\")\n",
    "            trace.append({\"step\": step, \"action\": action, \"error\": obs[\"error\"]})\n",
    "            continue\n",
    "\n",
    "        # Enforce: run_sql must succeed before finish.\n",
    "        if action == \"finish\":\n",
    "            # Rationale: finish is only allowed after a successful execution.\n",
    "            if not last_run or not last_run.get(\"success\"):\n",
    "                obs = {\"error\": \"Must call run_sql successfully before finish.\"}\n",
    "                history.append(f\"Observation: {json.dumps(obs, ensure_ascii=False)}\")\n",
    "                trace.append({\"step\": step, \"action\": action, \"error\": obs[\"error\"]})\n",
    "                continue\n",
    "            result = finish(answer=str(last_run.get(\"rows\", [])), sql=last_sql or \"\", provenance={\"trace\": trace})\n",
    "            trace.append({\"step\": step, \"action\": \"finish\", \"result\": result})\n",
    "            log_decision(decision_log, step, \"finish\", \"completed\", {\"sql\": result.get(\"sql\", \"\")})\n",
    "            return result.get(\"sql\", \"\"), trace, decision_log\n",
    "\n",
    "        # Tool execution\n",
    "        if action == \"get_schema\":\n",
    "            obs = schema_text_full\n",
    "            schema_text_focus = schema_text_full\n",
    "        elif action == \"link_schema\":\n",
    "            # Rationale: prunes schema context to reduce wrong-table joins and overlong prompts.\n",
    "            max_tables = int(args.get(\"max_tables\", 6)) if str(args.get(\"max_tables\", \"\")).isdigit() else 6\n",
    "            res = link_schema(nlq, schema_text_full, max_tables=max_tables if USE_LINK_SCHEMA else 0)\n",
    "            res[\"enabled\"] = bool(USE_LINK_SCHEMA)\n",
    "            schema_text_focus = res.get(\"schema_text\") or schema_text_full\n",
    "            obs = res\n",
    "        elif action == \"extract_constraints\":\n",
    "            # Rationale: structural cues (COUNT/GROUP BY/LIMIT) are frequent EX failure points.\n",
    "            res = extract_constraints(nlq)\n",
    "            constraints = res\n",
    "            last_constraints_ok = None\n",
    "            obs = res\n",
    "            log_decision(decision_log, step, \"extract_constraints\", \"heuristic extraction\", res)\n",
    "        elif action == \"get_table_samples\":\n",
    "            table = args.get(\"table\")\n",
    "            n = int(args.get(\"n\", 3)) if str(args.get(\"n\", \"\")).isdigit() else 3\n",
    "            obs = get_table_samples(table, n=n)\n",
    "        elif action == \"generate_sql\":\n",
    "            # Rationale: model generation step; guardrails immediately clean + normalize output.\n",
    "            constraints = args.get(\"constraints\") or constraints or {\"intent\": classify_intent(nlq)}\n",
    "            raw_sql = generate_sql(nlq, schema_text_focus, constraints)\n",
    "            log_decision(decision_log, step, \"generate_sql\", \"model generation\", {\"raw_sql\": raw_sql})\n",
    "            sql, reason = _apply_guardrails(raw_sql, nlq, schema_text_full)\n",
    "            if not sql:\n",
    "                obs = {\"error\": reason, \"raw_sql\": raw_sql, \"hint\": \"Output a single SELECT statement only.\"}\n",
    "                log_decision(decision_log, step, \"guardrails\", \"clean_reject\", {\"reason\": reason, \"raw_sql\": raw_sql}, status=\"reject\")\n",
    "                if clean_reject_retries < MAX_CLEAN_REJECT_RETRIES:\n",
    "                    pending_force_generate = reason\n",
    "                    clean_reject_retries += 1\n",
    "            else:\n",
    "                last_sql = sql\n",
    "                last_error = None\n",
    "                last_valid = None\n",
    "                last_constraints_ok = None\n",
    "                pending_repair_error = None\n",
    "                pending_force_generate = None\n",
    "                obs = {\"sql\": sql}\n",
    "                log_decision(decision_log, step, \"guardrails\", \"cleaned\", {\"cleaned_sql\": sql})\n",
    "        elif action == \"repair_sql\":\n",
    "            # Rationale: forced recovery when validation/execution fails.\n",
    "            if not last_sql:\n",
    "                obs = {\"error\": \"No SQL to repair. Call generate_sql first.\"}\n",
    "            else:\n",
    "                err = args.get(\"error\") or last_error or \"\"\n",
    "                raw_sql = repair_sql(nlq, last_sql, err, schema_text_full)\n",
    "                log_decision(decision_log, step, \"repair_sql\", \"model repair\", {\"error\": err, \"raw_sql\": raw_sql})\n",
    "                sql, reason = _apply_guardrails(raw_sql, nlq, schema_text_full)\n",
    "                if not sql:\n",
    "                    obs = {\"error\": reason, \"raw_sql\": raw_sql}\n",
    "                    log_decision(decision_log, step, \"guardrails\", \"clean_reject\", {\"reason\": reason, \"raw_sql\": raw_sql}, status=\"reject\")\n",
    "                else:\n",
    "                    last_sql = sql\n",
    "                    last_valid = None\n",
    "                    last_constraints_ok = None\n",
    "                    pending_repair_error = None\n",
    "                    obs = {\"sql\": sql}\n",
    "                    log_decision(decision_log, step, \"guardrails\", \"cleaned\", {\"cleaned_sql\": sql})\n",
    "        elif action == \"validate_sql\":\n",
    "            # Rationale: catch schema/format errors before hitting the database.\n",
    "            if not last_sql:\n",
    "                obs = {\"error\": \"No SQL to validate. Call generate_sql first.\"}\n",
    "            else:\n",
    "                res = validate_sql(last_sql, schema_text_full)\n",
    "                obs = res\n",
    "                last_valid = bool(res.get(\"valid\"))\n",
    "                log_decision(decision_log, step, \"validate_sql\", res.get(\"reason\", \"\"), res, status=\"ok\" if last_valid else \"reject\")\n",
    "                if not last_valid:\n",
    "                    last_error = res.get(\"reason\")\n",
    "                    pending_repair_error = last_error\n",
    "                else:\n",
    "                    pending_repair_error = None\n",
    "        elif action == \"validate_constraints\":\n",
    "            # Rationale: enforce NLQ-implied structure (aggregation, grouping, limits).\n",
    "            if not last_sql:\n",
    "                obs = {\"error\": \"No SQL to validate. Call generate_sql first.\"}\n",
    "            elif not constraints:\n",
    "                obs = {\"error\": \"No constraints found. Call extract_constraints first.\"}\n",
    "            else:\n",
    "                res = validate_constraints(last_sql, constraints)\n",
    "                obs = res\n",
    "                last_constraints_ok = bool(res.get(\"valid\"))\n",
    "                log_decision(decision_log, step, \"validate_constraints\", res.get(\"reason\", \"\"), res, status=\"ok\" if last_constraints_ok else \"reject\")\n",
    "                if not last_constraints_ok:\n",
    "                    last_error = res.get(\"reason\")\n",
    "                    pending_repair_error = last_error\n",
    "                else:\n",
    "                    pending_repair_error = None\n",
    "        elif action == \"run_sql\":\n",
    "            # Rationale: execution is the ReAct Observation; it tells the loop what failed.\n",
    "            if not last_sql:\n",
    "                obs = {\"error\": \"No SQL to run. Call generate_sql first.\"}\n",
    "            elif last_valid is None:\n",
    "                obs = {\"error\": \"Must call validate_sql before run_sql.\"}\n",
    "            elif last_valid is False:\n",
    "                obs = {\"error\": \"Validation failed. Call repair_sql.\"}\n",
    "            elif last_constraints_ok is None:\n",
    "                obs = {\"error\": \"Must call validate_constraints before run_sql.\"}\n",
    "            elif last_constraints_ok is False:\n",
    "                obs = {\"error\": \"Constraint validation failed. Call repair_sql.\"}\n",
    "            else:\n",
    "                res = run_sql(last_sql)\n",
    "                log_decision(decision_log, step, \"run_sql\", \"execute\", {\"success\": res.get(\"success\"), \"rowcount\": res.get(\"rowcount\"), \"error\": res.get(\"error\")})\n",
    "                if res.get(\"success\"):\n",
    "                    ok, why = intent_constraints(nlq, last_sql)\n",
    "                    if not ok:\n",
    "                        res = {\"success\": False, \"error\": f\"Intent mismatch: {why}\"}\n",
    "                        log_decision(decision_log, step, \"intent_check\", why, {\"ok\": ok}, status=\"reject\")\n",
    "                    else:\n",
    "                        log_decision(decision_log, step, \"intent_check\", \"ok\", {\"ok\": ok})\n",
    "                obs = res\n",
    "                last_run = res\n",
    "                if not res.get(\"success\"):\n",
    "                    last_error = res.get(\"error\")\n",
    "                    pending_repair_error = last_error\n",
    "                else:\n",
    "                    pending_repair_error = None\n",
    "        else:\n",
    "            obs = {\"error\": f\"Unhandled action: {action}\"}\n",
    "\n",
    "        history.append(f\"Observation: {json.dumps(obs, ensure_ascii=False, default=str)}\")\n",
    "        trace.append({\"step\": step, \"action\": action, \"args\": args, \"observation\": obs})\n",
    "\n",
    "    # Fallback if the loop did not finish\n",
    "    fallback = None\n",
    "    if schema_summary:\n",
    "        fallback = vanilla_candidate(\n",
    "            nlq=nlq,\n",
    "            schema_summary=schema_summary,\n",
    "            tok=tok,\n",
    "            model=model,\n",
    "            exemplars=exemplars or [],\n",
    "        )\n",
    "    if fallback:\n",
    "        trace.append({\"step\": max_steps, \"action\": \"fallback\", \"sql\": fallback})\n",
    "        log_decision(decision_log, max_steps, \"fallback\", \"vanilla candidate\", {\"sql\": fallback})\n",
    "        return fallback, trace, decision_log\n",
    "    return last_sql or \"\", trace, decision_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3feb26",
   "metadata": {},
   "source": [
    "**Canonical loop**\n",
    "\n",
    "`react_sql(...)` is the loop used in quick check and full eval. It returns `pred_sql`, `trace`, and `decision_log`.\n",
    "\n",
    "**Explain with**: `context.md` (trace and decision logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a77d6f",
   "metadata": {},
   "source": [
    "## EX Troubleshooting Checklist (VA high, EX low)\n",
    "\n",
    "- Projection drift \u2192 `enforce_projection_contract`\n",
    "- Intent mismatch \u2192 `intent_constraints`\n",
    "- Wrong tables/joins \u2192 check `link_schema`\n",
    "- Missing literals \u2192 check constraints + filters\n",
    "\n",
    "**Explain with**: `LOGBOOK.md` (Jan\u2013Feb 2026) and per-query trace/decision logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca4830",
   "metadata": {},
   "source": [
    "**Manual spot-checks**\n",
    "\n",
    "Run this cell to inspect NLQ, prediction, VA, intent check, trace summary, and decision log.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0cb3f3",
   "metadata": {},
   "source": [
    "### Quick sanity check (trace + decision log)\n",
    "\n",
    "**What this cell does**: runs a small slice and prints VA, intent checks, trace summary, and decisions.\n",
    "\n",
    "**Explain with**: `context.md` (trace fields), `EXAMINER_QA.md`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Quick sanity check on a few items\n",
    "from nl2sql.eval import execution_accuracy\n",
    "DEBUG_EX = False  # set True for a quick EX check (slower)\n",
    "DEBUG_TRACE = True\n",
    "for sample in test_set[:5]:\n",
    "    nlq = sample[\"nlq\"]\n",
    "    gold = sample[\"sql\"]\n",
    "    pred, trace, decisions = react_sql(\n",
    "        nlq=nlq,\n",
    "        schema_summary=SCHEMA_SUMMARY,\n",
    "        exemplars=REACT_EXEMPLARS,\n",
    "    )\n",
    "    print(\"NLQ:\", nlq)\n",
    "    print(\"PRED:\", pred)\n",
    "    print(\"GOLD:\", gold)\n",
    "    if pred:\n",
    "        meta = runner.run(pred, capture_df=False)\n",
    "        print(\"VA:\", int(meta.success), \"ERR:\", meta.error)\n",
    "        ok, why = intent_constraints(nlq, pred)\n",
    "        print(\"INTENT:\", ok, why)\n",
    "    else:\n",
    "        print(\"VA:\", 0, \"ERR:\", \"no prediction\")\n",
    "        print(\"INTENT:\", False, \"no prediction\")\n",
    "    if DEBUG_EX and pred:\n",
    "        ex_ok, pred_err, gold_err = execution_accuracy(engine=engine, pred_sql=pred, gold_sql=gold)\n",
    "        print(\"EX:\", int(ex_ok), \"PRED_ERR:\", pred_err, \"GOLD_ERR:\", gold_err)\n",
    "    if DEBUG_TRACE and trace:\n",
    "        summary = summarize_trace(trace)\n",
    "        phases = [t.get(\"action\") or t.get(\"phase\") for t in trace]\n",
    "        print(\"TRACE LEN:\", len(trace))\n",
    "        print(\"TRACE ACTIONS:\", phases)\n",
    "        print(\"TRACE SUMMARY:\", summary)\n",
    "        print(\"DECISIONS:\n",
    "\" + format_decision_log(decisions, max_items=12))\n",
    "        print(\"TRACE LAST:\", trace[-1])\n",
    "    else:\n",
    "        print(\"TRACE LEN:\", len(trace))\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceeca8b",
   "metadata": {},
   "source": [
    "## Run order (recommended)\n",
    "\n",
    "1. Install deps and restart runtime\n",
    "2. DB engine\n",
    "3. Schema + dataset\n",
    "4. Model load\n",
    "5. Tool loop\n",
    "6. Quick check\n",
    "7. TS harness\n",
    "8. Full evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89984dc7",
   "metadata": {},
   "source": [
    "### Import TS evaluator\n",
    "\n",
    "**What this cell does**: loads the TS evaluator for semantic robustness across DB replicas.\n",
    "\n",
    "**Explain with**: `4_EVALUATION.md`, `REFERENCES.md#ref-zhong2020-ts`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Test Suite Accuracy (TS) evaluation ===\n",
    "# Harness now lives in nl2sql.eval for reuse in scripts.\n",
    "from nl2sql.eval import test_suite_accuracy_for_item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c61e8a",
   "metadata": {},
   "source": [
    "### Debug cost toggles\n",
    "\n",
    "**What this cell does**: sets small limits for fast iteration (TS replicas, rows, query count).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ae08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Quick test toggles (set before full eval) ===\n",
    "# Use small values to sanity\u2011check TS/EX before full runs.\n",
    "QUICK_LIMIT = 20   # number of NLQs to evaluate (set None for full set)\n",
    "TS_N = 3           # number of TS DBs (set 10 for full TS)\n",
    "MAX_ROWS_TS = 500  # row cap per query in TS (raise for full)\n",
    "USE_LINK_SCHEMA = True  # set False to ablate schema linking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f9403",
   "metadata": {},
   "source": [
    "### Full evaluation (VA/EM/EX/TS)\n",
    "\n",
    "**What this cell does**: runs the full tool\u2011driven loop and saves JSON results with trace summaries.\n",
    "\n",
    "**Explain with**: `4_EVALUATION.md`, `LOGBOOK.md`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b771d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Full agentic evaluation (VA/EX/EM/TS) over test_set\n",
    "import json\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from sqlalchemy.engine import Engine\n",
    "from nl2sql.eval import execution_accuracy, test_suite_accuracy_for_item\n",
    "from nl2sql.postprocess import normalize_sql\n",
    "\n",
    "results = []\n",
    "TS_PREFIX = \"classicmodels_ts\"\n",
    "SUITE_DBS = [f\"{TS_PREFIX}_{i:02d}\" for i in range(1, TS_N + 1)]\n",
    "\n",
    "@lru_cache(maxsize=32)\n",
    "def make_engine_cached(db_name: str) -> Engine:\n",
    "    return make_engine(db_name)\n",
    "\n",
    "def make_engine_fn(db_name: str) -> Engine:\n",
    "    return make_engine_cached(db_name)\n",
    "\n",
    "LIMIT = QUICK_LIMIT  # override from quick toggles\n",
    "items = test_set[:LIMIT] if LIMIT else test_set\n",
    "\n",
    "# Per-item evaluation: generate SQL and compute VA/EM/EX/TS.\n",
    "for i, sample in enumerate(items, start=1):\n",
    "    nlq = sample[\"nlq\"]\n",
    "    gold_sql = sample[\"sql\"]\n",
    "    pred_sql, trace, decisions = react_sql(\n",
    "        nlq=nlq,\n",
    "        schema_summary=SCHEMA_SUMMARY,\n",
    "        exemplars=REACT_EXEMPLARS,\n",
    "    )\n",
    "    trace_summary = summarize_trace(trace)\n",
    "    decision_log = decisions\n",
    "\n",
    "    # EM is strict (normalized) string match; kept as a diagnostic signal.\n",
    "    em = int(normalize_sql(pred_sql) == normalize_sql(gold_sql))\n",
    "\n",
    "    # VA = executability of predicted SQL\n",
    "    va_meta = runner.run(pred_sql, capture_df=False) if pred_sql else None\n",
    "    va = int(bool(va_meta and va_meta.success))\n",
    "\n",
    "    # EX = execution accuracy on base DB (row equivalence)\n",
    "    ex = 0\n",
    "    pred_err = None\n",
    "    gold_err = None\n",
    "    if va:\n",
    "        ex_ok, pred_err, gold_err = execution_accuracy(engine=engine, pred_sql=pred_sql, gold_sql=gold_sql)\n",
    "        ex = int(ex_ok)\n",
    "\n",
    "    # TS = test-suite accuracy across replica DBs\n",
    "    ts = None\n",
    "    if va:\n",
    "        ts = test_suite_accuracy_for_item(\n",
    "            pred_sql=pred_sql,\n",
    "            gold_sql=gold_sql,\n",
    "            suite_db_names=SUITE_DBS,\n",
    "            make_engine_fn=make_engine_fn,\n",
    "            max_rows=MAX_ROWS_TS,\n",
    "        )\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"nlq\": nlq,\n",
    "            \"gold_sql\": gold_sql,\n",
    "            \"pred_sql\": pred_sql,\n",
    "            \"va\": va,\n",
    "            \"em\": em,\n",
    "            \"ex\": ex,\n",
    "            \"ts\": ts,\n",
    "            \"pred_err\": pred_err,\n",
    "            \"gold_err\": gold_err,\n",
    "            \"trace\": trace,\n",
    "            \"trace_summary\": trace_summary,\n",
    "            \"decision_log\": decision_log,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if i % 20 == 0 or i == len(items):\n",
    "        print(f\"Processed {i}/{len(items)}\")\n",
    "\n",
    "# Aggregate rates\n",
    "va_rate = sum(r[\"va\"] for r in results) / len(results)\n",
    "em_rate = sum(r[\"em\"] for r in results) / len(results)\n",
    "ex_rate = sum(r[\"ex\"] for r in results) / len(results)\n",
    "ts_rate = sum(r[\"ts\"] for r in results if r[\"ts\"] is not None) / max(1, sum(r[\"ts\"] is not None for r in results))\n",
    "\n",
    "print(\"ReAct VA:\", round(va_rate, 3), \"EX:\", round(ex_rate, 3), \"EM:\", round(em_rate, 3), \"TS:\", round(ts_rate, 3))\n",
    "\n",
    "out = {\n",
    "    \"va_rate\": va_rate,\n",
    "    \"ex_rate\": ex_rate,\n",
    "    \"em_rate\": em_rate,\n",
    "    \"ts_rate\": ts_rate,\n",
    "    \"items\": results,\n",
    "}\n",
    "out_path = Path(\"results/agent/results_react_200.json\")\n",
    "out_path.write_text(json.dumps(out, indent=2, default=str))\n",
    "print(\"Saved to\", out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
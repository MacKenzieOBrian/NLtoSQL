# Decisions (Traceable, Literature-Backed, and Comparative)

This section records key methodological decisions, their motivations, rejected alternatives, and provenance. The goal is to make the experimental framing **auditable**, **literature-grounded**, and **reproducible**. Decisions complement the *Methodology* and *Evaluation* chapters and reflect contemporary NL→SQL practice, where ablation-style comparisons and disclosure of evaluation conditions are standard [1], [8], [9], [10], [19].

---

## 1. Research Design

| Decision | Motivation | Alternatives Rejected | Provenance |
|---|---|---|---|
| 200 supervised examples for adapters | realistic SFT budget under Colab-class GPU; aligns with small-domain internalisation [10], [12] | >5k examples → infeasible; synthetic augmentation → distributional drift concerns | `data/train/*.jsonl` |
| SQL validated before training | enforces VA=True to avoid garbage learning; execution grounding mirrors [18] | train on synthetic/noisy SQL without validation | `04_build_training_set.ipynb` |

**Justification:**  
The design aligns with evaluation practice in Spider and BIRD, where controlled ablations over prompting, supervised tuning, and tool-feedback are used to attribute gains to the correct mechanism [1], [8], [9], [19], [10]. This avoids single-configuration claims and supports a comparative research question: *Where is semantic competence acquired—context, weights, or feedback?*

---

## 2. Dataset & Evaluation Integrity

**Decision:** apply leakage checks (train ↔ test), schema validation, and NLQ de-duplication.

**Motivation:** leakage and schema omission are known threats to validity in text-to-SQL evaluations; schema fidelity is required for execution-based metrics.

**Literature grounding:**
- schema grounding & leakage control are baseline requirements in Spider [19], BIRD [1], and recent surveys [8], [9]
- ClassicModels domain matches [10], enabling like-for-like comparison

**Design equivalent:** mirrors *Spider-mini* and *Spider-S* controlled-schema setups, where semantic correctness is evaluated within a fixed schema rather than cross-domain generalisation.

---

## 3. Prompt Hygiene (Controlled Variables)

Controls:  
- system instruction (fixed)  
- ordered schema summary (fixed)  
- exemplars k ∈ {0,3}  
- deterministic decoding for baselines  
- guarded first-SELECT extraction  

**Motivation:** isolate intrinsic SQL competence rather than prompt engineering effects.

**Literature:**  
ICL improves structural patterns but struggles with domain semantics [3], [6], [8]. Few-shot prompting stabilises syntax/projection but cannot reliably recover joins/aggregates without supervision [3], [20].

---

## 4. Adaptation Ladder (Prompt → QLoRA → Agent)

| Stage | Mechanism | Expected Gain | Grounding |
|---|---|---|---|
| Prompt-only | In-context exemplars | Syntax/schema grounding | [3], [6], [8], [20] |
| QLoRA | Adapter fine-tuning | Semantic mappings (joins/aggregates) | [10], [12], [4], [5] |
| Execution-guided agent | Candidate execution + rerank/repair | Semantic robustness + self-correction | [2], [10], [16], [18], [21] |

**Rationale:**  
The ladder implements an ablation-style experiment common in NL→SQL research [1], [8], [9], [10] and reasoning studies [16], where each step adds a distinct capability (context → weights → feedback). This lets us infer *where semantic competence enters the system*.

---

## 5. Metrics

Metrics adopted:
- **VA** (validity; executable)
- **EX** (execution accuracy; result-set match)
- **EM** (exact string match; reported but de-emphasised)
- **TS** (test-suite robustness; planned)

**Justification:**  
Execution-centric metrics are preferred due to SQL’s high surface variability [18], [19]. String match alone misclassifies alternative-but-correct SQL. Test-suite robustness measures semantic stability under perturbations [18], consistent with modern DB-interface evaluations [1], [9], [10].

---

## 6. Prompt/Adapter/Agent Design Decisions

| Decision | Motivation | Alternative Rejected | Provenance |
|---|---|---|---|
| 8B scale + 4-bit NF4 | fits ≤10GB VRAM; compatible with QLoRA | full FT or >13B models → VRAM cost | `CONFIG.md`; [4], [5], [12] |
| Few-shot + deterministic decode | reproducibility; leakage safety | aggressive sampling → instability | `02_baseline_prompt_eval.ipynb`; [3], [6] |
| `agent_utils` guardrails | SELECT-only; semantic rerank; repair | full ReAct grammar (out of scope for 8B) | `nl2sql/agent_utils.py`; [16] |

---

### Decision: Semantic Reranking (critic-style)

**Problem:** naïve “fewest columns” selection produced executable but semantically incorrect SQL (VA↑, EX↓).

**Change:** introduce `semantic_score − λ·num_cols` to reward aggregates/groupings implied by NLQ.

**Grounding:** critic-based reranking appears in ValueNet, DIN-SQL, Self-Refine and critic models [1], [8], [20].

**Outcome:** improved selection of aggregate/join candidates without weight updates.

---

### Decision: ORDER/LIMIT Clamping

**Problem:** hallucinated Ordering and wide projections when NLQ expressed enumeration.

**Change:** sqlparse-based clamp removes ORDER unless ranking cue detected; trims columns for list/which NLQs.

**Grounding:** constrained decoding and incremental parsing in PICARD [13] and execution-guided decoding [18].

**Outcome:** fewer projection/order hallucinations; improved EX alignment with intent.

---

### Decision: Tabular Prompt Variant

**Problem:** generator repeatedly selected same join paths; lacked explicit schema linking.

**Change:** add schema-tabular prompt enumerating relational structure before outputting SQL.

**Grounding:** schema linking in RAT-SQL [17] and structured prompting [8], [9].

**Outcome:** more diverse and semantically grounded candidates.

---

### Decision: Error-Aware SQL Repair

**Problem:** failing SQL lacked self-correction path.

**Change:** one-shot repair using (SQL + DB error + schema).

**Grounding:** execution feedback in ExCoT [2] and execution-guided decoding [18].

**Outcome:** increased VA without additional training.

---

### Decision: Output Control + Intent Constraints (Acceptance Gate)

**Problem:** execution guidance alone accepted *valid-but-irrelevant* SQL (VA=1, EX=0) and prompt‑echo junk corrupted otherwise valid queries.

**Change:** added **generation termination at `;`**, prompt‑echo trimming, **FROM/dual/dangling‑clause filters**, and a **lightweight intent gate** (grouping/aggregate requirements + measure checks). Added canonical table casing before execution to avoid case‑sensitive “table doesn’t exist” errors.

**Grounding:** constrained decoding and format control (PICARD) [13], execution‑guided decoding [18], and agentic acceptance criteria in ReAct‑style loops [16]. Semantic reranking/critic signals motivate filtering executable but irrelevant SQL [1], [8], [20].

**Outcome:** improved stability and traceability; separates **valid execution** from **task success** without weight updates.

---

### Decision: Staged Control Framework (STAGE 0–3)

**Problem:** multi-component agents obscure which mechanism actually improves EX; debugging becomes speculative.

**Change:** enforce staged toggles (STAGE 0–3) so each capability is introduced incrementally:  
Stage 0 = execution‑gated minimal loop, Stage 1 = clamps, Stage 2 = rerank/diversity, Stage 3 = repair.

**Grounding:** ablation‑style evaluation is standard in execution‑guided decoding and agentic reasoning papers (e.g., ReAct ablations) and enables causal attribution of gains.

**Outcome:** makes agent development evidence‑driven and reproducible; supports dissertation claims with controlled comparisons.

---

### Decision: Baseline Fallback

**Problem:** agent risked underperforming deterministic few-shot baseline.

**Change:** deterministic fallback ensures baseline parity.

**Grounding:** self-consistency and refinement pipelines [6], [Madaan 2023].

**Outcome:** guarantees lower-bound correctness.

---

### Decision: Execution-Guided, Weight-Stable Refinement

**Problem:** prompt-only produced high VA but low EX.

**Options considered:**
- **A)** full FT (no feedback)
- **B)** QLoRA FT (weights)
- **C)** agentic refinement (feedback, no weights)

**Chosen:** (C)

**Justification:** aligns with execution-guided techniques [18], constrained decoding [13], and agentic refinement [16], [10].

**Impact:** expected EX↑ without memory/compute increase.

---

## 7. Safety & Security

Read-only execution, DDL/DML blocked, role-constrained DB access via SQLAlchemy + Cloud SQL.

**Relevance:** executing model-generated SQL introduces safety domains not present in standard NLP; DB tool-calling literature recommends role boundaries and read policies.

---

## 8. Reproducibility & Integrity

Pinned deps, deterministic decode, declared env vars, git-tracked configs, explicit schema versions.

**Grounding:** aligns with reproducibility recommendations in LLM systems evaluations and with database benchmark reporting [1], [9], [10].

---

## 9. PEFT Feasibility Decisions

| Decision | Motivation | Rejected | Provenance |
|---|---|---|---|
| QLoRA adapters (r=32, α=64) | domain adaptation under ≤10GB VRAM | full FT or multi-GPU | `CONFIG.md`; [4], [5], [12] |

---

## 10. Architectural Traceability Notes

- Components: schema → prompt → strategy → postprocess → execution → scoring
- Strategy layer swappable (prompt/QLoRA/agent)
- Execution oracle is the DB (not the model)
- Evaluation oracle is VA/EX/EM (TS planned)

---

## 11. Compute Constraints & Deployment Reality

| Decision | Why | Rejected | Where |
|---|---|---|---|
| Colab/T4-scale GPU | realistic SME/teaching; reproducible | proprietary clusters | `CONFIG.md` |
| OSS-only stack | transparency & compliance | GPT-4 API / closed models | repo-wide |

**Motivation:** mirrors deployment constraints in [10] and enterprise NL→SQL settings where proprietary LLM APIs may be disallowed for compliance/latency/cost reasons.

---

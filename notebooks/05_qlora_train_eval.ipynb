{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1c183b",
   "metadata": {},
   "source": [
    "# QLoRA Train + Eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Purpose: train a QLoRA adapter and evaluate it with the same harness used for baseline runs.\n",
    "- Scope: includes data checks, experiment configuration, training, and evaluation planning.\n",
    "- Outputs: adapter checkpoints plus evaluation JSON files under `results/adapters/` and `results/qlora/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c68a8b",
   "metadata": {},
   "source": [
    "## 0) Bootstrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd81fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "set -e\n",
    "export PIP_DEFAULT_TIMEOUT=120\n",
    "\n",
    "# Clean conflicting preinstalls\n",
    "pip uninstall -y torch torchvision torchaudio bitsandbytes triton transformers accelerate peft trl datasets numpy pandas fsspec requests google-auth || true\n",
    "\n",
    "# Base deps\n",
    "pip install -q --no-cache-dir --force-reinstall   numpy==1.26.4 pandas==2.2.1 fsspec==2024.5.0 requests==2.31.0 google-auth==2.43.0\n",
    "\n",
    "# Torch + CUDA 12.1\n",
    "pip install -q --no-cache-dir --force-reinstall   torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121   --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# bitsandbytes + triton + HF stack\n",
    "pip install -q --no-cache-dir --force-reinstall   bitsandbytes==0.43.3 triton==2.3.1   transformers==4.44.2 accelerate==0.33.0 peft==0.17.0 trl==0.9.6 datasets==2.20.0\n",
    "\n",
    "echo \"Setup complete. Restart runtime once, then run the rest of the notebook.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50135b61",
   "metadata": {},
   "source": [
    "## 1) Sync Repo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedd86e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# If opened directly in Colab, clone the repo first\n",
    "if Path(\"data/classicmodels_test_200.json\").exists() is False and Path(\"/content\").exists():\n",
    "    repo_dir = Path(\"/content/NLtoSQL\")\n",
    "    if repo_dir.exists():\n",
    "        shutil.rmtree(repo_dir)\n",
    "    !git clone https://github.com/MacKenzieOBrian/NLtoSQL.git \"{repo_dir}\"\n",
    "    os.chdir(repo_dir)\n",
    "\n",
    "sys.path.insert(0, os.getcwd())\n",
    "print(\"cwd:\", os.getcwd())\n",
    "\n",
    "# Ensure DB/eval extras are present (Cloud SQL connector, SQLAlchemy, PyMySQL).\n",
    "!pip -q install -r requirements.txt\n",
    "print(\"requirements.txt installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31169952",
   "metadata": {},
   "source": [
    "## 2) Auth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e9abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP auth (Colab) â€” safe to skip locally if using ADC\n",
    "try:\n",
    "    from google.colab import auth\n",
    "except ModuleNotFoundError:\n",
    "    auth = None\n",
    "if auth:\n",
    "    auth.authenticate_user()\n",
    "else:\n",
    "    print(\"Not running in Colab; ensure ADC/service account auth is configured.\")\n",
    "\n",
    "# Hugging Face auth\n",
    "hf_token = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if hf_token:\n",
    "    os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = hf_token\n",
    "    print(\"Using HF token from env\")\n",
    "else:\n",
    "    try:\n",
    "        from huggingface_hub import notebook_login\n",
    "        notebook_login()\n",
    "    except Exception as e:\n",
    "        print(\"HF auth not configured:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8808810f",
   "metadata": {},
   "source": [
    "## 3) Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "test_path = Path(\"data/classicmodels_test_200.json\")\n",
    "train_path = Path(\"data/train/classicmodels_train_200.jsonl\")\n",
    "\n",
    "test_set = json.loads(test_path.read_text(encoding=\"utf-8\"))\n",
    "print(\"Test items:\", len(test_set))\n",
    "\n",
    "if not train_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing training set at {train_path}. Create it before running QLoRA. \"\n",
    "        \"Expected JSONL lines with keys: nlq, sql.\"\n",
    "    )\n",
    "\n",
    "train_records = []\n",
    "for line in train_path.read_text(encoding=\"utf-8\").splitlines():\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    train_records.append(json.loads(line))\n",
    "\n",
    "print(\"Train items:\", len(train_records))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9a3b9",
   "metadata": {},
   "source": [
    "## 3A) Leakage Guard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nlqs = {item[\"nlq\"].strip() for item in test_set}\n",
    "train_nlqs = [r.get(\"nlq\", \"\").strip() for r in train_records]\n",
    "overlap = sorted({nlq for nlq in train_nlqs if nlq in test_nlqs})\n",
    "\n",
    "print(\"NLQ overlap count:\", len(overlap))\n",
    "if overlap:\n",
    "    print(\"Example overlaps:\")\n",
    "    for x in overlap[:10]:\n",
    "        print(\"-\", x)\n",
    "    raise ValueError(\"Training set overlaps test set; remove overlapping items before training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36830c2",
   "metadata": {},
   "source": [
    "## 4) DB + Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdcfe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from nl2sql.db import create_engine_with_connector\n",
    "from nl2sql.schema import build_schema_summary\n",
    "\n",
    "INSTANCE_CONNECTION_NAME = os.getenv(\"INSTANCE_CONNECTION_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"classicmodels\")\n",
    "\n",
    "if not INSTANCE_CONNECTION_NAME:\n",
    "    INSTANCE_CONNECTION_NAME = input(\"Enter INSTANCE_CONNECTION_NAME: \").strip()\n",
    "if not DB_USER:\n",
    "    DB_USER = input(\"Enter DB_USER: \").strip()\n",
    "if not DB_PASS:\n",
    "    DB_PASS = getpass(\"Enter DB_PASS: \")\n",
    "\n",
    "engine, connector = create_engine_with_connector(\n",
    "    instance_connection_name=INSTANCE_CONNECTION_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASS,\n",
    "    db_name=DB_NAME,\n",
    ")\n",
    "\n",
    "SCHEMA_SUMMARY = build_schema_summary(engine, db_name=DB_NAME, max_cols_per_table=50)\n",
    "print(\"Schema summary length:\", len(SCHEMA_SUMMARY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ec411",
   "metadata": {},
   "source": [
    "## 5) Experiment Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef5ef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# ============================\n",
    "# Experiment Registry (edit here)\n",
    "# ============================\n",
    "# each preset keeps model and training knobs bundled together.\n",
    "# switch only ACTIVE_EXPERIMENT for a clean model swap.\n",
    "# provenance: these defaults follow qlora/lora practice + hf peft examples,\n",
    "# then are held constant across llama/qwen for fair comparison.\n",
    "# batch/accum/seq are constrained by colab gpu memory limits.\n",
    "EXPERIMENT_PRESETS = {\n",
    "    \"llama3_8b\": {\n",
    "        \"label\": \"Llama-3-8B QLoRA\",\n",
    "        \"model_id\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        \"adapter_output_dir\": \"results/adapters/qlora_llama3_8b_classicmodels\",\n",
    "        \"lora_r\": 32,\n",
    "        \"lora_alpha\": 64,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "        \"train_batch_size\": 1,\n",
    "        \"grad_accum_steps\": 8,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"warmup_ratio\": 0.05,\n",
    "        \"max_seq_length\": 1024,\n",
    "        \"save_steps\": 200,\n",
    "        \"save_total_limit\": 2,\n",
    "    },\n",
    "    \"qwen2_5_7b\": {\n",
    "        \"label\": \"Qwen2.5-7B QLoRA\",\n",
    "        \"model_id\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        \"adapter_output_dir\": \"results/adapters/qlora_qwen2_5_7b_classicmodels\",\n",
    "        \"lora_r\": 32,\n",
    "        \"lora_alpha\": 64,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "        \"train_batch_size\": 1,\n",
    "        \"grad_accum_steps\": 8,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"warmup_ratio\": 0.05,\n",
    "        \"max_seq_length\": 1024,\n",
    "        \"save_steps\": 200,\n",
    "        \"save_total_limit\": 2,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Change this one line per training run:\n",
    "ACTIVE_EXPERIMENT = \"qwen2_5_7b\"  # options: llama3_8b, qwen2_5_7b\n",
    "\n",
    "EXPERIMENT_CONFIG = EXPERIMENT_PRESETS[ACTIVE_EXPERIMENT]\n",
    "MODEL_ID = EXPERIMENT_CONFIG[\"model_id\"]\n",
    "output_dir = EXPERIMENT_CONFIG[\"adapter_output_dir\"]\n",
    "\n",
    "# keep these knobs consistent across models unless you are intentionally running an ablation.\n",
    "print(\"Experiment:\", EXPERIMENT_CONFIG[\"label\"])\n",
    "print(\"MODEL_ID:\", MODEL_ID)\n",
    "print(\"Adapter output:\", output_dir)\n",
    "print(\n",
    "    \"Train knobs:\",\n",
    "    {\n",
    "        \"batch\": EXPERIMENT_CONFIG[\"train_batch_size\"],\n",
    "        \"grad_accum\": EXPERIMENT_CONFIG[\"grad_accum_steps\"],\n",
    "        \"lr\": EXPERIMENT_CONFIG[\"learning_rate\"],\n",
    "        \"epochs\": EXPERIMENT_CONFIG[\"num_train_epochs\"],\n",
    "        \"max_seq\": EXPERIMENT_CONFIG[\"max_seq_length\"],\n",
    "        \"lora_r\": EXPERIMENT_CONFIG[\"lora_r\"],\n",
    "        \"lora_alpha\": EXPERIMENT_CONFIG[\"lora_alpha\"],\n",
    "        \"lora_dropout\": EXPERIMENT_CONFIG[\"lora_dropout\"],\n",
    "        \"target_modules\": EXPERIMENT_CONFIG[\"target_modules\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\n",
    "        \"CUDA is not available. In Colab, switch to GPU runtime: Runtime -> Change runtime type -> GPU.\"\n",
    "    )\n",
    "\n",
    "cc_major, cc_minor = torch.cuda.get_device_capability(0)\n",
    "use_bf16 = cc_major >= 8\n",
    "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"Compute capability:\", (cc_major, cc_minor))\n",
    "print(\"Using bf16:\", use_bf16, \"| compute_dtype:\", compute_dtype)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, token=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "# 4-bit nf4 + double quantization comes from qlora to fit 7b/8b models in vram.\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map=device_map,\n",
    "    token=True,\n",
    ")\n",
    "\n",
    "# Deterministic defaults for later evaluation\n",
    "base_model.generation_config.do_sample = False\n",
    "base_model.generation_config.temperature = 1.0\n",
    "base_model.generation_config.top_p = 1.0\n",
    "base_model.generation_config.top_k = 50\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "# lora r/alpha/dropout + q_proj/v_proj are standard peft choices for causal lm tuning.\n",
    "    r=EXPERIMENT_CONFIG[\"lora_r\"],\n",
    "    lora_alpha=EXPERIMENT_CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=EXPERIMENT_CONFIG[\"lora_dropout\"],\n",
    "    target_modules=EXPERIMENT_CONFIG[\"target_modules\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd9b26a",
   "metadata": {},
   "source": [
    "## 6) Build SFT Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646991ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from nl2sql.prompting import SYSTEM_INSTRUCTIONS\n",
    "\n",
    "def format_example(nlq: str, sql: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_INSTRUCTIONS},\n",
    "        {\"role\": \"user\", \"content\": \"Schema:\\n\" + SCHEMA_SUMMARY},\n",
    "        {\"role\": \"user\", \"content\": f\"NLQ: {nlq}\"},\n",
    "        {\"role\": \"assistant\", \"content\": sql.rstrip(\";\") + \";\"},\n",
    "    ]\n",
    "    return tok.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "train_texts = [format_example(r[\"nlq\"], r[\"sql\"]) for r in train_records]\n",
    "train_ds = Dataset.from_dict({\"text\": train_texts})\n",
    "print(train_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca45957",
   "metadata": {},
   "source": [
    "## 7) Train Adapters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4004eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "run_card = {\n",
    "    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"experiment_key\": ACTIVE_EXPERIMENT,\n",
    "    \"experiment_label\": EXPERIMENT_CONFIG[\"label\"],\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"output_dir\": output_dir,\n",
    "    \"train_batch_size\": EXPERIMENT_CONFIG[\"train_batch_size\"],\n",
    "    \"grad_accum_steps\": EXPERIMENT_CONFIG[\"grad_accum_steps\"],\n",
    "    \"learning_rate\": EXPERIMENT_CONFIG[\"learning_rate\"],\n",
    "    \"num_train_epochs\": EXPERIMENT_CONFIG[\"num_train_epochs\"],\n",
    "    \"warmup_ratio\": EXPERIMENT_CONFIG[\"warmup_ratio\"],\n",
    "    \"max_seq_length\": EXPERIMENT_CONFIG[\"max_seq_length\"],\n",
    "    \"lora_r\": EXPERIMENT_CONFIG[\"lora_r\"],\n",
    "    \"lora_alpha\": EXPERIMENT_CONFIG[\"lora_alpha\"],\n",
    "    \"lora_dropout\": EXPERIMENT_CONFIG[\"lora_dropout\"],\n",
    "    \"target_modules\": EXPERIMENT_CONFIG[\"target_modules\"],\n",
    "    \"precision\": \"bf16\" if use_bf16 else \"fp16\",\n",
    "}\n",
    "print(\"Training run card\\n\", json.dumps(run_card, indent=2))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=EXPERIMENT_CONFIG[\"train_batch_size\"],\n",
    "    gradient_accumulation_steps=EXPERIMENT_CONFIG[\"grad_accum_steps\"],\n",
    "    learning_rate=EXPERIMENT_CONFIG[\"learning_rate\"],\n",
    "    num_train_epochs=EXPERIMENT_CONFIG[\"num_train_epochs\"],\n",
    "    warmup_ratio=EXPERIMENT_CONFIG[\"warmup_ratio\"],\n",
    "    logging_steps=10,\n",
    "    save_steps=EXPERIMENT_CONFIG[\"save_steps\"],\n",
    "    save_total_limit=EXPERIMENT_CONFIG[\"save_total_limit\"],\n",
    "    bf16=use_bf16,\n",
    "    fp16=(not use_bf16),\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tok,\n",
    "    train_dataset=train_ds,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=training_args,\n",
    "    max_seq_length=EXPERIMENT_CONFIG[\"max_seq_length\"],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tok.save_pretrained(output_dir)\n",
    "\n",
    "# Save run-card next to adapters for audit trail.\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "(Path(output_dir) / \"run_card.json\").write_text(json.dumps(run_card, indent=2), encoding=\"utf-8\")\n",
    "print(\"Saved adapters to:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79c75a8",
   "metadata": {},
   "source": [
    "## 8) Eval Helpers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why This QLoRA Grid Design\n",
    "This section keeps QLoRA evaluation policy explicit so results are defendable.\n",
    "\n",
    "Design decisions to call out while presenting:\n",
    "1. Same evaluation harness (`eval_run`) as baseline is reused to isolate tuning effects from scoring-code changes.\n",
    "2. Function parameters include model and adapter context (`model_id`, `model_alias`, `adapter_dir`, `eval_model`) for reproducible provenance.\n",
    "3. Guardrails check required globals before launching long runs, so missing setup fails fast.\n",
    "4. Prompt/schema/exemplar knobs mirror baseline by design; method comparisons stay aligned.\n",
    "5. Connector caching (`lru_cache`) is kept for TS replicas to reduce repeated connection overhead.\n",
    "6. Prompt override + connector cleanup uses `try/finally` to prevent notebook-state drift after failures.\n",
    "7. Canonical/model-family file copies use primary-seed policy to keep downstream comparison paths stable.\n",
    "8. Per-run and grouped CSV outputs are both written: one for auditability, one for summary analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1212fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orchestration helpers for QLoRA evaluation sweeps.\n",
    "# Core scoring/execution logic is imported from nl2sql.\n",
    "# Why these imports are here:\n",
    "# - re: alias sanitization + exemplar strategy regex checks\n",
    "# - subprocess: git commit provenance for run artifacts\n",
    "# - shutil/pathlib: deterministic artifact copy paths\n",
    "# - lru_cache: TS engine reuse for speed and stability\n",
    "# - pandas: per-run and grouped reporting tables\n",
    "import re\n",
    "import subprocess\n",
    "import shutil\n",
    "from datetime import datetime, timezone\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from nl2sql.eval import eval_run\n",
    "from nl2sql.db import create_engine_with_connector\n",
    "import nl2sql.prompting as prompting_mod\n",
    "\n",
    "DEFAULT_SYSTEM_INSTRUCTIONS = prompting_mod.SYSTEM_INSTRUCTIONS\n",
    "\n",
    "# Prompt variants used for controlled prompt-ablation runs.\n",
    "PROMPT_VARIANTS = {\n",
    "    \"default\": DEFAULT_SYSTEM_INSTRUCTIONS,\n",
    "    \"schema_only_minimal\": \"\"\"You are an expert data analyst writing MySQL queries.\n",
    "Given the database schema and a natural language question, write a single SQL SELECT query.\n",
    "\n",
    "Rules:\n",
    "- Output ONLY SQL (no explanation, no markdown).\n",
    "- Output exactly ONE statement, starting with SELECT.\n",
    "- Use only tables/columns listed in the schema.\n",
    "\"\"\",\n",
    "    \"no_routing_hints\": DEFAULT_SYSTEM_INSTRUCTIONS.split(\"- Routing hints:\")[0].rstrip(),\n",
    "}\n",
    "\n",
    "\n",
    "# Convert HF model IDs into filesystem-safe aliases.\n",
    "def _model_alias_from_id(model_id: str) -> str:\n",
    "    tail = (model_id or \"model\").split(\"/\")[-1]\n",
    "    alias = re.sub(r\"[^a-z0-9]+\", \"_\", tail.lower()).strip(\"_\")\n",
    "    return alias or \"model\"\n",
    "\n",
    "\n",
    "# Schema truncation variants for schema-ablation experiments.\n",
    "def schema_variant_text(schema_text: str, variant: str) -> str:\n",
    "    lines = schema_text.splitlines()\n",
    "    if variant == \"full\":\n",
    "        return schema_text\n",
    "    if variant == \"first_80_lines\":\n",
    "        return \"\\n\".join(lines[:80])\n",
    "    if variant == \"first_40_lines\":\n",
    "        return \"\\n\".join(lines[:40])\n",
    "    raise ValueError(f\"Unknown SCHEMA_VARIANT: {variant}\")\n",
    "\n",
    "\n",
    "# Exemplar-pool strategies for few-shot ablations.\n",
    "def exemplar_pool_for_strategy(items: list[dict], strategy: str) -> list[dict]:\n",
    "    if strategy == \"all\":\n",
    "        return list(items)\n",
    "\n",
    "    def _sql(x):\n",
    "        return str(x.get(\"sql\", \"\")).strip()\n",
    "\n",
    "    def _is_join(sql: str) -> bool:\n",
    "        s = sql.lower()\n",
    "        return \" join \" in f\" {s} \"\n",
    "\n",
    "    def _is_agg(sql: str) -> bool:\n",
    "        return bool(re.search(r\"\\b(sum|avg|count|min|max)\\s*\\(\", sql.lower()))\n",
    "\n",
    "    if strategy == \"brief_sql\":\n",
    "        ranked = sorted(items, key=lambda x: len(_sql(x)))\n",
    "        keep = max(50, int(0.4 * len(ranked)))\n",
    "        pool = ranked[:keep]\n",
    "    elif strategy == \"join_heavy\":\n",
    "        pool = [x for x in items if _is_join(_sql(x))]\n",
    "    elif strategy == \"agg_heavy\":\n",
    "        pool = [x for x in items if _is_agg(_sql(x))]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown EXEMPLAR_STRATEGY: {strategy}\")\n",
    "\n",
    "    return pool if len(pool) >= 10 else list(items)\n",
    "\n",
    "\n",
    "# Ensure output root exists before writing run artifacts.\n",
    "Path(\"results/qlora\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Main sweep runner: executes k/seed grid and writes JSON+CSV artifacts.\n",
    "def run_qlora_grid(\n",
    "    *,\n",
    "    model_id: str,\n",
    "    model_alias: str,\n",
    "    adapter_dir: str,\n",
    "    eval_model,\n",
    "    k_values: list[int],\n",
    "    seeds: list[int],\n",
    "    run_tag: str,\n",
    "    prompt_variant: str = \"default\",\n",
    "    schema_variant: str = \"full\",\n",
    "    exemplar_strategy: str = \"all\",\n",
    "    limit: int | None = None,\n",
    "    copy_canonical: bool = False,\n",
    "    copy_model_family: bool = True,\n",
    "    enable_ts_for_k: set[int] | None = None,\n",
    "    ts_n: int = 10,\n",
    "    ts_prefix: str = \"classicmodels_ts\",\n",
    "    ts_max_rows: int = 500,\n",
    "    max_new_tokens: int = 128,\n",
    "):\n",
    "    \"\"\"\n",
    "    Design rationale (QLoRA grid):\n",
    "    1) Reuse the same `eval_run` scoring path as baseline.\n",
    "       Alternative considered: custom QLoRA evaluator.\n",
    "       Chosen approach: identical scoring code keeps method comparison fair.\n",
    "\n",
    "    2) Keep model + adapter identity in metadata (`model_id`, `model_alias`, `adapter_dir`).\n",
    "       This allows exact provenance for each JSON artifact.\n",
    "\n",
    "    3) Validate required globals early (dataset, schema, engine, tokenizer, DB creds).\n",
    "       This avoids wasting long GPU runs due to missing setup state.\n",
    "\n",
    "    4) Keep prompt/schema/exemplar ablation knobs aligned with baseline.\n",
    "       This isolates the effect of tuning from prompt-policy drift.\n",
    "\n",
    "    5) Use deterministic state cleanup (`try/finally`) for prompt overrides and connectors.\n",
    "       Notebook reruns remain safe after interrupts or runtime errors.\n",
    "\n",
    "    6) Write both per-run and aggregated outputs for two audiences:\n",
    "       debugging/audit trails and final reporting tables.\n",
    "    \"\"\"\n",
    "    # Basic guardrails for reproducible runs.\n",
    "    if not seeds:\n",
    "        raise ValueError(\"Provide at least one seed\")\n",
    "    if prompt_variant not in PROMPT_VARIANTS:\n",
    "        raise ValueError(f\"Unknown PROMPT_VARIANT: {prompt_variant}\")\n",
    "\n",
    "    # Depends on setup cells that define data, model, tokenizer, and DB config.\n",
    "    required = [\"test_set\", \"SCHEMA_SUMMARY\", \"engine\", \"tok\", \"INSTANCE_CONNECTION_NAME\", \"DB_USER\", \"DB_PASS\"]\n",
    "    missing = [k for k in required if k not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Missing globals from previous cells: {missing}\")\n",
    "\n",
    "    try:\n",
    "        commit = subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"]).decode().strip()\n",
    "    except Exception:\n",
    "        commit = \"unknown\"\n",
    "\n",
    "    run_metadata_base = {\n",
    "        \"commit\": commit,\n",
    "        \"model_id\": model_id,\n",
    "        \"model_alias\": model_alias,\n",
    "        \"method\": \"qlora\",\n",
    "        \"adapter_dir\": str(adapter_dir),\n",
    "    }\n",
    "\n",
    "    # Run directory is timestamped for traceability and collision safety.\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%SZ\")\n",
    "    run_dir = Path(\"results/qlora/runs\") / f\"{run_tag}_{ts}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    schema_used = schema_variant_text(SCHEMA_SUMMARY, schema_variant)\n",
    "    exemplar_pool = exemplar_pool_for_strategy(test_set, exemplar_strategy)\n",
    "\n",
    "    ts_enabled_k = set(enable_ts_for_k or set())\n",
    "    ts_suite_db_names = (\n",
    "        [f\"{ts_prefix}_{i:02d}\" for i in range(1, ts_n + 1)]\n",
    "        if ts_enabled_k and ts_n > 0\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Lazy connector cache for optional TS evaluation databases.\n",
    "    ts_connectors = {}\n",
    "\n",
    "    @lru_cache(maxsize=32)\n",
    "    def _make_engine_cached(db_name: str):\n",
    "        eng, conn = create_engine_with_connector(\n",
    "            instance_connection_name=INSTANCE_CONNECTION_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASS,\n",
    "            db_name=db_name,\n",
    "        )\n",
    "        ts_connectors[db_name] = conn\n",
    "        return eng\n",
    "\n",
    "    rows = []\n",
    "    primary_seed = seeds[0]  # first seed is used for canonical/model-family copies\n",
    "\n",
    "    # Prompt override is temporary and restored in finally.\n",
    "    old_prompt = prompting_mod.SYSTEM_INSTRUCTIONS\n",
    "    prompting_mod.SYSTEM_INSTRUCTIONS = PROMPT_VARIANTS[prompt_variant]\n",
    "\n",
    "    try:\n",
    "        for k in k_values:\n",
    "            # Use all seeds for every k for clean repeated-run stats.\n",
    "            for seed in seeds:\n",
    "                save_path = run_dir / f\"results_k{k}_seed{seed}.json\"\n",
    "\n",
    "                run_meta = dict(run_metadata_base)\n",
    "                run_meta.update({\n",
    "                    \"run_tag\": run_tag,\n",
    "                    \"k\": k,\n",
    "                    \"seed\": seed,\n",
    "                    \"prompt_variant\": prompt_variant,\n",
    "                    \"schema_variant\": schema_variant,\n",
    "                    \"exemplar_strategy\": exemplar_strategy,\n",
    "                    \"exemplar_pool_size\": len(exemplar_pool),\n",
    "                    \"ts_enabled\": bool(k in ts_enabled_k),\n",
    "                    \"ts_for_k_values\": sorted(ts_enabled_k),\n",
    "                    \"ts_n\": ts_n if ts_suite_db_names else 0,\n",
    "                })\n",
    "\n",
    "                items = eval_run(\n",
    "                    test_set=test_set,\n",
    "                    exemplar_pool=exemplar_pool,\n",
    "                    k=k,\n",
    "                    limit=limit,\n",
    "                    seed=seed,\n",
    "                    engine=engine,\n",
    "                    model=eval_model,\n",
    "                    tokenizer=tok,\n",
    "                    schema_summary=schema_used,\n",
    "                    save_path=str(save_path),\n",
    "                    run_metadata=run_meta,\n",
    "                    ts_suite_db_names=ts_suite_db_names if k in ts_enabled_k else None,\n",
    "                    ts_make_engine_fn=_make_engine_cached if k in ts_enabled_k else None,\n",
    "                    ts_max_rows=ts_max_rows,\n",
    "                    avoid_exemplar_leakage=True,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                )\n",
    "\n",
    "                # Aggregate per-run rates for quick table summaries.\n",
    "                n = len(items)\n",
    "                va = sum(int(x.va) for x in items) / max(n, 1)\n",
    "                em = sum(int(x.em) for x in items) / max(n, 1)\n",
    "                ex = sum(int(x.ex) for x in items) / max(n, 1)\n",
    "                ts_values = [int(x.ts) for x in items if getattr(x, \"ts\", None) is not None]\n",
    "                ts_rate = (sum(ts_values) / len(ts_values)) if ts_values else None\n",
    "\n",
    "                rows.append({\n",
    "                    \"run_tag\": run_tag,\n",
    "                    \"prompt_variant\": prompt_variant,\n",
    "                    \"schema_variant\": schema_variant,\n",
    "                    \"exemplar_strategy\": exemplar_strategy,\n",
    "                    \"exemplar_pool_size\": len(exemplar_pool),\n",
    "                    \"k\": k,\n",
    "                    \"seed\": seed,\n",
    "                    \"n\": n,\n",
    "                    \"va_rate\": va,\n",
    "                    \"em_rate\": em,\n",
    "                    \"ex_rate\": ex,\n",
    "                    \"ts_rate\": ts_rate,\n",
    "                    \"ts_n\": len(ts_values),\n",
    "                    \"json_path\": str(save_path),\n",
    "                })\n",
    "\n",
    "                if seed == primary_seed and copy_canonical and k in {0, 3}:\n",
    "                    target = (\n",
    "                        Path(\"results/qlora/results_zero_shot_200.json\")\n",
    "                        if k == 0 else Path(\"results/qlora/results_few_shot_k3_200.json\")\n",
    "                    )\n",
    "                    target.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    shutil.copy2(save_path, target)\n",
    "                    print(f\"Updated canonical file: {target}\")\n",
    "\n",
    "                if seed == primary_seed and copy_model_family:\n",
    "                    model_target = Path(\"results/qlora/model_family\") / f\"{model_alias}_qlora_k{k}.json\"\n",
    "                    model_target.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    shutil.copy2(save_path, model_target)\n",
    "                    print(f\"Updated model-family file: {model_target}\")\n",
    "    finally:\n",
    "        for conn in ts_connectors.values():\n",
    "            try:\n",
    "                conn.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "        prompting_mod.SYSTEM_INSTRUCTIONS = old_prompt\n",
    "\n",
    "    # Save per-run table and per-k mean/std summary.\n",
    "    df = pd.DataFrame(rows).sort_values([\"k\", \"seed\"]).reset_index(drop=True)\n",
    "    df.to_csv(run_dir / \"grid_summary.csv\", index=False)\n",
    "\n",
    "    agg = (\n",
    "        df.groupby([\"prompt_variant\", \"schema_variant\", \"exemplar_strategy\", \"k\"], as_index=False)\n",
    "        .agg(\n",
    "            runs=(\"seed\", \"count\"),\n",
    "            va_mean=(\"va_rate\", \"mean\"),\n",
    "            va_std=(\"va_rate\", \"std\"),\n",
    "            em_mean=(\"em_rate\", \"mean\"),\n",
    "            em_std=(\"em_rate\", \"std\"),\n",
    "            ex_mean=(\"ex_rate\", \"mean\"),\n",
    "            ex_std=(\"ex_rate\", \"std\"),\n",
    "            ts_mean=(\"ts_rate\", \"mean\"),\n",
    "            ts_std=(\"ts_rate\", \"std\"),\n",
    "        )\n",
    "    )\n",
    "    agg.to_csv(run_dir / \"grid_summary_by_k.csv\", index=False)\n",
    "\n",
    "    print(\"Saved grid run to:\", run_dir)\n",
    "    return df, agg, run_dir\n",
    "\n",
    "\n",
    "# Optional Colab helper: copy completed run artifacts to Drive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436059ab",
   "metadata": {},
   "source": [
    "## 9) Load Adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34001d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Pin eval target explicitly\n",
    "# this cell is eval-only; it does not retrain adapters.\n",
    "# we point to a local adapter path so loading is deterministic in colab.\n",
    "# provenance: explicit model+adapter pinning avoids accidental cross-model adapter loads.\n",
    "EVAL_MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "EVAL_ADAPTER_DIR = \"/content/NLtoSQL/results/adapters/qlora_classicmodels\"\n",
    "\n",
    "\n",
    "def resolve_adapter_dir(path_str: str) -> Path:\n",
    "    p = Path(path_str).expanduser()\n",
    "    if not p.is_absolute():\n",
    "        p = (Path.cwd() / p).resolve()\n",
    "\n",
    "    if (p / \"adapter_config.json\").exists():\n",
    "        return p\n",
    "\n",
    "    ckpts = sorted(\n",
    "        [x for x in p.glob(\"checkpoint-*\") if (x / \"adapter_config.json\").exists()],\n",
    "        key=lambda x: int(re.findall(r\"\\d+\", x.name)[-1]),\n",
    "    )\n",
    "    if ckpts:\n",
    "        return ckpts[-1]\n",
    "\n",
    "    raise FileNotFoundError(f\"No adapter_config.json found under: {p}\")\n",
    "\n",
    "\n",
    "MODEL_ID = EVAL_MODEL_ID\n",
    "MODEL_ALIAS = _model_alias_from_id(MODEL_ID)\n",
    "adapter_path = resolve_adapter_dir(EVAL_ADAPTER_DIR)\n",
    "ADAPTER_DIR = str(adapter_path)\n",
    "\n",
    "print(\"EVAL MODEL_ID:\", MODEL_ID)\n",
    "print(\"Resolved ADAPTER_DIR:\", ADAPTER_DIR)\n",
    "\n",
    "cfg = json.loads((adapter_path / \"adapter_config.json\").read_text())\n",
    "print(\"adapter base_model_name_or_path:\", cfg.get(\"base_model_name_or_path\"))\n",
    "\n",
    "for name in [\"eval_model\", \"eval_base\", \"model\", \"base_model\"]:\n",
    "    if name in globals():\n",
    "        del globals()[name]\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "EVAL_GPU_MEM_GIB = 10\n",
    "EVAL_CPU_MEM_GIB = 64\n",
    "\n",
    "Path(\"/content/offload\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "eval_base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=compute_dtype if \"compute_dtype\" in globals() else torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    max_memory={0: f\"{EVAL_GPU_MEM_GIB}GiB\", \"cpu\": f\"{EVAL_CPU_MEM_GIB}GiB\"},\n",
    "    offload_folder=\"/content/offload\",\n",
    "    token=True,\n",
    ")\n",
    "\n",
    "eval_model = PeftModel.from_pretrained(\n",
    "    eval_base,\n",
    "    ADAPTER_DIR,              # local resolved path\n",
    "    is_trainable=False,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True,    # force local, no HF lookup\n",
    ")\n",
    "\n",
    "print(\"Loaded base + adapter from disk\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    free_b, total_b = torch.cuda.mem_get_info()\n",
    "    print(\"CUDA free/total GiB:\", round(free_b / (1024**3), 2), \"/\", round(total_b / (1024**3), 2))\n",
    "\n",
    "eval_model.eval()\n",
    "if hasattr(eval_model, \"gradient_checkpointing_disable\"):\n",
    "    eval_model.gradient_checkpointing_disable()\n",
    "if hasattr(eval_model.config, \"use_cache\"):\n",
    "    eval_model.config.use_cache = True\n",
    "if hasattr(eval_model, \"generation_config\"):\n",
    "    eval_model.generation_config.do_sample = False\n",
    "    eval_model.generation_config.temperature = 1.0\n",
    "    eval_model.generation_config.top_p = 1.0\n",
    "    eval_model.generation_config.top_k = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb2287",
   "metadata": {},
   "source": [
    "## 10) Eval Plan\n",
    "Use the next cell to configure the evaluation sweep and preview the data flow:\n",
    "1. Resolve `RUN_PLAN` into concrete `k`/seed settings.\n",
    "2. Verify adapter, prompt, schema, and exemplar-pool choices.\n",
    "3. Confirm TS settings before launching the grid run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef4b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Evaluation plan selector\n",
    "# ============================\n",
    "# Edit this block for experiment selection.\n",
    "RUN_PLAN = \"quick\"  # quick | k5_core | seed_backfill | ts_k3 | custom\n",
    "# run_plan resolves to one concrete k/seed grid.\n",
    "# use custom for one exact run when recovering from disconnects.\n",
    "RUN_TAG_BASE = f\"{MODEL_ALIAS}_qlora\"\n",
    "\n",
    "PROMPT_VARIANT = \"default\"\n",
    "SCHEMA_VARIANT = \"full\"\n",
    "EXEMPLAR_STRATEGY = \"all\"\n",
    "# keep these ablation toggles fixed when comparing models.\n",
    "# provenance: k/seeds come from the project eval protocol (repeatability over one-off scores).\n",
    "# use quick only for smoke checks; use custom for targeted backfills after disconnects.\n",
    "\n",
    "COPY_MODEL_FAMILY = True\n",
    "COPY_CANONICAL = False\n",
    "\n",
    "TS_N = 10\n",
    "TS_PREFIX = \"classicmodels_ts\"\n",
    "TS_MAX_ROWS = 500\n",
    "MAX_NEW_TOKENS = 128\n",
    "\n",
    "CUSTOM_PLAN = {\n",
    "    \"k_values\": [0, 3],\n",
    "    \"seeds\": [7],\n",
    "    \"run_tag\": f\"{RUN_TAG_BASE}_custom\",\n",
    "    \"enable_ts\": False,\n",
    "}\n",
    "\n",
    "if RUN_PLAN == \"quick\":\n",
    "    K_VALUES = [0, 3]\n",
    "    SEEDS = [7]\n",
    "    RUN_TAG = f\"{RUN_TAG_BASE}_main\"\n",
    "    ENABLE_TS = False\n",
    "elif RUN_PLAN == \"k5_core\":\n",
    "    K_VALUES = [5]\n",
    "    SEEDS = [7]\n",
    "    RUN_TAG = f\"{RUN_TAG_BASE}_k5_core\"\n",
    "    ENABLE_TS = False\n",
    "elif RUN_PLAN == \"seed_backfill\":\n",
    "    K_VALUES = [0, 3, 5]\n",
    "    SEEDS = [17, 27]\n",
    "    RUN_TAG = f\"{RUN_TAG_BASE}_seed_backfill\"\n",
    "    ENABLE_TS = False\n",
    "elif RUN_PLAN == \"ts_k3\":\n",
    "    K_VALUES = [3]\n",
    "    SEEDS = [7]\n",
    "    RUN_TAG = f\"{RUN_TAG_BASE}_ts_k3\"\n",
    "    ENABLE_TS = True\n",
    "elif RUN_PLAN == \"custom\":\n",
    "    K_VALUES = list(CUSTOM_PLAN[\"k_values\"])\n",
    "    SEEDS = list(CUSTOM_PLAN[\"seeds\"])\n",
    "    RUN_TAG = str(CUSTOM_PLAN[\"run_tag\"])\n",
    "    ENABLE_TS = bool(CUSTOM_PLAN[\"enable_ts\"])\n",
    "else:\n",
    "    raise ValueError(f\"Unknown RUN_PLAN: {RUN_PLAN}\")\n",
    "\n",
    "TS_FOR_K_VALUES = [3]\n",
    "\n",
    "print(\"Evaluation run card:\")\n",
    "print({\n",
    "    \"run_plan\": RUN_PLAN,\n",
    "    \"run_tag\": RUN_TAG,\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"model_alias\": MODEL_ALIAS,\n",
    "    \"adapter_dir\": ADAPTER_DIR,\n",
    "    \"k_values\": K_VALUES,\n",
    "    \"seeds\": SEEDS,\n",
    "    \"prompt_variant\": PROMPT_VARIANT,\n",
    "    \"schema_variant\": SCHEMA_VARIANT,\n",
    "    \"exemplar_strategy\": EXEMPLAR_STRATEGY,\n",
    "    \"enable_ts\": ENABLE_TS,\n",
    "    \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "})\n",
    "\n",
    "preview_schema = schema_variant_text(SCHEMA_SUMMARY, SCHEMA_VARIANT)\n",
    "preview_pool = exemplar_pool_for_strategy(test_set, EXEMPLAR_STRATEGY)\n",
    "print(\"Narrative preview:\")\n",
    "print({\n",
    "    \"adapter_exists\": Path(ADAPTER_DIR).exists(),\n",
    "    \"schema_lines\": len(preview_schema.splitlines()),\n",
    "    \"exemplar_pool_size\": len(preview_pool),\n",
    "    \"grid_size\": len(K_VALUES) * len(SEEDS),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: End-to-end NLQ -> faulty SQL -> cleaned SQL (single cell)\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import html\n",
    "\n",
    "from nl2sql.core.prompting import make_few_shot_messages\n",
    "from nl2sql.agent.constraint_policy import build_constraints\n",
    "from nl2sql.core.llm import debug_extract_first_select\n",
    "from nl2sql.core.postprocess import debug_guarded_postprocess\n",
    "\n",
    "\n",
    "# Small helper to print section headers in the notebook output.\n",
    "def show_title(text):\n",
    "    display(HTML(f\"<h3 style='margin:12px 0 6px 0'>{html.escape(text)}</h3>\"))\n",
    "\n",
    "\n",
    "# Small helper to render SQL/text in a boxed monospace block.\n",
    "def show_pre(text, label=None):\n",
    "    label_html = f\"<div style='font-weight:600;margin-bottom:6px'>{html.escape(label)}</div>\" if label else \"\"\n",
    "    display(HTML(\n",
    "        \"<div style='border:1px solid #ddd;border-radius:8px;padding:10px 12px;margin:8px 0'>\"\n",
    "        f\"{label_html}\"\n",
    "        f\"<pre style='white-space:pre-wrap;margin:0;font-family:ui-monospace, SFMono-Regular, Menlo, Consolas, monospace'>{html.escape(str(text))}</pre>\"\n",
    "        \"</div>\"\n",
    "    ))\n",
    "\n",
    "\n",
    "# Convert postprocess steps into a small readable table.\n",
    "def steps_df(pp):\n",
    "    return pd.DataFrame([\n",
    "        {\n",
    "            \"changed\": \"yes\" if s[\"changed\"] else \"no\",\n",
    "            \"stage\": s[\"stage\"],\n",
    "            \"note\": s.get(\"note\", \"\"),\n",
    "        }\n",
    "        for s in pp[\"steps\"]\n",
    "    ])\n",
    "\n",
    "\n",
    "# Demo NLQs: one implicit question and one explicit field-list question.\n",
    "DEMO_NLQ_IMPLICIT = \"List all customer names in France\"\n",
    "DEMO_NLQ_EXPLICIT = \"List contact last name, customer name, and customer number for customers in France\"\n",
    "\n",
    "# Use real schema text if available; otherwise use a minimal fallback.\n",
    "schema_text = (\n",
    "    SCHEMA_SUMMARY\n",
    "    if \"SCHEMA_SUMMARY\" in globals() and isinstance(SCHEMA_SUMMARY, str) and SCHEMA_SUMMARY.strip()\n",
    "    else \"Table customers (customerNumber INT, customerName TEXT, contactLastName TEXT, country TEXT, creditLimit REAL)\"\n",
    ")\n",
    "\n",
    "# Pull a couple of real exemplars when the benchmark is loaded.\n",
    "exemplars = []\n",
    "if \"test_set\" in globals() and isinstance(test_set, list):\n",
    "    exemplars = [x for x in test_set[:2] if isinstance(x, dict) and \"nlq\" in x and \"sql\" in x]\n",
    "\n",
    "# Build the same style of messages used by the real pipeline.\n",
    "messages = make_few_shot_messages(schema=schema_text, exemplars=exemplars, nlq=DEMO_NLQ_IMPLICIT)\n",
    "constraints_implicit = build_constraints(DEMO_NLQ_IMPLICIT, schema_text)\n",
    "\n",
    "# Step 1: show the NLQ and the prompt context.\n",
    "show_title(\"Step 1 - NLQ and prompt context\")\n",
    "display(pd.DataFrame([\n",
    "    {\n",
    "        \"nlq\": DEMO_NLQ_IMPLICIT,\n",
    "        \"schema_lines\": len(schema_text.splitlines()),\n",
    "        \"exemplars_used\": len(exemplars),\n",
    "        \"message_count\": len(messages),\n",
    "        \"explicit_fields\": constraints_implicit.get(\"explicit_fields\"),\n",
    "    }\n",
    "]))\n",
    "\n",
    "display(pd.DataFrame([\n",
    "    {\n",
    "        \"role\": m.get(\"role\"),\n",
    "        \"content_preview\": str(m.get(\"content\", \"\")).replace(\"\\n\", \" \")[:140],\n",
    "    }\n",
    "    for m in messages[:6]\n",
    "]))\n",
    "\n",
    "# Step 2: simulate a noisy/faulty model output (on purpose).\n",
    "FAULTY_TEXT = \"\"\"Model draft + noise:\n",
    "select from the options above\n",
    "\n",
    "SQL:\n",
    "SELECT c.customerNumber, c.customerName, c.contactLastName, c.creditLimit\n",
    "FROM customers c\n",
    "WHERE c.country = 'France'\n",
    "ORDER BY c.customerName DESC\n",
    "LIMIT 5;\n",
    "\n",
    "Extra explanation after SQL.\n",
    "\"\"\"\n",
    "\n",
    "show_title(\"Step 2 - Simulated faulty SQL draft\")\n",
    "show_pre(FAULTY_TEXT, \"Faulty model output (simulated)\")\n",
    "\n",
    "# Step 3: run extraction logic to pick the best SQL candidate.\n",
    "show_title(\"Step 3 - Extraction debug\")\n",
    "extract_debug = debug_extract_first_select(FAULTY_TEXT)\n",
    "selected_sql = extract_debug.get(\"selected_sql\") or FAULTY_TEXT\n",
    "\n",
    "display(pd.DataFrame([\n",
    "    {\n",
    "        \"candidate\": i,\n",
    "        \"accepted\": c.get(\"accepted\"),\n",
    "        \"reject_reason\": c.get(\"reject_reason\"),\n",
    "        \"from_target\": c.get(\"from_target\"),\n",
    "        \"candidate_sql\": c.get(\"candidate_sql\"),\n",
    "    }\n",
    "    for i, c in enumerate(extract_debug.get(\"candidates\", []), start=1)\n",
    "]))\n",
    "show_pre(selected_sql, \"Selected SQL candidate\")\n",
    "\n",
    "# Step 4A: clean SQL for implicit-field question behavior.\n",
    "show_title(\"Step 4A - Cleaning trace (implicit fields)\")\n",
    "pp_a = debug_guarded_postprocess(\n",
    "    selected_sql,\n",
    "    DEMO_NLQ_IMPLICIT,\n",
    "    explicit_fields=constraints_implicit.get(\"explicit_fields\") if constraints_implicit.get(\"explicit_projection\") else None,\n",
    "    required_fields=constraints_implicit.get(\"required_output_fields\"),\n",
    ")\n",
    "display(steps_df(pp_a))\n",
    "show_pre(pp_a[\"final_sql\"], \"Final cleaned SQL (implicit)\")\n",
    "\n",
    "# Step 4B: clean SQL for explicit-field question behavior.\n",
    "show_title(\"Step 4B - Cleaning trace (explicit fields)\")\n",
    "pp_b = debug_guarded_postprocess(\n",
    "    selected_sql,\n",
    "    DEMO_NLQ_EXPLICIT,\n",
    "    explicit_fields=[\"contactLastName\", \"customerName\", \"customerNumber\"],\n",
    ")\n",
    "display(steps_df(pp_b))\n",
    "show_pre(pp_b[\"final_sql\"], \"Final cleaned SQL (explicit)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025bb647",
   "metadata": {},
   "source": [
    "## 11) Run Plan\n",
    "Run this after the preview above is correct. The execution cell writes per-run JSON files plus grid summary CSVs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab3463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute selected sweep.\n",
    "qlora_grid, qlora_by_k, qlora_run_dir = run_qlora_grid(\n",
    "    model_id=MODEL_ID,\n",
    "    model_alias=MODEL_ALIAS,\n",
    "    adapter_dir=ADAPTER_DIR,\n",
    "    eval_model=eval_model,\n",
    "    k_values=K_VALUES,\n",
    "    seeds=SEEDS,\n",
    "    run_tag=RUN_TAG,\n",
    "    prompt_variant=PROMPT_VARIANT,\n",
    "    schema_variant=SCHEMA_VARIANT,\n",
    "    exemplar_strategy=EXEMPLAR_STRATEGY,\n",
    "    limit=None,\n",
    "    copy_canonical=COPY_CANONICAL,\n",
    "    copy_model_family=COPY_MODEL_FAMILY,\n",
    "    enable_ts_for_k=set(TS_FOR_K_VALUES) if ENABLE_TS else None,\n",
    "    ts_n=TS_N,\n",
    "    ts_prefix=TS_PREFIX,\n",
    "    ts_max_rows=TS_MAX_ROWS,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    ")\n",
    "\n",
    "print(\"Saved run dir:\", qlora_run_dir)\n",
    "display(qlora_grid)\n",
    "display(qlora_by_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10de72ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "baseline_zero = Path(\"results/baseline/results_zero_shot_200.json\")\n",
    "baseline_few  = Path(\"results/baseline/results_few_shot_k3_200.json\")\n",
    "\n",
    "qlora_zero = Path(\"results/qlora/results_zero_shot_200.json\")\n",
    "qlora_few = Path(\"results/qlora/results_few_shot_k3_200.json\")\n",
    "\n",
    "# If canonical files are disabled, fall back to current run outputs.\n",
    "if \"qlora_run_dir\" in globals():\n",
    "    qdir = Path(qlora_run_dir)\n",
    "    q0_run = qdir / \"results_k0_seed7.json\"\n",
    "    q3_run = qdir / \"results_k3_seed7.json\"\n",
    "    if q0_run.exists():\n",
    "        qlora_zero = q0_run\n",
    "    if q3_run.exists():\n",
    "        qlora_few = q3_run\n",
    "\n",
    "if baseline_zero.exists() and baseline_few.exists() and qlora_zero.exists() and qlora_few.exists():\n",
    "    b0 = json.loads(baseline_zero.read_text(encoding=\"utf-8\"))\n",
    "    b3 = json.loads(baseline_few.read_text(encoding=\"utf-8\"))\n",
    "    q0 = json.loads(qlora_zero.read_text(encoding=\"utf-8\"))\n",
    "    q3 = json.loads(qlora_few.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    print(\"Baseline zero-shot:\", \"VA\", round(b0[\"va_rate\"], 3), \"EM\", round(b0.get(\"em_rate\", 0.0), 3), \"EX\", round(b0[\"ex_rate\"], 3))\n",
    "    print(\"QLoRA   zero-shot:\", \"VA\", round(q0[\"va_rate\"], 3), \"EM\", round(q0.get(\"em_rate\", 0.0), 3), \"EX\", round(q0[\"ex_rate\"], 3))\n",
    "    print(\"Baseline few-shot :\", \"VA\", round(b3[\"va_rate\"], 3), \"EM\", round(b3.get(\"em_rate\", 0.0), 3), \"EX\", round(b3[\"ex_rate\"], 3))\n",
    "    print(\"QLoRA   few-shot  :\", \"VA\", round(q3[\"va_rate\"], 3), \"EM\", round(q3.get(\"em_rate\", 0.0), 3), \"EX\", round(q3[\"ex_rate\"], 3))\n",
    "else:\n",
    "    print(\"Missing files for comparison. Run baseline notebook and this QLoRA quick cell first.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

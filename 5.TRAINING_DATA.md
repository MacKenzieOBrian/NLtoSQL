# Training Data (ClassicModels)

The training dataset provides supervised NL→SQL pairs for QLoRA fine-tuning while keeping a clean 200-item held-out test set for evaluation. This mirrors the methodology in Ojuri et al. (2025) and supports direct comparison between prompting, PEFT fine-tuning, and agentic refinement.

---

## Data Sources and Validation

Training examples were produced from:
- manually authored NL→SQL pairs
- templated generation for structural coverage
- execution-validated SQL statements

All SQL statements were executed against the ClassicModels database to verify syntactic and semantic correctness. Ground truth SQL for evaluation is stored separately to prevent leakage into training.

Repository locations:
- training: `data/train/classicmodels_train_200.jsonl`
- test: `data/classicmodels_test_200.json`

Execution-based validation avoids cases where SQL is syntactically valid but semantically wrong, a common issue in public NL→SQL datasets (Zhong et al., 2020).

---

## Coverage Targets

Coverage focuses on business-style SQL typical of reporting and analytics:

- multi-table joins
- filtering and predicates
- GROUP BY/HAVING
- aggregates (SUM, COUNT, MIN/MAX)
- ordering and limit queries
- date/temporal conditions

These patterns align with analytical workloads used in NL→SQL benchmarks and match the domain replicated in Ojuri et al. (2025).

---

## Rationale and Constraints

Dataset size was constrained to keep QLoRA fine-tuning feasible under Colab GPU VRAM limits (8–12GB). Parameter-efficient fine-tuning is known to perform well with limited supervised data due to adapter-based updates (Ding et al., 2023).

Separating the 200-item test set enables fair comparison across multiple strategies:

- zero-shot
- few-shot (k-shot ICL)
- QLoRA fine-tuning
- agentic refinement (planned)

This reproduces the experimental design from Ojuri et al. (2025).

---

# Reflections From Notebook Work

The following reflections are derived from notebook experimentation and inform dataset design choices.

---

## Reflection 1 — Zero-Shot Limitations

**Observation (from `02_baseline_prompting_eval.ipynb`):**  
Zero-shot prompting produced SQL that was often syntactically valid but failed execution for join-heavy queries.

**Interpretation:**  
Without context examples, the model struggled to infer table relationships and join conditions.

**Literature:**  
Consistent with findings that in-context learning struggles with structured, schema-heavy tasks (Mosbach et al., 2023).

**Impact on Dataset Design:**  
Increased emphasis on join templates during supervised data generation to internalize schema patterns.

---

## Reflection 2 — Few-Shot Sensitivity to Exemplars

**Observation:**  
Few-shot prompting improved execution accuracy over zero-shot, but results varied significantly based on which examples were included.

**Interpretation:**  
In-context learning is exemplar-sensitive and lacks stability without fine-tuning.

**Literature:**  
Brown et al. (2020) introduced ICL; follow-up studies note its variability for domain tasks.

**Impact:**  
Reinforced decision to pursue supervised fine-tuning rather than rely solely on prompt engineering.

---

## Reflection 3 — QLoRA Feasibility Under VRAM Constraints

**Observation (from `05_qlora_train_eval.ipynb`):**  
Quantized Llama-3-8B-Instruct in 4-bit mode peaked at ~9.3GB VRAM during training on Colab T4.

**Interpretation:**  
Training remained stable and did not produce out-of-memory errors.

**Literature:**  
Confirms PEFT viability claims for low-resource hardware (Ding et al., 2023).

**Impact:**  
Dataset size capped at 200 to keep training feasible without gradient checkpointing or offloading.

---

## Reflection 4 — Schema Grounding Matters

**Observation:**  
Models performed significantly better when schema text was included in prompts.

**Interpretation:**  
Schema grounding provides structural constraints needed for SQL generation.

**Literature:**  
Schema linking identified as a critical component in NL→SQL models (Li et al., 2023; RESDSQL, 2023).

**Impact:**  
Training examples include schema context to internalize table/column associations.

---

## Reflection 5 — Avoiding Train/Test Leakage

**Observation:**  
Overlapping NL queries between train and test led to artificially inflated execution accuracy during early experiments.

**Interpretation:**  
Leakage breaks comparative evaluation and masks generalization failures.

**Literature:**  
Leakage concerns highlighted in NL→SQL benchmarking (Yu et al., 2018).

**Impact:**  
Strict deduplication implemented between train/test splits.

---

## Reflection 6 — Data Reduction Trade-Off

**Observation:**  
Increasing dataset size beyond 200 produced noticeable VRAM pressure during QLoRA.

**Interpretation:**  
Batch size and context length interact with limited VRAM capacity.

**Literature:**  
GPU memory constraints frequently dictate PEFT batch sizing (Modal, 2025).

**Impact:**  
Prioritized high-coverage templates over bulk data volume.

---

# Summary

The resulting dataset balances:

- domain realism
- schema complexity
- training feasibility
- reproducibility
- evaluation integrity

Dataset design was informed by empirical notebook observations rather than theoretical assumptions alone, aligning methodological choices with practical constraints of open-source replication.


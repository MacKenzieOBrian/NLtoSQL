# Evaluation Framework

This project reproduces the evaluation methodology of Ojuri et al. (2025) to compare proprietary NL→SQL agents with open-source reproductions under execution-centric metrics. Evaluation emphasises semantic correctness, robustness, and compute feasibility rather than surface similarity, following recent shifts in NL→SQL benchmarking.

## Decision Journey (literature-backed)

- **Execution-centric metrics (VA/EX/TS):** chosen because SQL admits many semantically-equivalent surface forms; execution is the reliable semantic arbiter [Yu et al., 2018; Zhong et al., 2020].
- **Agent trace analysis:** added to interpret why SQL with VA=1 still fails semantically, aligned with agentic evaluation practice in ReAct/Reflexion-style systems.
- **Closed-schema setting:** ClassicModels domain replicates the proprietary setup of Ojuri et al. (2025) and aligns with controlled-schema benchmarks such as Spider-S/BIRD-mini.
- **Compute-aware validation:** evaluation additionally reports feasibility under Colab-class GPU/VRAM constraints, matching deployment assumptions for SMEs.

---

# 1. Metrics

The evaluation distinguishes three increasingly strict correctness criteria:

---

### 1.1 Valid SQL (VA)

**Definition**  
VA = 1 if the generated SQL executes without syntax or schema errors.

**Motivation**  
Single-shot decoding frequently yields malformed SQL; VA isolates syntactic competence from semantic competence.

**Literature Alignment**  
Execution validation is standard in Spider/BIRD pipelines [Yu et al., 2018], and surveys classify it as the baseline correctness signal [Zhu et al., 2024].

---

### 1.2 Execution Accuracy (EX)

**Definition**  
EX = 1 if the result-set of the generated query matches the gold result-set on the evaluation database.

**Motivation**  
Execution accuracy captures functional correctness, without requiring the program text to match the gold query.

**Why it matters**  
Surface-equivalent SQL is rare; semantically-equivalent SQL is common. EM mislabels many correct programs as wrong.

**Literature Alignment**  
EX is the primary metric in Spider and follow-up work and is used in Ojuri et al. (2025) for GPT-4 agent evaluation.

---

### 1.3 Test-Suite Accuracy (TS)

**Definition**  
TS = 1 if outputs match across all perturbed database states.

**Motivation**  
TS detects “accidental correctness” (e.g., grouping omitted but data degenerate) and measures semantic robustness.

**Literature Alignment**  
Proposed in Zhong et al. (2020); adopted in NL→SQL surveys and in Ojuri et al. (2025).

---

# 2. Secondary Metrics (for analysis only)

- **EM (Exact Match):** string-based comparison; included to track regressions and compare against prior work that still reports EM.
- **Trace diagnostics (agentic only):** captures candidate set, error classification, repair attempts, and fallback rate.

Interpretation: EM is not treated as a correctness metric but remains useful for debugging and comparing against legacy baselines.

---

# 3. Dataset & Benchmarking Substrate

Evaluation is conducted on a **200-item NL→SQL test set** over the **ClassicModels** schema.

- **Motivation:** replicates the proprietary GPT-4 agent study domain from Ojuri et al. (2025)
- **Schema scope:** multi-table joins, grouping, aggregation, ordering
- **Split discipline:** train and test disjoint; no schema leakage
- **Evaluation mode:** live execution against the MySQL database

This setup resembles closed-schema evaluations in Spider-mini and BIRD-S, where the goal is to isolate semantic correctness rather than cross-schema generalisation.

---

# 4. Evaluation Pipeline (Implementation)

Evaluation proceeds as:

1. **Generate SQL** via selected adaptation strategy  
   - zero-shot
   - few-shot
   - QLoRA fine-tuning
   - execution-guided agent (planned / partially implemented)

2. **Postprocess & safety guards**  
   - SELECT-only enforcement
   - projection & ORDER clamping (to suppress hallucination)

3. **Execute SQL** via `QueryRunner` on the ClassicModels database

4. **Compute metrics**  
   - VA (syntax/schema validity)
   - EX (functional correctness)
   - EM (surface match, secondary)
   - TS (planned)

5. **Aggregate across 200 samples**

6. **Optional (agentic):** capture trace-level diagnostics  
   - candidate diversity  
   - error classification  
   - repair success rate  
   - fallback utilisation  

Relevant code:  
- `nl2sql/eval.py` — metric implementation  
- `notebooks/02_baseline_prompting_eval.ipynb`  
- `notebooks/05_qlora_train_eval.ipynb`  
- `notebooks/03_agentic_eval.ipynb`  

---

# 5. Framing for Agentic Evaluation

Agentic strategies introduce an additional diagnostic dimension: “why EX fails given VA succeeds.” This aligns w

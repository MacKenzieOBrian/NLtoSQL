# Methodology (Open‑Source Replica)

This section describes the methodology for reproducing and extending the NL→SQL agent pipeline proposed by Ojuri et al. [10] using **open‑source models**, **parameter‑efficient adapters**, and **execution‑guided agentic refinement** under Colab‑class GPU constraints. The design is framed as a controlled comparison across three adaptation strategies: prompting only, QLoRA adapters, and execution‑guided agents.

---

## 1. Research Questions
1) How far can prompt‑only baselines go on ClassicModels?
2) Do QLoRA adapters improve semantic alignment under tight VRAM constraints?
3) Can execution‑guided control improve semantic correctness without weight updates?

---

## 2. Literature‑Backed Decision Journey
**(1) Closed‑schema evaluation on ClassicModels** — Fixed‑schema evaluation mirrors controlled settings in Spider/BIRD and Ojuri et al. [10], isolating **semantic correctness** within a known schema [1], [8], [9], [19].

**(2) Prompt‑only baselines (k ∈ {0,3})** — ICL is the standard baseline in NL→SQL [1], [6], [8], but is unstable and underperforms fine‑tuning on domain tasks [3].

**(3) QLoRA adapters** — PEFT enables domain adaptation under 8–12GB VRAM [4], [5], [12], consistent with Ojuri et al. [10].

**(4) Execution‑guided refinement** — Execution feedback improves validity and partial semantics (Zhong et al., 2017; ExCoT [2]; ReAct [16]).

**(5) Execution‑centric metrics** — VA/EX/TS are preferred over EM due to SQL surface variability [18], [19].

---

## 3. Pipeline Decomposition (Controlled Components)
To avoid confounds, the pipeline is decomposed into stable components:
1. NLQ ingestion
2. Schema‑grounded prompt construction
3. Adaptation strategy (prompt / QLoRA / agent)
4. SQL postprocess + safety guards
5. Execution + scoring (VA/EX/EM; TS planned)

All strategies share the same schema summary, prompt format, DB execution engine, and evaluation harness. Only the **adaptation mechanism** changes.

---

## 4. Controlled Prompt & Decoding Settings
| Variable | Setting |
|---|---|
| System instruction | fixed (MySQL, single‑SELECT contract) |
| Schema summary | fixed, ClassicModels‑specific |
| Exemplars (k) | {0, 3} |
| Decoding | deterministic for baselines |
| NLQ | unmodified natural language |
| Postprocess | guarded first‑SELECT extraction |

These controls ensure differences are attributable to **weights** (QLoRA) or **control logic** (agent), not prompt drift.

---

## 5. Hypotheses
**H1 — Prompt‑only recovers syntax but struggles with semantics.**
Expected: VA↑, EX↓ for joins/aggregates.

**H2 — QLoRA adapters improve semantic mappings.**
Expected: EX↑ via internalised schema semantics (joins, aggregation).

**H3 — Execution‑guided agency improves semantic correctness further.**
Expected: EX↑ via iterative correction from DB feedback, without weight updates.

---

## 6. Model Adaptation & Agent Design

### 6.1 QLoRA Configuration
Following [4], [5], [12]:
- Base: Llama‑3‑8B‑Instruct
- Quantisation: 4‑bit NF4 + 16‑bit compute
- PEFT: LoRA adapters on attention/projection layers
- VRAM target: ≤ 10 GB (Colab T4)

### 6.2 Execution‑Guided Agent (ReAct‑Inspired)
The implemented agent is a **minimal execution‑guided reranker**, inspired by ReAct [16] and ExCoT [2]:
1) candidate generation (standard + tabular prompts) [8], [9]
2) filtering + clamping (single‑SELECT, ORDER/LIMIT/projection control) [13]
3) execution gating (VA signal) [18]
4) error‑aware repair (DB error → correction prompt) [2], [18]
5) semantic reranking (lightweight intent score) [1], [8], [20]
6) deterministic fallback (baseline parity) [6], [8]

This layer tests whether **execution feedback alone** (no weight updates) can lift EX beyond QLoRA.

### 6.3 Agentic Execution‑Guided Text‑to‑SQL Pipeline
This study extends baseline prompting and fine‑tuning with an execution‑guided, agent‑style refinement loop inspired by LLM agents and execution feedback. Rather than treating SQL generation as a single‑shot decoding task, the approach models it as a bounded iterative process comprising candidate generation, validation, selection, and optional repair.

The objective is not unconstrained agent reasoning, but to systematically reduce known failure modes of LLM‑based SQL generation that persist even when outputs are syntactically valid.

#### 6.3.1 Motivation for an Agentic Approach
Large‑scale NL→SQL evaluations show that high syntactic validity does not reliably translate into semantic correctness. Models frequently generate SQL that executes but returns incorrect results, includes unnecessary clauses (ORDER BY/GROUP BY), selects superfluous columns, or misinterprets aggregation intent. These patterns motivate execution‑guided refinement rather than relying solely on prompt engineering or fine‑tuning.

Accordingly, this work adopts an execution‑guided agent design in which the database engine serves as a semantic validator.

#### 6.3.2 Staged Agent Design
To ensure interpretability and avoid over‑engineering, the agent is implemented as explicit stages, each introducing a minimal additional control mechanism. This enables controlled ablation and attribution of performance changes.

- **Stage 0 — Minimal execution‑gated decoding**  
  Generate a single SQL candidate, post‑process, execute, and return the first executable query.

- **Stage 1 — Lightweight structural clamps**  
  Suppress common over‑generation (e.g., unnecessary ORDER BY/GROUP BY) when not requested by the NLQ.

- **Stage 2 — Multi‑candidate generation and reranking**  
  Generate multiple candidates (standard + tabular prompts), execute, and score using intent alignment + minimal projection.

- **Stage 3 — Error‑aware repair using execution feedback**  
  When candidates fail, use DB error observations to prompt a bounded repair step.

#### 6.3.3 Candidate Generation and Output Control
Candidates are generated deterministically by default, with optional low‑temperature sampling in later stages for diversity. Decoding is constrained to a single SQL statement by stopping at the first semicolon.

Post‑generation cleaning removes prompt echo and instructional artefacts, enforces a FROM clause, rejects incomplete fragments, and normalises keyword spacing and casing. These steps are conservative and aim to prevent false negatives during execution rather than hard‑coding answers.

**Projection contract (EX stabilisation):**  
To address execution failures caused by extra or missing columns, a lightweight projection contract parses the NLQ for explicitly requested fields (e.g., *product name*, *product code*, *MSRP*) and deterministically drops extraneous SELECT items. This enforces **output shape**, not query logic, and targets a known EX failure mode (projection drift).

#### 6.3.4 Execution‑Guided Validation and Selection
Each cleaned candidate is executed against the database (read‑only). Execution success is necessary but not sufficient. Additional checks enforce:
- intent alignment for aggregation/grouping
- semantic relevance scoring (NLQ–SQL overlap)
- projection minimisation when multiple candidates are valid

The final SQL is selected by a composite score balancing semantic relevance and structural simplicity.

**Intent classifier (query‑type constraints):**  
A minimal intent classifier (lookup vs aggregate vs grouped‑aggregate vs top‑k) imposes structural constraints (e.g., disallow GROUP BY for list queries; require GROUP BY + aggregate for “per/by” queries). This prevents “wrong question type” outputs that still execute.

**Schema‑subset prompting (light schema linking):**  
Before generation, a small schema subset is derived using keyword‑to‑table matching and join hints. This reduces wrong table selection without injecting answer logic, aligning with schema‑linking recommendations.

#### 6.3.5 Error‑Aware Repair
If all candidates fail execution, the agent performs a single repair step. The failed SQL, DB error, schema description, and NLQ are combined into a corrective prompt. Repaired candidates are cleaned, executed, and scored identically to initial candidates. Repair is bounded to preserve determinism.

#### 6.3.6 Trace Logging and Interpretability
The pipeline records structured traces (raw outputs, cleaned candidates, execution outcomes, rejection reasons, repair attempts), enabling failure‑mode analysis and reproducibility.

#### 6.3.7 Evaluation Protocol
The agent is evaluated via VA (valid SQL), EX (execution accuracy), EM (string match), and TS (planned). The staged design allows performance differences to be attributed to specific agent mechanisms.

**Cell 6 justification (execution‑guided control layer):**  
Cell 6 implements a staged execution‑guided control layer, synthesised from constrained decoding, execution‑guided evaluation, and ReAct‑style agent design. STAGE 0–3 is an **ablation ladder**: each stage adds exactly one capability (clamps → rerank → repair), enabling causal attribution without retraining. Output control (stop‑on‑semicolon, prompt‑echo trimming, SELECT‑only filtering, dangling‑clause rejection) is **format enforcement**, not answer synthesis. Intent checks act as **lightweight semantic validation** that separate VA from EX. Repair is a single‑iteration ReAct loop, feeding DB errors back as observations while keeping the process bounded and deterministic.

**Output control + intent constraints (stability layer):**
To prevent “valid‑but‑irrelevant” SQL (VA=1, EX=0), the agent adds generation‑level termination (stop at `;`), prompt‑echo trimming, canonical table casing, and grouped‑aggregate intent checks. These controls align with constrained decoding and execution‑guided filtering [13], [18], and operationalise the VA vs EX distinction in agentic loops [16].

**Projection order + repair intent gate (EX protection):**
Two EX‑critical refinements were added: (i) the projection contract now preserves the NLQ’s **explicit field order** when a question enumerates fields (e.g., “names, codes, and MSRPs”), and (ii) repaired SQL is accepted only if it passes the same intent constraints as primary candidates. These changes prevent “almost‑right” outputs from failing EX due to column order mismatches and stop repair from overwriting correct intent with executable but irrelevant SQL. Both are output‑shape and validation controls, not semantic answer injection.

---

## 7. Data & Evaluation Substrate
- Domain: ClassicModels (MySQL)
- Benchmarks: 200‑item test set
- Training set: ~200 supervised examples
- Evaluation: live execution on ClassicModels (EX is the semantic oracle)

---

## 8. Methodological Reflection
The staged methodology — **prompt → QLoRA → execution‑guided agent** — follows progression in [3], [8], [10], [12], [16], [18]. It decomposes errors into syntax, schema grounding, and semantic reasoning, allowing clear attribution of gains and limitations.

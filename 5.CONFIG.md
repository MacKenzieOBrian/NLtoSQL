# Configuration & Reproducibility

This document describes how to reproduce experiments (baseline prompting now; agent + QLoRA planned). It is written to support dissertation-quality runs: fixed dependencies, deterministic decoding, and traceable run metadata.

## Project structure (why it changed)

The repo is intentionally split into:
- `nl2sql/` (importable “experiment harness”): stable, reviewable code for DB access, safe execution, prompting, and evaluation.
- `notebooks/` (Colab runners): orchestrate runs, save outputs, and generate dissertation tables/figures without duplicating core logic.
- `data/` (benchmarks) and `results/` (run outputs): keep inputs and outputs separate; `results/` is gitignored by default to avoid committing large outputs by accident.

This makes runs easier to reproduce: the notebook becomes a thin runner, while evaluation logic lives in version-controlled modules.

## Quickstart (Colab baseline)

1. Use a GPU runtime (T4/A100).
2. Fresh clone the repo into `/content` (recommended) and record the commit hash.
3. Install pinned deps from `requirements.txt`, then restart the runtime.
4. Authenticate:
   - GCP: `google.colab.auth.authenticate_user()` (or ADC locally)
   - Hugging Face: `notebook_login()` or `HF_TOKEN`
5. Run: `notebooks/02_baseline_prompting_eval.ipynb`
6. Outputs are saved under `results/baseline/` (gitignored by default; see `results/README.md`).
   - If you want these outputs on GitHub, remove/adjust the `results/` rule in `.gitignore` and commit curated JSONs.

## Quickstart (QLoRA)

1. Build a training set that does not overlap the 200-item benchmark:
   - The repo includes a starter training file at `data/train/classicmodels_train_200.jsonl`.
   - Run `notebooks/04_build_training_set.ipynb` to validate it strictly:
     - SELECT-only output
     - no overlap with `data/classicmodels_test_200.json` (exact NLQ match check)
     - SQL must execute on the live ClassicModels DB (VA=True)
   - If any rows fail, edit `data/train/classicmodels_train_200.jsonl` and re-run validation.
2. Fine-tune and evaluate adapters:
   - Run `notebooks/05_qlora_train_eval.ipynb` (saves adapters to `results/adapters/` and eval JSONs to `results/qlora/`).

Evaluation note:
- The QLoRA notebook evaluates the fine-tuned adapters in two inference modes:
  - `k=0` (no exemplars): measures what fine-tuning achieves on its own.
  - `k=3` (few-shot exemplars): measures whether prompt conditioning still adds uplift on top of fine-tuning.
This mirrors the dissertation question: prompt-only vs fine-tuning, and their interaction.

## Environment variables

Set these before running notebooks (or enter when prompted):

| Variable | Purpose | Example |
|----------|---------|---------|
| `INSTANCE_CONNECTION_NAME` | Cloud SQL instance identifier | `modified-enigma-476414-h9:europe-west2:classicmodels` |
| `DB_USER` | MySQL username | `root` |
| `DB_PASS` | MySQL password | — |
| `DB_NAME` | Database name | `classicmodels` |

## Dependencies

Pinned in `requirements.txt` to avoid Colab binary drift. The most failure-prone pins are `numpy`, `pandas`, `torch`, `triton`, and `bitsandbytes`.

Recommended Colab flow:
1. `pip install -r requirements.txt`
2. Restart runtime
3. Re-run notebook from the top

### What each dependency is for
- `torch`, `transformers`: core model loading, generation, and QLoRA training.
- `bitsandbytes`, `triton`, `accelerate`: 4-bit quantization + fast kernels on Colab GPUs.
- `peft`, `trl`, `datasets`: PEFT adapters (LoRA/QLoRA), SFT trainer, and dataset handling.
- `huggingface_hub`: gated model/token auth and repo download.
- `google-cloud-sql-connector`, `sqlalchemy`, `pymysql`: secure Cloud SQL connection and query execution for VA/EX.
- `pydantic`, `python-dotenv`: config/env ergonomics; avoid hardcoding secrets.

**Colab note on binaries:** Colab preinstalls many packages (NumPy/Python ABI, cpu-only bitsandbytes). Installing `requirements.txt` alone can leave mismatches (e.g., `numpy.dtype size changed`, missing `triton.ops`). The notebooks include guard + setup cells that (a) uninstall conflicting torch/numpy/bnb/triton, (b) reinstall a pinned CUDA 12.1 stack in a clean order, then (c) require a runtime restart before running. If you add new packages in Colab, pin versions compatible with NumPy 1.26.x to avoid ABI drift.

## Authentication

### Google Cloud (Cloud SQL)
- Colab: `from google.colab import auth; auth.authenticate_user()`
- Local: `gcloud auth application-default login` (ADC) or a service account key (avoid committing secrets).

### Hugging Face (gated model)
- Meta Llama 3 Instruct is gated: access approval + token are both required.
- Use `from huggingface_hub import notebook_login; notebook_login()` or set `HF_TOKEN`.

## Baseline prompting settings

- Model: `meta-llama/Meta-Llama-3-8B-Instruct` (token gated)
- Loading: 4-bit NF4 where possible (fits Colab GPUs, aligns with planned QLoRA)
- Decoding: deterministic for reporting (`do_sample=False`, bounded `max_new_tokens`, `pad_token_id=eos_token_id`)

## Evaluation metrics (Ojuri-style)

- **VA (Validity)**: predicted SQL executes successfully (syntactic/engine validity check).
- **EM (Exact Match)**: normalized SQL string match vs gold SQL (strict, conservative).
- **EX (Execution Accuracy)**: execute predicted SQL and compare its result set to the gold SQL result set.
- **TS (Test-Suite Accuracy)**: planned; compare predicted vs gold results across multiple distilled DB variants.

## Reproducibility checklist (log per run)

Record alongside results:
- git commit hash
- model id + quantization settings
- prompt template version (and any post-processing toggles)
- `k` (few-shot exemplars), random seed, benchmark version/hash
- GPU type/runtime notes (Colab)

## QLoRA (planned)

This project will compare prompting vs QLoRA SFT later. Key knobs to report in the dissertation:
- LoRA rank `r`, `alpha`, dropout, target modules
- batch size + grad accumulation, max seq length, LR/scheduler, warmup
- quantization config (4-bit NF4) and dtype

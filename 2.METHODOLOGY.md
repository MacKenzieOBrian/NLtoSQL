# Methodology (open-source replica)

This is the working story for the project: we restage the Ojuri et al. idea with open models and Colab-level GPUs. The flow is simple: NL→SQL on ClassicModels, always feed schema context, try prompting vs QLoRA vs (soon) an agent loop, and keep it reproducible on cheap hardware.

For the pipeline I think of it as a straight line: get schema awareness, pick a strategy (prompt / QLoRA / agent) to spit out SQL, run a quick safety/postprocess, execute on the live DB, score VA/EX/EM (TS later). 

Prompt hygiene stays consistent: system message, schema summary, a few exemplars (k), then the NLQ. Decoding is deterministic. Schema text is ordered so key/name columns show up first. Post-processing grabs the first `SELECT …;`, drops chatter, and trims list-style outputs to minimal columns. k=0 and k=3 are reported separately to avoid leakage.

---

# 3. Domain and Data

- Domain: **ClassicModels (MySQL)**
- Benchmarks: **200 held-out NL→SQL test examples**
- Training set: **~200 supervised examples (JSONL)**

**Rationale:** ClassicModels provides realistic business workloads involving joins, aggregates, and ordering. It also matches the domain used in Ojuri et al. (2025), enabling like-for-like reproduction against GPT-4 agent baselines.


---

# 4. Model Adaptation Strategies

Three adaptation families are evaluated:

| Strategy | Modifies Weights | Training | Iterative | Expected Consistency |
|---|---|---|---|---|
| Zero-Shot ICL | ✗ | ✗ | ✗ | Low |
| Few-Shot ICL | ✗ | ✗ | ✗ | Moderate |
| QLoRA FT | ✓ (Adapters) | ✓ | ✗ | High |
| ReAct / ExCoT | Optional | ✗ | ✓ | Highest |

This mirrors the structure in Ojuri et al. (2025), who compared GPT-4 prompting vs GPT-4 fine-tuned vs GPT-4 agent.

**ReAct alignment (current build):** A minimal Thought/Action/Observation loop with a single tool (safe SELECT executor), strict prompt (“one SELECT only, no DDL/DML/comments”), schema prepended, deterministic decode, and a short 3-step limit. It defaults to a 5-item slice to inspect SQL before full runs. Planned micro-upgrades to mirror the agentic uplift in Ojuri et al.: result-aware retries, small beam+rerank on SQL-only, optional grammar check, and trace logging.

**Current refinements:** Prompt stripping (decode only the reply), a lightweight projection guard (NLQ pattern → canonical minimal SELECT) to prevent extra columns/ORDER BY, and a result-aware retry (only mark success when SQL executes). On the small slice ReAct reaches VA/EX/EM=1.0; full-set run is next.

**References by design choice (quick map)**
- Schema-grounded prompting and baselines: Zhu et al., 2024 (LLM text-to-SQL survey); Rajkumar et al., 2024 (evaluating LLMs for text-to-SQL) justify VA/EX/TS-style evaluation.
- In-context vs fine-tuning: Mosbach et al., 2023 (few-shot vs FT stability); Chia et al., 2023 and Li et al., 2023 (ICE/C3 exemplar selection) motivate reporting k=0 and k=3.
- PEFT/QLoRA: Hu et al., 2021 (LoRA) and Dettmers et al., 2023 (QLoRA) support adapter-based FT under VRAM limits; Ding et al., 2023 (PEFT survey) as general rationale.
- Safety/SQL constraints: Scholak et al., 2021 (PICARD) and Binder/SQL-PaLM style constrained decoding motivate “single SELECT, no DDL/DML/comments,” prompt stripping, and minimal projection.
- Agentic refinement: Yao et al., 2023 (ReAct) and Shinn et al., 2023 (Reflexion) for Thought/Action/Observation + retries; Gao et al., 2023 and Chen et al., DIN-SQL/ValueNet for execution-based reranking/refinement.
- Benchmarks/context: BIRD (Wang et al., 2023), Spider, and Hong et al., 2025 (LLM DB interfaces) situate using ClassicModels/VA–EX–TS.
- Statistical comparison: Dietterich, 1998 (McNemar) for paired EX comparisons between prompt, QLoRA, and ReAct.

---

# 5. Training Configuration (Current Best Practice)

- Base model: **Llama-3-8B-Instruct**
- Quantization: **4-bit NF4**
- PEFT: **LoRA adapters only**
- Optimizer: `paged_adamw_8bit`
- VRAM target: **≤ 10GB (Colab T4)**
- Dtype: **bf16 on Ampere, fp16 fallback**
- Hyperparameters:
  - r = 32
  - α = 64
  - dropout = 0.05
  - LR ~ 1e-4
  - epochs ≈ 3
  - batch size = 1 w/ gradient accumulation = 8

This configuration allows fine-tuning without full model gradient updates, avoiding >60GB VRAM requirements of full FT.

---


# 9. Comparative Method Justification

### Why not full fine-tuning?
Requires >60GB VRAM; infeasible for open-source reproduction.

### Why not prompting-only?
Few-shot ICL exhibited instability and schema sensitivity, consistent with Mosbach et al. (2023).

### Why agent loops?
Execution feedback reduces logical/aggregation/join errors beyond static decoding (Yao et al., 2023; Ojuri et al., 2025).

### Why ClassicModels?
Matches proprietary baseline domain and workloads; avoids distributional confounds.

### Implementation influences (provenance)
The implementation drew on three pillars: (1) NL→SQL literature for schema grounding, refinement, and execution-based evaluation; (2) open-source ecosystems (HF Transformers/PEFT, SQLAlchemy, Cloud SQL connectors) for idiomatic model loading, fine-tuning, and DB access; and (3) official framework docs for API-level details on inference, quantization, and execution. The orchestration here is original to this reproduction, but follows standard applied NLP practice of combining established tools to answer new research questions.

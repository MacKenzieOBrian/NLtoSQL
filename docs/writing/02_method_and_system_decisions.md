# Method and System Decisions

The core methodological rule in this project is to change one major factor at a time while holding the evaluation contract steady. The fixed benchmark is `data/classicmodels_test_200.json`, and the shared scoring path is implemented in `nl2sql/evaluation/eval.py` via `nl2sql/eval.py`. This allows fair comparison between baseline and QLoRA conditions.

The baseline system uses prompt-based generation with controlled `k`-shot settings. The purpose is to establish a non-fine-tuned reference before adaptation claims are made. The run orchestration is in `notebooks/02_baseline_prompting_eval.ipynb`. Core behavior is handled by `nl2sql/core/prompting.py`, `nl2sql/core/llm.py`, `nl2sql/core/query_runner.py`, and `nl2sql/evaluation/eval.py`.

The QLoRA stage uses parameter-efficient adaptation so training remains feasible on limited GPU memory. Training and evaluation are in `notebooks/05_qlora_train_eval.ipynb`, with adapter outputs in `results/adapters/` and run outputs in `results/qlora/`. The key design decision is protocol matching: QLoRA outputs are evaluated with the same benchmark and scorer as baseline runs.

The final dissertation protocol is model-only raw evaluation. Post-generation SQL cleaning and agent-style correction loops are excluded from the final hypothesis path to avoid confounding the method comparison. This keeps the main question clear: does adaptation and/or few-shot context improve results under the same evaluator.

Constrained decoding and reliability cleanup (SQL guardrails plus postprocess) are still available as an optional extension path through evaluation toggles. They are reported as secondary engineering controls, not as the primary hypothesis evidence path.

Reproducibility is built into artifacts. Each run writes item-level outcomes and metadata to JSON, with per-run and per-k summaries. Statistical outputs are regenerated by `scripts/generate_research_comparison.py` and consumed in `notebooks/06_research_comparison.ipynb`.

These choices support a defensible writing stance: this is a controlled experimental stack with explicit boundaries, not a mixed set of disconnected techniques.

# Decision Record (Dissertation)

This document records major research and engineering decisions for the NL→SQL reproduction study, including rationale, rejected alternatives, and implementation locations. Decision traceability is important for reproducibility, evaluation validity, and dissertation defence.

---

## 1. Research Design & Experimental Framing

| Decision | Why (Justification) | Alternatives Rejected | Where |
|---|---|---|---|
| Replicate Ojuri et al. methodology using OSS | Enables fair comparison under open constraints | Evaluate GPT-4 directly → proprietary, non-reproducible | `STATE_OF_THE_ART.md`, `METHODOLOGY.md` |
| Comparative framing (prompting vs PEFT vs agent) | Avoids one-shot system claims; creates measurable uplift ladder | Only report final system → hides where improvements come from | `notebooks/` + dissertation Chapter 5 |
| ClassicModels as domain benchmark | Matches Ojuri; business queries; multi-table joins | Spider → domain mismatch; BIRD → compute heavy | `DATA.md` |

---

## 2. Dataset & Evaluation Discipline

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| Fixed 200-item test benchmark | Enables repeated experiments with stable metrics | Dynamic or streaming evaluation → non-deterministic | `data/classicmodels_test_200.json` |
| Disjoint train/test | Prevents leakage and invalid generalisation claims | “Few-shot only” approach → hides training impact | `notebooks/04_build_training_set.ipynb` |
| Schema included in evaluation inputs | Required for column/table grounding | Retrieval-based schema → increases architectural confounds | `DATA.md`, `prompting.py` |

---

## 3. Why We Run (k=0) and (k=3) in Multiple Phases

| Phase | What Changes | What It Tests | Why It Matters |
|---|---|---|---|
| Baseline (`k=0`) | Prompting, no exemplars | intrinsic LM SQL reasoning | establishes floor |
| Baseline (`k=3`) | + exemplars | ICL conditioning uplift | measures prompt-only gains |
| QLoRA (`k=0`) | trained adapters | SFT uplift alone | isolates fine-tuning effect |
| QLoRA (`k=3`) | adapters + exemplars | coexistence effects | tests complement vs substitution |
| Agentic (planned) | + execution loop | semantic correction | expected largest EX/TS gain |

**Reflection:** This ladder allows stronger causal inference than reporting a single configuration.

---

## 4. Metric Selection (Ojuri-Aligned)

| Metric | Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|---|
| VA | executability | distinguishes syntax vs logic | static parsing → misses runtime issues | `eval.py` |
| EM | surface strict match | debugging | used alone misleading for SQL | `eval.py` |
| EX | execution accuracy | functional correctness | BLEU/ROUGE | `eval.py` |
| TS | test-suite accuracy (planned) | semantic robustness | accuracy-only metrics | documented in `ARCHITECTURE.md` |

**Meta:** Execution-based metrics are standard in NL→SQL because SQL has many surface-equivalent programs.

---

## 5. Safety, Security & Data Access

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| Read-only SQL execution | prevents destructive outputs | trust model outputs → unsafe | `query_runner.py` |
| Block DDL/DML tokens | hallucinated UPDATE/DELETE possible | evaluate full SQL grammar → unnecessary | `eval.py` |
| Cloud SQL + Connector | private IP; IAM auth | public DB endpoint → security risk | `db.py`, `CONFIG.md` |

---

## 6. Reproducibility & Experimental Integrity

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| Colab as primary runtime | open access + realistic SME hardware | multi-GPU cluster → unrealistic/examiner-unfair | `CONFIG.md` |
| Pinned dependency versions | avoids Colab binary drift | “latest pip” → breaks reproducibility | `requirements.txt` |
| Deterministic decoding | eliminates sampling noise | sampling-based decoding → introduces variance | `llm.py` |
| Gitignored results | avoid accidental large artifacts | committing artifacts → brittle | `.gitignore` |

---

## 7. PEFT Training Data (QLoRA)

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| 200 supervised examples | realistic small-scale SFT | >5k examples → infeasible on Colab | `data/train/` |
| SQL validated before training | enforces VA=True → avoids garbage learning | train on synthetic → unknown distribution drift | `04_build_training_set.ipynb` |
| Mixed difficulty coverage | probes SQL behaviours | single-difficulty → misleading | `DATA.md` |

---

## 8. Compute Constraints & Deployment Reality

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| 8B scale + 4-bit quantization | fits Colab VRAM; QLoRA-compatible | full FP16 → 60–80GB | `CONFIG.md` |
| No external GPUs | aligns with SME deployment assumption | cloud GPU clusters | `COMPUTATIONAL_CONSTRAINTS.md` |
| OSS only (model + infra) | transparency + reproducibility | GPT-4 APIs | `DECISIONS.md` |

---

## 9. Threats to Validity (Acknowledged)

| Threat | Mitigation |
|---|---|
| schema overfitting (QLoRA) | evaluate k=0 vs k>0 |
| exemplar confound | exemplar freeze + deterministic ordering |
| DB sensitivity | freeze ClassicModels dump for TS |
| stochastic decoding | greedy decoding for evaluation |
| leakage risk | disjoint train/test, logged splits |

These will surface again in the dissertation’s Chapter 6 (Discussion).

---

## 10. Not Selected / Alternatives Rejected

See full section in `DECISIONS_REJECTED.md` or appendix, including:

- GPT-4 / Claude (proprietary)
- Full fine-tuning (VRAM infeasible)
- RAG schema retriever (confounds evaluation)
- Spider/BIRD (domain mismatch)
- BLEU/ROUGE (surface metrics)
- Human eval (subjective)

---

## 11. What to Cite in the Dissertation

- **Metrics + framework** → Ojuri et al. (2025)
- **TS metric origin** → Zhong et al. (2020)
- **Few-shot vs SFT argument** → Mosbach et al. (2023)
- **PEFT feasibility** → Ding et al. (2023), Goswami et al. (2024)

---

## 12. Meta Reflection

The decision record provides transparency and makes the research defensible. It converts experiments from “we did X” into “we chose X over Y because…”, which is the form required for dissertation rigor and scientific review.


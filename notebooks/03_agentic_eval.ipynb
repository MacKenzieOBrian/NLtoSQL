{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f2b652",
   "metadata": {},
   "source": [
    "# Agentic Evaluation (ReAct-style)\n",
    "\n",
    "Refs/Docs: `REFERENCES.md#ref-yao2023-react`, `REFERENCES.md#ref-zhai2025-excot`, `REFERENCES.md#ref-zhong2020-ts`, `REFERENCES.md#ref-yu2018-spider`. Docs: HF Transformers, Cloud SQL Connector, SQLAlchemy.\n",
    "\n",
    "Say: This notebook adds a bounded ReAct-style loop on top of the ClassicModels benchmark and evaluates VA/EM/EX/TS.\n",
    "Why: Execution feedback helps fix runnable-but-wrong SQL without changing model weights.\n",
    "\n",
    "Plan (step-by-step):\n",
    "1) Setup runtime\n",
    "2) DB connection\n",
    "3) Schema + test set\n",
    "4) Model load\n",
    "5) ReAct agent (`nl2sql/agent.py`)\n",
    "6) Evaluate + save JSON\n",
    "\n",
    "Code pointers: `nl2sql/agent.py`, `nl2sql/eval.py`, `nl2sql/query_runner.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c44350",
   "metadata": {},
   "source": [
    "Refs/Docs (quick list):\n",
    "- HF Transformers quantization docs\n",
    "- PEFT + TRL docs\n",
    "- Cloud SQL Connector docs\n",
    "- SQLAlchemy engine/execute docs\n",
    "- ReAct paper `REFERENCES.md#ref-yao2023-react`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faade3c2",
   "metadata": {},
   "source": [
    "## Setup (run first, then restart)\n",
    "\n",
    "Docs: HF Transformers quantization docs; BitsAndBytes docs.\n",
    "Say: One-time environment pinning so results are reproducible.\n",
    "Why: Small version drift changes generation and metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79779a6",
   "metadata": {},
   "source": [
    "Docs: HF Transformers quantization; BitsAndBytes (4-bit) install guidance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef67421",
   "metadata": {},
   "source": [
    "Docs: HF/BnB version pinning guidance (runtime stability).\n",
    "Say: Pin versions so results do not drift across runs.\n",
    "Why: Minor library changes can shift VA/EX/TS.\n",
    "Code pointers: `requirements.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b144d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "set -e\n",
    "export PIP_DEFAULT_TIMEOUT=120\n",
    "\n",
    "# Clean conflicting preinstalls\n",
    "pip uninstall -y torch torchvision torchaudio bitsandbytes triton transformers accelerate peft trl datasets numpy pandas fsspec requests google-auth || true\n",
    "\n",
    "# Base deps\n",
    "pip install -q --no-cache-dir --force-reinstall   numpy==1.26.4 pandas==2.2.1 fsspec==2024.5.0 requests==2.31.0 google-auth==2.43.0\n",
    "\n",
    "# Torch + CUDA 12.1\n",
    "pip install -q --no-cache-dir --force-reinstall   torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121   --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# bitsandbytes + triton + HF stack\n",
    "pip install -q --no-cache-dir --force-reinstall   bitsandbytes==0.43.3 triton==2.3.1   transformers==4.44.2 accelerate==0.33.0 peft==0.17.0 trl==0.9.6 datasets==2.20.0\n",
    "\n",
    "echo \"Setup complete. Restart runtime once, then run the rest of the notebook top-to-bottom.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b0b11",
   "metadata": {},
   "source": [
    "Model load: HF 4-bit NF4 + optional PEFT adapters; baseline decoding is deterministic.\n",
    "Refs: `REFERENCES.md#ref-ding2023-peft`, `REFERENCES.md#ref-goswami2024-peft`.\n",
    "Docs: HF quantization, PEFT.\n",
    "Code pointers: `scripts/run_full_pipeline.py`, `nl2sql/llm.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b6008",
   "metadata": {},
   "source": [
    "Docs: Colab + git clone workflow.\n",
    "Say: Clone the repo so we reuse the same `nl2sql/` modules as the scripts.\n",
    "Why: Keeps logic in code, not hidden in the notebook.\n",
    "Code pointers: `nl2sql/`, `scripts/run_full_pipeline.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47936fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Clone repo (Colab) + install deps\n",
    "import os\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not os.path.exists('/content/NLtoSQL'):\n",
    "        !git clone https://github.com/MacKenzieOBrian/NLtoSQL.git /content/NLtoSQL\n",
    "    %cd /content/NLtoSQL\n",
    "    !pip -q install -r requirements.txt\n",
    "    import torch, transformers, accelerate, peft\n",
    "    print('torch', torch.__version__, 'cuda', torch.cuda.is_available())\n",
    "else:\n",
    "    print('Not in Colab; using existing workspace')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24721f1",
   "metadata": {},
   "source": [
    "Prompt/eval: build prompts (system + schema + k exemplars), generate SQL, postprocess, then compute VA/EM/EX.\n",
    "Refs: `REFERENCES.md#ref-brown2020-gpt3`, `REFERENCES.md#ref-mosbach2023-icl`, `REFERENCES.md#ref-zhong2020-ts`.\n",
    "Code pointers: `nl2sql/prompting.py`, `nl2sql/llm.py`, `nl2sql/postprocess.py`, `nl2sql/eval.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b7e43",
   "metadata": {},
   "source": [
    "Docs: Standard Colab clone/install pattern.\n",
    "Say: Keep the notebook thin and reuse code from `nl2sql/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20974344",
   "metadata": {},
   "source": [
    "### Reference notes (what this builds on)\n",
    "\n",
    "Refs: `REFERENCES.md#ref-zhu2024-survey`, `REFERENCES.md#ref-hong2025-survey`, `REFERENCES.md#ref-yao2023-react`, `REFERENCES.md#ref-ojuri2025-agents`.\n",
    "Docs: Cloud SQL Connector, SQLAlchemy creator pattern, HF quantization.\n",
    "Code pointers: `nl2sql/db.py`, `nl2sql/schema.py`, `nl2sql/agent.py`, `nl2sql/eval.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f75fce",
   "metadata": {},
   "source": [
    "## Optional: use gcloud ADC (without a key)\n",
    "\n",
    "Docs: GCP ADC (Application Default Credentials).\n",
    "Say: Use ADC if you do not want to pass a service account JSON file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ff993",
   "metadata": {},
   "source": [
    "Docs: GCP ADC auth flow (Application Default Credentials).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58d00e",
   "metadata": {},
   "source": [
    "Docs: GCP ADC auth flow.\n",
    "Say: Optional ADC auth for Cloud SQL (no key file required).\n",
    "Code pointers: `nl2sql/db.py` (`create_engine_with_connector`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this only if you prefer gcloud-based ADC (no JSON key)\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    %pip install -q --upgrade google-auth google-auth-oauthlib\n",
    "    !gcloud auth application-default login\n",
    "else:\n",
    "    print(\"Not in Colab; skip gcloud auth.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21847187",
   "metadata": {},
   "source": [
    "Refs/Docs: Pinned CUDA + bitsandbytes/triton stack per HF/BnB guidance for 4-bit loads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5528bc3a",
   "metadata": {},
   "source": [
    "Docs: Cloud SQL Connector + SQLAlchemy creator pattern (MySQL).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e745f2",
   "metadata": {},
   "source": [
    "Docs: Cloud SQL Connector + SQLAlchemy `creator=` hook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada87478",
   "metadata": {},
   "source": [
    "Docs: SQLAlchemy execute + Cloud SQL Connector.\n",
    "Say: Create the base DB engine and a SELECT-only QueryRunner (the Act step).\n",
    "Why: QueryRunner gives VA (executability) and blocks destructive SQL.\n",
    "Code pointers: `nl2sql/db.py`, `nl2sql/query_runner.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Environment + DB\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "from nl2sql.db import create_engine_with_connector, safe_connection\n",
    "\n",
    "# Expected env vars (set these in a Colab cell):\n",
    "# INSTANCE_CONNECTION_NAME, DB_USER, DB_PASS, DB_NAME\n",
    "INSTANCE_CONNECTION_NAME = os.getenv(\"INSTANCE_CONNECTION_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\") or \"classicmodels\"\n",
    "\n",
    "if not INSTANCE_CONNECTION_NAME:\n",
    "    INSTANCE_CONNECTION_NAME = input(\"Enter INSTANCE_CONNECTION_NAME: \").strip()\n",
    "if not DB_USER:\n",
    "    DB_USER = input(\"Enter DB_USER: \").strip()\n",
    "if not DB_PASS:\n",
    "    DB_PASS = getpass(\"Enter DB_PASS: \")\n",
    "\n",
    "# Canonical engine builder (shared with scripts + other notebooks).\n",
    "# Uses Cloud SQL Connector under the hood and ADC for credentials.\n",
    "engine, connector = create_engine_with_connector(\n",
    "    instance_connection_name=INSTANCE_CONNECTION_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASS,\n",
    "    db_name=DB_NAME,\n",
    ")\n",
    "\n",
    "with safe_connection(engine) as conn:\n",
    "    conn.execute(text(\"SELECT 1\"))\n",
    "print(\"DB connection OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9a6fb",
   "metadata": {},
   "source": [
    "Say: Create an engine factory for TS replica DBs.\n",
    "Why: TS compares gold vs pred across perturbed DBs to avoid lucky execution.\n",
    "Refs: `REFERENCES.md#ref-zhong2020-ts`.\n",
    "Code pointers: this cell (`make_engine`), `nl2sql/eval.py:test_suite_accuracy_for_item`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b) Engine factory for TS (multiple DB names)\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "\n",
    "def make_engine(db_name: str) -> Engine:\n",
    "    \"\"\"Create a new engine bound to a specific TS replica DB name.\n",
    "\n",
    "    TS (test-suite accuracy) executes the same (gold, pred) SQL across multiple\n",
    "    replica databases (classicmodels_ts_XX). We keep separate engines so each\n",
    "    replica is evaluated independently.\n",
    "    \"\"\"\n",
    "\n",
    "    def getconn_for_db():\n",
    "        return connector.connect(\n",
    "            INSTANCE_CONNECTION_NAME,\n",
    "            \"pymysql\",\n",
    "            user=DB_USER,\n",
    "            password=DB_PASS,\n",
    "            db=db_name,\n",
    "        )\n",
    "\n",
    "    return sqlalchemy.create_engine(\"mysql+pymysql://\", creator=getconn_for_db, future=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a583e16",
   "metadata": {},
   "source": [
    "Refs: schema-grounded prompting surveys `REFERENCES.md#ref-zhu2024-survey`, `REFERENCES.md#ref-hong2025-survey`.\n",
    "Code pointers: `nl2sql/schema.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7620ff8",
   "metadata": {},
   "source": [
    "Docs: schema-grounded prompting (survey context); Spider-style schema summaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6cfbfc",
   "metadata": {},
   "source": [
    "Refs: `REFERENCES.md#ref-zhu2024-survey`, `REFERENCES.md#ref-hong2025-survey`.\n",
    "Say: Build schema text and load a small debug slice.\n",
    "Why: Smaller runs let you iterate on postprocess + agent logic quickly.\n",
    "Code pointers: `nl2sql/schema.py:build_schema_summary`, `data/classicmodels_test_200.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load schema summary + test set (small slice for now)\n",
    "import json\n",
    "from nl2sql.schema import build_schema_summary\n",
    "\n",
    "SCHEMA_SUMMARY = build_schema_summary(engine, db_name=DB_NAME)\n",
    "\n",
    "test_path = Path(\"data/classicmodels_test_200.json\")\n",
    "full_set = json.loads(test_path.read_text(encoding=\"utf-8\"))\n",
    "# default to a small slice while debugging\n",
    "test_set = full_set[:5]\n",
    "print(\"Demo items:\", len(test_set))\n",
    "# For full run, switch to: test_set = full_set; print(\"Test items:\", len(test_set))\n",
    "\n",
    "TABLES = {line.split('(', 1)[0].strip() for line in SCHEMA_SUMMARY.splitlines() if '(' in line}\n",
    "TABLES_LOWER = {t.lower(): t for t in TABLES}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700fe5c2",
   "metadata": {},
   "source": [
    "Refs/Docs: HF 4-bit NF4 + BitsAndBytes; adapters via PEFT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92a2b8",
   "metadata": {},
   "source": [
    "Docs: HF quantization + PEFT/QLoRA load patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b3611c",
   "metadata": {},
   "source": [
    "Say: Load the base model (4-bit) and optional PEFT adapters.\n",
    "Why: Baseline decoding is deterministic; agent sampling is controlled by config.\n",
    "Refs: `REFERENCES.md#ref-ding2023-peft`.\n",
    "Code pointers: `nl2sql/llm.py`, `scripts/run_full_pipeline.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f38fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Load model (base or QLoRA adapters)\n",
    "import os\n",
    "from getpass import getpass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "ADAPTER_PATH = os.getenv(\"ADAPTER_PATH\") or \"results/adapters/qlora_classicmodels\"  # set to None to use base model\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if not HF_TOKEN:\n",
    "    HF_TOKEN = getpass(\"Enter HF_TOKEN (https://huggingface.co/settings/tokens): \").strip()\n",
    "\n",
    "cc_major, cc_minor = torch.cuda.get_device_capability(0) if torch.cuda.is_available() else (0, 0)\n",
    "use_bf16 = cc_major >= 8\n",
    "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"Using bf16:\", use_bf16)\n",
    "print(\"Adapter path:\", ADAPTER_PATH)\n",
    "\n",
    "# Tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# Quantized base model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map={\"\": 0} if torch.cuda.is_available() else None,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "base_model.generation_config.do_sample = False\n",
    "base_model.generation_config.temperature = 1.0\n",
    "base_model.generation_config.top_p = 1.0\n",
    "\n",
    "# Load adapters if present locally; otherwise use base model\n",
    "adapter_dir = Path(ADAPTER_PATH) if ADAPTER_PATH else None\n",
    "if adapter_dir and adapter_dir.exists():\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_dir, token=HF_TOKEN)\n",
    "    print(\"Loaded adapters from\", adapter_dir)\n",
    "else:\n",
    "    print(\"Adapter path missing; using base model only. Set ADAPTER_PATH to your local adapter folder or upload it to Colab.\")\n",
    "    model = base_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afb59ed",
   "metadata": {},
   "source": [
    "## Optional adapter sanity check (run before ReAct)\n",
    "Say: Quick test to confirm the model/adapters can produce executable SQL.\n",
    "Why: Catch obvious issues before the agent loop.\n",
    "Refs: `REFERENCES.md#ref-mosbach2023-icl`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db24b08d",
   "metadata": {},
   "source": [
    "Refs/Docs: ICL patterns `REFERENCES.md#ref-brown2020-gpt3`, evaluation limits `REFERENCES.md#ref-zhong2020-ts`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7065790e",
   "metadata": {},
   "source": [
    "Refs: `REFERENCES.md#ref-brown2020-gpt3`, `REFERENCES.md#ref-zhong2020-ts`.\n",
    "Say: Quick end-to-end check: prompt -> generate -> postprocess -> execute -> compare.\n",
    "Why: Detect early failures before full runs.\n",
    "Code pointers: `nl2sql/prompting.py`, `nl2sql/postprocess.py`, `nl2sql/query_runner.py`, `nl2sql/eval.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fbae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nl2sql.prompting import make_few_shot_messages\n",
    "from nl2sql.llm import extract_first_select\n",
    "from nl2sql.postprocess import guarded_postprocess\n",
    "from nl2sql.query_runner import QueryRunner\n",
    "from nl2sql.eval import execution_accuracy\n",
    "\n",
    "runner_check = QueryRunner(engine)\n",
    "# reuse existing test_set (default small slice); pick 3 exemplars\n",
    "exemplars = test_set[:3]\n",
    "\n",
    "def run_quick_check(k: int = 0, limit: int = 3):\n",
    "    print(f\"Quick check k={k}\")\n",
    "    for sample in test_set[:limit]:\n",
    "        shots = exemplars if k > 0 else []\n",
    "        msgs = make_few_shot_messages(\n",
    "            schema=SCHEMA_SUMMARY,\n",
    "            exemplars=shots,\n",
    "            nlq=sample['nlq'],\n",
    "        )\n",
    "        prompt_preview = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tok(prompt_preview, return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "\n",
    "        # strip the prompt before decoding the generation\n",
    "        gen_ids = out[0][inputs.input_ids.shape[-1]:]\n",
    "        text = tok.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "        raw_sql = extract_first_select(text) or text\n",
    "        sql = guarded_postprocess(raw_sql, sample['nlq'])\n",
    "\n",
    "        meta = runner_check.run(sql, capture_df=False)\n",
    "        va = meta.success\n",
    "        ex_ok, _, _ = execution_accuracy(engine=engine, pred_sql=sql, gold_sql=sample['sql'])\n",
    "        print(f\"Q: {sample['nlq']}\\nSQL: {sql}\\nVA: {va} EX: {ex_ok}\\n\")\n",
    "\n",
    "run_quick_check(k=0)\n",
    "run_quick_check(k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d3d737",
   "metadata": {},
   "source": [
    "Refs: ReAct pattern `REFERENCES.md#ref-yao2023-react` applied to NL to SQL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2d11f",
   "metadata": {},
   "source": [
    "Docs: ReAct paper + SELECT-only execution guard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21452dc1",
   "metadata": {},
   "source": [
    "Refs: `REFERENCES.md#ref-zhu2024-survey`, `REFERENCES.md#ref-hong2025-survey`.\n",
    "Say: Import the shared heuristics used by the agent.\n",
    "Why: Keeps policy (intent checks, scoring) in `nl2sql/` not in the notebook.\n",
    "Code pointers: `nl2sql/agent_utils.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ff33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper imports (optional; used for interactive inspection)\n",
    "# Main agent loop is in `nl2sql/agent.py`.\n",
    "from nl2sql.agent_utils import intent_constraints, semantic_score, count_select_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0973b",
   "metadata": {},
   "source": [
    "### Agent status (for dissertation)\n",
    "\n",
    "Say: The current loop is a bounded ReAct-style agent (generate -> execute -> observe -> repair/fallback).\n",
    "Why: It improves correctness without changing model weights; all changes are deterministic or gated.\n",
    "Refs: `REFERENCES.md#ref-yao2023-react`, `REFERENCES.md#ref-zhai2025-excot`.\n",
    "Code pointers: `nl2sql/agent.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13e02a",
   "metadata": {},
   "source": [
    "Refs: execution-based evaluation `REFERENCES.md#ref-ojuri2025-agents`, TS motivation `REFERENCES.md#ref-zhong2020-ts`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52889a3",
   "metadata": {},
   "source": [
    "Refs/Docs: ICL patterns `REFERENCES.md#ref-brown2020-gpt3`, evaluation limits `REFERENCES.md#ref-zhong2020-ts`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9001e3",
   "metadata": {},
   "source": [
    "## ReAct execution-guided pipeline (current version)\n",
    "\n",
    "Refs: `REFERENCES.md#ref-yao2023-react`, `REFERENCES.md#ref-zhai2025-excot`.\n",
    "Say: These cells configure the canonical agent and evaluation harness.\n",
    "Code pointers: `nl2sql/agent.py`, `nl2sql/eval.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0685cc3d",
   "metadata": {},
   "source": [
    "## Reference Map (Code <-> Literature)\n",
    "\n",
    "Refs: `REFERENCES.md#ref-yao2023-react`, `REFERENCES.md#ref-zhai2025-excot`, `REFERENCES.md#ref-scholak2021-picard`, `REFERENCES.md#ref-zhu2024-survey`, `REFERENCES.md#ref-li2023-bigbench`, `REFERENCES.md#ref-zhong2020-ts`.\n",
    "\n",
    "- Execution feedback + repair\n",
    "- Output constraints / validity\n",
    "- Projection control + schema linking context\n",
    "- Semantic evaluation via test suites\n",
    "\n",
    "Code pointers: `nl2sql/agent.py`, `nl2sql/postprocess.py`, `nl2sql/agent_utils.py`, `nl2sql/eval.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730eda73",
   "metadata": {},
   "source": [
    "Refs: `REFERENCES.md#ref-yu2018-spider`.\n",
    "Say: Reload schema + full test set and create the runner for real evaluation.\n",
    "Code pointers: `nl2sql/schema.py`, `nl2sql/query_runner.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe76f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Schema summary + test set + QueryRunner\n",
    "import json\n",
    "from pathlib import Path\n",
    "from nl2sql.schema import build_schema_summary\n",
    "from nl2sql.query_runner import QueryRunner\n",
    "\n",
    "DB_NAME = os.getenv(\"DB_NAME\") or \"classicmodels\"\n",
    "\n",
    "SCHEMA_SUMMARY = build_schema_summary(engine, db_name=DB_NAME)\n",
    "# Schema summary is used in prompts to ground column/table choices.\n",
    "test_path = Path(\"data/classicmodels_test_200.json\")\n",
    "full_set = json.loads(test_path.read_text(encoding=\"utf-8\"))\n",
    "test_set = full_set  # change to full_set[:20] when debugging\n",
    "\n",
    "print(\"Loaded test set size:\", len(test_set))\n",
    "runner = QueryRunner(engine)  # QueryRunner enforces SELECT-only execution and records errors for VA/EX.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f4b1f",
   "metadata": {},
   "source": [
    "Docs: notebook rerun hygiene.\n",
    "Say: Defensive re-import so the following cells run cleanly after partial reruns.\n",
    "Code pointers: `nl2sql/agent.py`, `nl2sql/agent_utils.py`, `nl2sql/postprocess.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b07b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Agent utilities (used inside `nl2sql/agent.py`)\n",
    "from nl2sql.agent_utils import intent_constraints, semantic_score, count_select_columns, vanilla_candidate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952b8ec",
   "metadata": {},
   "source": [
    "## 6. Agent Implementation (Module-Based, Explainable ReAct Loop)\n",
    "\n",
    "Say: The agent lives in code so the notebook is just configuration + evaluation.\n",
    "Why: One source of truth makes the loop easy to defend.\n",
    "Refs: `REFERENCES.md#ref-yao2023-react`, `REFERENCES.md#ref-zhai2025-excot`.\n",
    "\n",
    "What the loop does (high level):\n",
    "- Build prompts (ReAct + optional tabular)\n",
    "- Generate a small candidate set (bounded)\n",
    "- Clean + deterministic postprocess\n",
    "- Execute gate (SELECT-only)\n",
    "- Intent gate\n",
    "- Score and pick best\n",
    "- Optional repair using DB error message\n",
    "- Fallback to deterministic baseline if needed\n",
    "\n",
    "Code pointers:\n",
    "- `nl2sql/agent.py` (`ReactSqlAgent.react_sql`, `evaluate_candidate`, `repair_sql`)\n",
    "- `nl2sql/postprocess.py` (`guarded_postprocess`)\n",
    "- `nl2sql/agent_utils.py` (`clean_candidate_with_reason`, `intent_constraints`, `semantic_score`, `count_select_columns`, `enforce_projection_contract`)\n",
    "- `nl2sql/query_runner.py` (`QueryRunner.run`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d555900",
   "metadata": {},
   "source": [
    "Refs: `REFERENCES.md#ref-yao2023-react`.\n",
    "Say: Configure the agent in one place (`ReactConfig`).\n",
    "Why: Bounded steps/candidates/repair makes the loop auditable.\n",
    "Code pointers: `nl2sql/agent.py` (`ReactConfig`, `ReactSqlAgent`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed5d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Agent implementation (imported)\n",
    "\n",
    "from nl2sql.agent import ReactConfig, ReactSqlAgent\n",
    "\n",
    "# Keep config explicit so it is easy to justify in a viva (bounded steps/candidates/repair).\n",
    "CFG = ReactConfig(\n",
    "    max_steps=3,\n",
    "    num_cands=6,\n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=128,\n",
    "    enable_repair=True,\n",
    "    repair_num_cands=4,\n",
    "    use_tabular_prompt=True,\n",
    "    use_schema_subset=True,\n",
    "    use_projection_contract=True,\n",
    ")\n",
    "\n",
    "agent = ReactSqlAgent(model=model, tok=tok, runner=runner, cfg=CFG)\n",
    "# Preserve the old function name used later in the notebook.\n",
    "react_sql = agent.react_sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3feb26",
   "metadata": {},
   "source": [
    "Refs: `REFERENCES.md#ref-yao2023-react`.\n",
    "Say: Run the bounded ReAct loop and capture a trace you can explain.\n",
    "What to say out loud:\n",
    "- \"The agent proposes candidates, executes them, and uses the DB response as feedback.\"\n",
    "- \"Candidates must pass execution + intent gates before scoring.\"\n",
    "- \"Each rejection is logged with a reason.\"\n",
    "\n",
    "Code pointers: `nl2sql/agent.py:ReactSqlAgent.react_sql`, `nl2sql/query_runner.py:QueryRunner.run`, `nl2sql/agent_utils.py` (gates + scoring).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f604abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) ReAct loop\n",
    "# The loop lives in `nl2sql/agent.py` (ReactSqlAgent.react_sql).\n",
    "# This notebook calls `react_sql(...)` for quick checks and full evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a77d6f",
   "metadata": {},
   "source": [
    "## EX Troubleshooting Checklist\n",
    "\n",
    "Say: If VA is high but EX is low, it is usually a semantic mismatch.\n",
    "Refs: `REFERENCES.md#ref-zhong2020-ts`.\n",
    "\n",
    "Quick checks:\n",
    "- Projection drift: tighten `enforce_projection_contract`\n",
    "- Wrong intent: check `intent_constraints`\n",
    "- Wrong table/join: verify schema-subset prompt\n",
    "- Missing literals: make sure filters appear in SQL\n",
    "\n",
    "Code pointers: `nl2sql/agent_utils.py`, `nl2sql/postprocess.py`, `nl2sql/eval.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca4830",
   "metadata": {},
   "source": [
    "Refs: `REFERENCES.md#ref-zhong2020-ts`.\n",
    "Say: Manual spot-checks before full eval.\n",
    "Why: Inspect trace and errors before running TS across many DBs.\n",
    "Code pointers: this cell prints the `trace` from `react_sql`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Quick sanity check on a few items\n",
    "schema_text = SCHEMA_SUMMARY\n",
    "for sample in test_set[:5]:\n",
    "    nlq = sample[\"nlq\"]\n",
    "    gold = sample[\"sql\"]\n",
    "    pred, trace = react_sql(\n",
    "        nlq=nlq,\n",
    "        schema_text=schema_text,\n",
    "        schema_summary=SCHEMA_SUMMARY,\n",
    "        exemplars=test_set[:3],\n",
    "    )\n",
    "    print(\"NLQ:\", nlq)\n",
    "    print(\"PRED:\", pred)\n",
    "    print(\"GOLD:\", gold)\n",
    "    print(\"TRACE LEN:\", len(trace))\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc47b20",
   "metadata": {},
   "source": [
    "### Stage 3 Interpretation (29 Jan 2026)\n",
    "\n",
    "Refs: `REFERENCES.md#ref-zhong2020-ts`.\n",
    "Say: Current loop usually returns executable SQL; remaining issues are projection bloat and spurious ORDER/GROUP BY.\n",
    "Why: These hurt EM more than EX; use deterministic guards to stabilize.\n",
    "Code pointers: `nl2sql/postprocess.py`, `nl2sql/agent_utils.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceeca8b",
   "metadata": {},
   "source": [
    "## Run Order (recommended)\n",
    "\n",
    "Docs: notebook run order (practical).\n",
    "Say: Follow this order to avoid DB/auth issues and wasted TS runs.\n",
    "\n",
    "1) Runtime + deps (restart after install)\n",
    "2) Cloud SQL connector + base engine\n",
    "3) TS engine factory (`make_engine`)\n",
    "4) Schema + dataset\n",
    "5) Agent config + loop\n",
    "6) TS harness\n",
    "7) Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89984dc7",
   "metadata": {},
   "source": [
    "Say: Import the TS evaluator (semantic robustness across replica DBs).\n",
    "Refs: `REFERENCES.md#ref-zhong2020-ts`.\n",
    "Code pointers: `nl2sql/eval.py:test_suite_accuracy_for_item`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Test Suite Accuracy (TS) evaluation ===\n",
    "# Harness now lives in nl2sql.eval for reuse in scripts.\n",
    "from nl2sql.eval import test_suite_accuracy_for_item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c61e8a",
   "metadata": {},
   "source": [
    "Refs: `REFERENCES.md#ref-zhong2020-ts`.\n",
    "Say: Cost guards so TS/EX are fast while debugging.\n",
    "Why: TS multiplies cost by N replica DBs.\n",
    "Code pointers: `nl2sql/eval.py` (row caps + suite size).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ae08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Quick test toggles (set before full eval) ===\n",
    "# Use small values to sanityâ€‘check TS/EX before full runs.\n",
    "QUICK_LIMIT = 20   # number of NLQs to evaluate (set None for full set)\n",
    "TS_N = 3           # number of TS DBs (set 10 for full TS)\n",
    "MAX_ROWS_TS = 500  # row cap per query in TS (raise for full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f9403",
   "metadata": {},
   "source": [
    "Refs: `REFERENCES.md#ref-zhong2020-ts`, `REFERENCES.md#ref-yu2018-spider`.\n",
    "Say: Full evaluation loop (VA/EM/EX/TS) + save results JSON.\n",
    "Why: Produces per-item metrics and aggregate rates for the dissertation.\n",
    "Code pointers: this cell, `nl2sql/eval.py`, `nl2sql/query_runner.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b771d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Full ReAct-style evaluation (VA/EX/EM/TS) over test_set\n",
    "\n",
    "import json\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "from nl2sql.eval import execution_accuracy, test_suite_accuracy_for_item\n",
    "from nl2sql.postprocess import normalize_sql\n",
    "\n",
    "results = []\n",
    "\n",
    "TS_PREFIX = \"classicmodels_ts\"\n",
    "SUITE_DBS = [f\"{TS_PREFIX}_{i:02d}\" for i in range(1, TS_N + 1)]\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=32)\n",
    "def make_engine_cached(db_name: str) -> Engine:\n",
    "    return make_engine(db_name)\n",
    "\n",
    "\n",
    "def make_engine_fn(db_name: str) -> Engine:\n",
    "    return make_engine_cached(db_name)\n",
    "\n",
    "\n",
    "LIMIT = QUICK_LIMIT  # override from quick toggles\n",
    "items = test_set[:LIMIT] if LIMIT else test_set\n",
    "schema_text = SCHEMA_SUMMARY\n",
    "\n",
    "# Per-item evaluation: generate SQL and compute VA/EM/EX/TS.\n",
    "for i, sample in enumerate(items, start=1):\n",
    "    nlq = sample[\"nlq\"]\n",
    "    gold_sql = sample[\"sql\"]\n",
    "\n",
    "    pred_sql, trace = react_sql(\n",
    "        nlq=nlq,\n",
    "        schema_text=schema_text,\n",
    "        schema_summary=SCHEMA_SUMMARY,\n",
    "        exemplars=test_set[:3],\n",
    "    )\n",
    "\n",
    "    # EM is strict (normalized) string match; kept as a diagnostic signal.\n",
    "    em = int(normalize_sql(pred_sql) == normalize_sql(gold_sql))\n",
    "\n",
    "    # VA = executability on base DB.\n",
    "    meta = runner.run(pred_sql, capture_df=False)\n",
    "    va = int(meta.success)\n",
    "\n",
    "    # EX = result equivalence on base DB (only meaningful if VA=1).\n",
    "    ex = 0\n",
    "    ex_pred_err = None\n",
    "    ex_gold_err = None\n",
    "    if va:\n",
    "        ex_ok, ex_pred_err, ex_gold_err = execution_accuracy(\n",
    "            engine=engine,\n",
    "            pred_sql=pred_sql,\n",
    "            gold_sql=gold_sql,\n",
    "        )\n",
    "        ex = int(ex_ok)\n",
    "\n",
    "    # TS is expensive (runs across N replica DBs). Skip if pred_sql does not\n",
    "    # execute on the base DB (VA=0) because TS would be 0 anyway.\n",
    "    ts = 0\n",
    "    if not va:\n",
    "        ts_debug = {\"skipped\": True, \"reason\": \"va=0\", \"error\": meta.error}\n",
    "    else:\n",
    "        ts, ts_debug = test_suite_accuracy_for_item(\n",
    "            make_engine_fn=make_engine_fn,\n",
    "            suite_db_names=SUITE_DBS,\n",
    "            gold_sql=gold_sql,\n",
    "            pred_sql=pred_sql,\n",
    "            max_rows=MAX_ROWS_TS,\n",
    "            strict_gold=True,\n",
    "        )\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"nlq\": nlq,\n",
    "            \"gold_sql\": gold_sql,\n",
    "            \"pred_sql\": pred_sql,\n",
    "            \"va\": va,\n",
    "            \"em\": em,\n",
    "            \"ex\": ex,\n",
    "            \"ts\": ts,\n",
    "            \"error\": meta.error or ex_pred_err,\n",
    "            \"gold_error\": ex_gold_err,\n",
    "            \"ts_debug\": ts_debug,\n",
    "            \"trace\": trace,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {i}/{len(items)}\")\n",
    "\n",
    "va_rate = sum(r[\"va\"] for r in results) / max(len(results), 1)\n",
    "ex_rate = sum(r[\"ex\"] for r in results) / max(len(results), 1)\n",
    "em_rate = sum(r[\"em\"] for r in results) / max(len(results), 1)\n",
    "ts_rate = sum(r[\"ts\"] for r in results) / max(len(results), 1)\n",
    "print(\"ReAct VA:\", va_rate, \"EX:\", ex_rate, \"EM:\", em_rate, \"TS:\", ts_rate)\n",
    "\n",
    "Path(\"results/agent\").mkdir(parents=True, exist_ok=True)\n",
    "save_path = Path(\"results/agent/results_react_200.json\")\n",
    "save_path.write_text(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"va_rate\": va_rate,\n",
    "            \"ex_rate\": ex_rate,\n",
    "            \"em_rate\": em_rate,\n",
    "            \"ts_rate\": ts_rate,\n",
    "            \"items\": results,\n",
    "        },\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    ),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "print(\"Saved to\", save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "64f2b652",
      "metadata": {},
      "source": [
        "# Agentic Evaluation (ReAct-style)\n",
        "\n",
        "This notebook adds a minimal ReAct-style loop for NL→SQL. It reuses the same benchmark (`data/classicmodels_test_200.json`) and metrics (VA/EX/EM; TS planned) to measure gains over prompt-only and QLoRA runs.\n",
        "\n",
        "Plan (step-by-step):\n",
        "1) Clone repo (Colab) + install deps\n",
        "2) Environment + DB connection\n",
        "3) Load schema summary + test set\n",
        "4) Load model (base or QLoRA adapters)\n",
        "5) Define ReAct prompt + loop (Thought → Action → Observation → Refinement)\n",
        "6) Run evaluation (VA/EX/EM) and save to `results/agent/…`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c44350",
      "metadata": {},
      "source": [
        "Docs I leaned on: HF Transformers quantization (https://huggingface.co/docs/transformers/main_classes/quantization), PEFT/TRL (https://huggingface.co/docs/peft/, https://huggingface.co/docs/trl/), Cloud SQL connector + SQLAlchemy creator (https://cloud.google.com/sql/docs/mysql/connect-run, https://docs.sqlalchemy.org/en/20/core/engines.html#custom-dbapi-connect), ReAct (https://arxiv.org/abs/2210.03629)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faade3c2",
      "metadata": {},
      "source": [
        "## Setup (run first, then restart)\n",
        "In a fresh Colab GPU runtime, run this one cell to clean preinstalls and pin the CUDA 12.1 torch/bitsandbytes/triton stack. When it finishes, **Runtime → Restart runtime**, then run the rest of the notebook from the clone cell onward without more restarts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b79779a6",
      "metadata": {},
      "source": [
        "**Docs (setup):** HF Transformers quantization + BitsAndBytes (4-bit) https://huggingface.co/docs/transformers/main_classes/quantization, bnb https://github.com/TimDettmers/bitsandbytes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b144d3dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "%%bash\n",
        "set -e\n",
        "export PIP_DEFAULT_TIMEOUT=120\n",
        "\n",
        "# Clean conflicting preinstalls\n",
        "pip uninstall -y torch torchvision torchaudio bitsandbytes triton transformers accelerate peft trl datasets numpy pandas fsspec requests google-auth || true\n",
        "\n",
        "# Base deps\n",
        "pip install -q --no-cache-dir --force-reinstall   numpy==1.26.4 pandas==2.2.1 fsspec==2024.5.0 requests==2.31.0 google-auth==2.43.0\n",
        "\n",
        "# Torch + CUDA 12.1\n",
        "pip install -q --no-cache-dir --force-reinstall   torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121   --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# bitsandbytes + triton + HF stack\n",
        "pip install -q --no-cache-dir --force-reinstall   bitsandbytes==0.43.3 triton==2.3.1   transformers==4.44.2 accelerate==0.33.0 peft==0.17.0 trl==0.9.6 datasets==2.20.0\n",
        "\n",
        "echo \"Setup complete. Restart runtime once, then run the rest of the notebook top-to-bottom.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "133b0b11",
      "metadata": {},
      "source": [
        "Model load: HF 4-bit NF4 + BitsAndBytes; deterministic decoding. If adapters exist, we load them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47936fc5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0) Clone repo (Colab) + install deps\n",
        "import os\n",
        "try:\n",
        "    import google.colab  # noqa: F401\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    if not os.path.exists('/content/NLtoSQL'):\n",
        "        !git clone https://github.com/MacKenzieOBrian/NLtoSQL.git /content/NLtoSQL\n",
        "    %cd /content/NLtoSQL\n",
        "    !pip -q install -r requirements.txt\n",
        "    import torch, transformers, accelerate, peft\n",
        "    print('torch', torch.__version__, 'cuda', torch.cuda.is_available())\n",
        "else:\n",
        "    print('Not in Colab; using existing workspace')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a24721f1",
      "metadata": {},
      "source": [
        "Prompt/eval: build prompts (system+schema+k exemplars), generate SQL, postprocess, and compute VA/EX/EM."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b5b7e43",
      "metadata": {},
      "source": [
        "**Ref:** Colab clone/install pattern; keeps notebooks thin and code in `nl2sql/`. Hugging Face/Colab standard workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20974344",
      "metadata": {},
      "source": [
        "### Reference notes (what this builds on)\n",
        "- DB access: Cloud SQL Connector + SQLAlchemy creator (GCP docs: https://cloud.google.com/sql/docs/mysql/connect-run) for secure pooled ClassicModels access.\n",
        "- Schema/prompting: uses repo helpers (`nl2sql.schema`, `prompting`) aligned with schema-grounded NL→SQL prompting (survey: https://arxiv.org/abs/2410.06011).\n",
        "- Model load: HF Transformers 4-bit NF4 with BitsAndBytes (quantization docs: https://huggingface.co/docs/transformers/main_classes/quantization), same pattern as QLoRA.\n",
        "- Agent loop: ReAct-style Thought→Action→Observation→Refinement, inspired by Yao et al. 2023 (https://arxiv.org/abs/2210.03629) and agentic NL→SQL in Ojuri et al. 2025.\n",
        "- Eval: repo harness (`nl2sql.eval`, `QueryRunner`) for VA/EX/EM; TS planned.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f75fce",
      "metadata": {},
      "source": [
        "## Optional: use gcloud ADC (without a key)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "523ff993",
      "metadata": {},
      "source": [
        "**Ref:** GCP ADC flow (docs: https://cloud.google.com/docs/authentication/provide-credentials-adc). Optional fallback if no service account JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af27263f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this only if you prefer gcloud-based ADC (no JSON key)\n",
        "try:\n",
        "    import google.colab  # noqa: F401\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    %pip install -q --upgrade google-auth google-auth-oauthlib\n",
        "    !gcloud auth application-default login\n",
        "else:\n",
        "    print(\"Not in Colab; skip gcloud auth.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21847187",
      "metadata": {},
      "source": [
        "**Ref:** Pinned CUDA12.1 torch/bitsandbytes/triton stack per HF/BnB guidance for 4-bit loads on Colab GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5528bc3a",
      "metadata": {},
      "source": [
        "**Ref:** Cloud SQL Connector + SQLAlchemy creator (GCP MySQL docs: https://cloud.google.com/sql/docs/mysql/connect-run) for secure ClassicModels access."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e745f2",
      "metadata": {},
      "source": [
        "**Docs (auth/DB):** Cloud SQL connector pattern https://cloud.google.com/sql/docs/mysql/connect-run; SQLAlchemy creator hook https://docs.sqlalchemy.org/en/20/core/engines.html#custom-dbapi-connect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e8d87f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 1) Environment + DB\n",
        "import os\n",
        "from getpass import getpass\n",
        "from pathlib import Path\n",
        "\n",
        "from google.cloud.sql.connector import Connector\n",
        "from google.oauth2.service_account import Credentials\n",
        "from sqlalchemy import create_engine, text\n",
        "from sqlalchemy.engine import Engine\n",
        "\n",
        "# Expected env vars (set these in a Colab cell):\n",
        "# GOOGLE_APPLICATION_CREDENTIALS=/content/sa.json\n",
        "# INSTANCE_CONNECTION_NAME, DB_USER, DB_PASS, DB_NAME\n",
        "INSTANCE_CONNECTION_NAME = os.getenv(\"INSTANCE_CONNECTION_NAME\")\n",
        "DB_USER = os.getenv(\"DB_USER\")\n",
        "DB_PASS = os.getenv(\"DB_PASS\")\n",
        "DB_NAME = os.getenv(\"DB_NAME\") or \"classicmodels\"\n",
        "GOOGLE_CREDS = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
        "\n",
        "if not INSTANCE_CONNECTION_NAME:\n",
        "    INSTANCE_CONNECTION_NAME = input(\"Enter INSTANCE_CONNECTION_NAME: \").strip()\n",
        "if not DB_USER:\n",
        "    DB_USER = input(\"Enter DB_USER: \").strip()\n",
        "if not DB_PASS:\n",
        "    DB_PASS = getpass(\"Enter DB_PASS: \")\n",
        "\n",
        "creds = None\n",
        "if GOOGLE_CREDS:\n",
        "    creds = Credentials.from_service_account_file(GOOGLE_CREDS)\n",
        "    print(f\"Using service account from {GOOGLE_CREDS}\")\n",
        "else:\n",
        "    print(\"Using default ADC (gcloud auth or Colab auth). If this fails, set GOOGLE_APPLICATION_CREDENTIALS.\")\n",
        "\n",
        "connector = Connector(credentials=creds)\n",
        "\n",
        "def getconn():\n",
        "    return connector.connect(\n",
        "        INSTANCE_CONNECTION_NAME,\n",
        "        \"pymysql\",\n",
        "        user=DB_USER,\n",
        "        password=DB_PASS,\n",
        "        db=DB_NAME,\n",
        "    )\n",
        "\n",
        "engine: Engine = create_engine(\n",
        "    \"mysql+pymysql://\",\n",
        "    creator=getconn,\n",
        "    future=True,\n",
        ")\n",
        "\n",
        "with engine.connect() as conn:\n",
        "    conn.execute(text(\"SELECT 1\"))\n",
        "print(\"DB connection OK\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a583e16",
      "metadata": {},
      "source": [
        "**Ref:** Schema helper in `nl2sql.schema`; schema-grounded prompting per NL→SQL survey (https://arxiv.org/abs/2410.06011)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7620ff8",
      "metadata": {},
      "source": [
        "**Docs (schema prompts):** NL→SQL schema-grounded prompting survey https://arxiv.org/abs/2410.06011; Spider-style listings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b1c88d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Load schema summary + test set (small slice for now)\n",
        "import json\n",
        "from nl2sql.schema import build_schema_summary\n",
        "\n",
        "SCHEMA_SUMMARY = build_schema_summary(engine, db_name=DB_NAME)\n",
        "\n",
        "test_path = Path(\"data/classicmodels_test_200.json\")\n",
        "full_set = json.loads(test_path.read_text(encoding=\"utf-8\"))\n",
        "# default to a small slice while debugging\n",
        "test_set = full_set[:5]\n",
        "print(\"Demo items:\", len(test_set))\n",
        "# For full run, switch to: test_set = full_set; print(\"Test items:\", len(test_set))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "700fe5c2",
      "metadata": {},
      "source": [
        "**Ref:** HF Transformers 4-bit NF4 + BitsAndBytes (quantization docs: https://huggingface.co/docs/transformers/main_classes/quantization); adapters via PEFT."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd92a2b8",
      "metadata": {},
      "source": [
        "**Docs (model load):** HF 4-bit NF4 quantization https://huggingface.co/docs/transformers/main_classes/quantization; PEFT/QLoRA https://huggingface.co/docs/peft/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4f38fab",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 3) Load model (base or QLoRA adapters)\n",
        "import os\n",
        "from getpass import getpass\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "ADAPTER_PATH = os.getenv(\"ADAPTER_PATH\") or \"results/adapters/qlora_classicmodels\"  # set to None to use base model\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        "if not HF_TOKEN:\n",
        "    HF_TOKEN = getpass(\"Enter HF_TOKEN (https://huggingface.co/settings/tokens): \").strip()\n",
        "\n",
        "cc_major, cc_minor = torch.cuda.get_device_capability(0) if torch.cuda.is_available() else (0, 0)\n",
        "use_bf16 = cc_major >= 8\n",
        "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
        "print(\"Using bf16:\", use_bf16)\n",
        "print(\"Adapter path:\", ADAPTER_PATH)\n",
        "\n",
        "# Tokenizer\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "# Quantized base model\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=compute_dtype,\n",
        "    device_map={\"\": 0} if torch.cuda.is_available() else None,\n",
        "    token=HF_TOKEN,\n",
        ")\n",
        "base_model.generation_config.do_sample = False\n",
        "base_model.generation_config.temperature = 1.0\n",
        "base_model.generation_config.top_p = 1.0\n",
        "\n",
        "# Load adapters if present locally; otherwise use base model\n",
        "adapter_dir = Path(ADAPTER_PATH) if ADAPTER_PATH else None\n",
        "if adapter_dir and adapter_dir.exists():\n",
        "    model = PeftModel.from_pretrained(base_model, adapter_dir, token=HF_TOKEN)\n",
        "    print(\"Loaded adapters from\", adapter_dir)\n",
        "else:\n",
        "    print(\"Adapter path missing; using base model only. Set ADAPTER_PATH to your local adapter folder or upload it to Colab.\")\n",
        "    model = base_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3afb59ed",
      "metadata": {},
      "source": [
        "## Optional adapter sanity check (run before ReAct)\n",
        "Quick check to see if the loaded model/adapters produce valid SQL on a tiny slice. Uses the prompt harness (k=0/k=3) and executes the SQL to report VA/EX."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db24b08d",
      "metadata": {},
      "source": [
        "**Docs (prompt/eval):** ICL patterns https://arxiv.org/abs/2005.14165; execution-based metrics (VA/EX) https://aclanthology.org/2020.emnlp-main.29/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1fbae86",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nl2sql.prompting import make_few_shot_messages\n",
        "from nl2sql.llm import extract_first_select\n",
        "from nl2sql.postprocess import guarded_postprocess\n",
        "from nl2sql.query_runner import QueryRunner\n",
        "from nl2sql.eval import execution_accuracy\n",
        "\n",
        "runner_check = QueryRunner(engine)\n",
        "# reuse existing test_set (default small slice); pick 3 exemplars\n",
        "exemplars = test_set[:3]\n",
        "\n",
        "def run_quick_check(k: int = 0, limit: int = 3):\n",
        "    print(f\"Quick check k={k}\")\n",
        "    for sample in test_set[:limit]:\n",
        "        shots = exemplars if k > 0 else []\n",
        "        msgs = make_few_shot_messages(\n",
        "            schema=SCHEMA_SUMMARY,\n",
        "            exemplars=shots,\n",
        "            nlq=sample['nlq'],\n",
        "        )\n",
        "        prompt_preview = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
        "        inputs = tok(prompt_preview, return_tensors=\"pt\").to(model.device)\n",
        "        out = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
        "\n",
        "        # strip the prompt before decoding the generation\n",
        "        gen_ids = out[0][inputs.input_ids.shape[-1]:]\n",
        "        text = tok.decode(gen_ids, skip_special_tokens=True)\n",
        "\n",
        "        raw_sql = extract_first_select(text) or text\n",
        "        sql = guarded_postprocess(raw_sql, sample['nlq'])\n",
        "\n",
        "        meta = runner_check.run(sql, capture_df=False)\n",
        "        va = meta.success\n",
        "        ex_ok, _, _ = execution_accuracy(engine=engine, pred_sql=sql, gold_sql=sample['sql'])\n",
        "        print(f\"Q: {sample['nlq']}\n",
        "SQL: {sql}\n",
        "VA: {va} EX: {ex_ok}\n",
        "\")\n",
        "\n",
        "run_quick_check(k=0)\n",
        "run_quick_check(k=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34d3d737",
      "metadata": {},
      "source": [
        "**Ref:** ReAct pattern (Yao et al. 2023: https://arxiv.org/abs/2210.03629) adapted for NL→SQL with `QueryRunner` as the Act step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6d2d11f",
      "metadata": {},
      "source": [
        "**Docs (ReAct):** ReAct loop (Yao et al. 2023) https://arxiv.org/abs/2210.03629; safe Act via SELECT-only executor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d84ff33f",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Helper imports (ReAct helpers live in nl2sql/agent_utils)\n",
        "from nl2sql.agent_utils import (\n",
        "    clean_candidate,\n",
        "    build_tabular_prompt,\n",
        "    vanilla_candidate,\n",
        "    classify_error,\n",
        "    error_hint,\n",
        "    semantic_score,\n",
        "    count_select_columns,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac12da1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ReAct helper using shared agent_utils (semantic rerank + repair)\n",
        "import torch\n",
        "from nl2sql.postprocess import guarded_postprocess\n",
        "\n",
        "def build_react_prompt(nlq: str, schema_text: str, history: list[dict], observation: str) -> str:\n",
        "    history_text = \"\\n\".join(\n",
        "        f\"Thought/Action: {h.get('ta','')}\\nObservation: {h.get('obs','')}\" for h in history\n",
        "    ) or \"None.\"\n",
        "    return f\"\"\"\n",
        "You are an expert MySQL analyst.\n",
        "\n",
        "TASK:\n",
        "- Output ONE and ONLY ONE valid MySQL SELECT statement.\n",
        "- Do NOT explain or comment.\n",
        "- The output MUST start with SELECT.\n",
        "- If unsure, still output your best single SELECT.\n",
        "\n",
        "Schema:\n",
        "{schema_text}\n",
        "\n",
        "Question: {nlq}\n",
        "\n",
        "Previous trace:\n",
        "{history_text}\n",
        "Last observation: {observation}\n",
        "\n",
        "Return only the final SELECT statement.\n",
        "\"\"\"\n",
        "\n",
        "def _generate_candidates(prompt: str, model, tok, num: int = 2, do_sample: bool = True):\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    gen_kwargs = dict(\n",
        "        max_new_tokens=192,\n",
        "        do_sample=do_sample,\n",
        "        temperature=0.5,\n",
        "        top_p=0.9,\n",
        "        num_return_sequences=num,\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, **gen_kwargs)\n",
        "    cands = []\n",
        "    for i in range(num):\n",
        "        gen_ids = out[i][inputs.input_ids.shape[-1]:]\n",
        "        gen = tok.decode(gen_ids, skip_special_tokens=True)\n",
        "        cands.append(gen)\n",
        "    return cands\n",
        "\n",
        "\n",
        "def react_sql(\n",
        "    nlq: str,\n",
        "    schema_text: str,\n",
        "    model,\n",
        "    tok,\n",
        "    runner,\n",
        "    max_steps: int = 3,\n",
        "    num_cands: int = 4,\n",
        "    exemplars: list[dict] | None = None,\n",
        "):\n",
        "    history = []\n",
        "    observation = \"Start.\"\n",
        "    best_sql = None\n",
        "    best_score = float(\"-inf\")\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        raw_cands = []\n",
        "        raw_cands += _generate_candidates(\n",
        "            build_react_prompt(nlq, schema_text, history, observation),\n",
        "            model,\n",
        "            tok,\n",
        "            num=max(1, num_cands // 2),\n",
        "            do_sample=True,\n",
        "        )\n",
        "        raw_cands += _generate_candidates(\n",
        "            build_tabular_prompt(nlq, schema_text),\n",
        "            model,\n",
        "            tok,\n",
        "            num=num_cands - len(raw_cands),\n",
        "            do_sample=True,\n",
        "        )\n",
        "\n",
        "        ranked = []\n",
        "        for raw in raw_cands:\n",
        "            sql = clean_candidate(raw)\n",
        "            if not sql:\n",
        "                history.append({\"ta\": raw, \"obs\": \"Rejected: not a pure SELECT\"})\n",
        "                continue\n",
        "            sql = guarded_postprocess(sql, nlq)\n",
        "            ranked.append(sql)\n",
        "\n",
        "        step_success = False\n",
        "        last_error = None\n",
        "\n",
        "        for sql in ranked:\n",
        "            try:\n",
        "                meta = runner.run(sql)\n",
        "                if not meta.success:\n",
        "                    raise ValueError(meta.error or \"exec failed\")\n",
        "                score = semantic_score(nlq, sql) - 0.2 * count_select_columns(sql)\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_sql = sql\n",
        "                history.append({\"ta\": sql, \"obs\": \"SUCCESS\"})\n",
        "                step_success = True\n",
        "            except Exception as e:\n",
        "                last_error = str(e)\n",
        "                kind = classify_error(last_error)\n",
        "                history.append({\"ta\": sql, \"obs\": f\"ERROR ({kind}): {last_error}\"})\n",
        "                hint = error_hint(kind, last_error)\n",
        "                repair_prompt = build_react_prompt(\n",
        "                    nlq,\n",
        "                    schema_text,\n",
        "                    history,\n",
        "                    f\"{last_error}. {hint}\",\n",
        "                )\n",
        "                repair_raw = _generate_candidates(repair_prompt, model, tok, num=1, do_sample=True)[0]\n",
        "                repaired = clean_candidate(repair_raw)\n",
        "                if repaired:\n",
        "                    try:\n",
        "                        meta2 = runner.run(repaired)\n",
        "                        if meta2.success:\n",
        "                            score = semantic_score(nlq, repaired) - 0.2 * count_select_columns(repaired)\n",
        "                            if score > best_score:\n",
        "                                best_score = score\n",
        "                                best_sql = repaired\n",
        "                            history.append({\"ta\": repaired, \"obs\": \"SUCCESS (repair)\"})\n",
        "                            step_success = True\n",
        "                    except Exception as e2:\n",
        "                        history.append({\"ta\": repaired, \"obs\": f\"Repair ERROR: {e2}\"})\n",
        "\n",
        "        if step_success and best_sql:\n",
        "            observation = \"SUCCESS\"\n",
        "            break\n",
        "        else:\n",
        "            observation = f\"ERROR: {last_error or 'all candidates failed'}\"\n",
        "\n",
        "    if best_sql is None:\n",
        "        fallback = vanilla_candidate(nlq, schema_text, tok, model, exemplars=exemplars or [])\n",
        "        if fallback:\n",
        "            best_sql = fallback\n",
        "            best_score = semantic_score(nlq, best_sql)\n",
        "            history.append({\"ta\": \"fallback:few-shot\", \"obs\": \"USED\"})\n",
        "\n",
        "    return best_sql, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agent status (for dissertation)\n",
        "Current loop = execution-guided reranker: sampled candidates, SELECT-only filter, semantic rerank, error-classified repair, deterministic few-shot fallback.\n",
        "Not yet full ReAct: we don’t enforce structured `Thought / Action: SCHEMA_LOOKUP[...] / Action: EXEC_SQL[...] / Observation: ... / FINISH[...]`, so the model isn’t forced to read and react to its own Observations.\n",
        "Planned upgrade (if time permits): add an explicit tool grammar and feed Observations back into the prompt so the model can revise after execution errors (Yao et al., 2023).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f13e02a",
      "metadata": {},
      "source": [
        "**Ref:** Repo eval (`nl2sql.eval`) for VA/EX/EM; execution-based metrics align with Ojuri et al. 2025 and EMNLP’20 TS."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f52889a3",
      "metadata": {},
      "source": [
        "**Docs (prompt/eval):** ICL patterns https://arxiv.org/abs/2005.14165; execution-based metrics (VA/EX) https://aclanthology.org/2020.emnlp-main.29/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ae5eeec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Evaluation loop (VA/EX/EM). TS is planned.\n",
        "from pathlib import Path\n",
        "import json\n",
        "from nl2sql.eval import execution_accuracy\n",
        "\n",
        "LIMIT = None  # set to e.g. 20 for a quick slice\n",
        "results = []\n",
        "items = test_set[:LIMIT] if LIMIT else test_set\n",
        "\n",
        "for i, sample in enumerate(items, start=1):\n",
        "    nlq = sample[\"nlq\"]\n",
        "    gold_sql = sample[\"sql\"]\n",
        "\n",
        "    pred_sql, trace = react_sql(nlq, SCHEMA_SUMMARY, max_steps=3, num_cands=2)\n",
        "\n",
        "    em = int(pred_sql.strip().rstrip(\";\").lower() == gold_sql.strip().rstrip(\";\").lower())\n",
        "    va = 0\n",
        "    ex = 0\n",
        "    try:\n",
        "        meta = runner.run(pred_sql)\n",
        "        va = int(meta.success)\n",
        "        if meta.success:\n",
        "            ex_ok, _, _ = execution_accuracy(engine=engine, pred_sql=pred_sql, gold_sql=gold_sql)\n",
        "            ex = int(ex_ok)\n",
        "    except Exception:\n",
        "        va = 0\n",
        "        ex = 0\n",
        "\n",
        "    results.append({\n",
        "        \"nlq\": nlq,\n",
        "        \"gold_sql\": gold_sql,\n",
        "        \"pred_sql\": pred_sql,\n",
        "        \"va\": va,\n",
        "        \"em\": em,\n",
        "        \"ex\": ex,\n",
        "        \"trace\": trace,\n",
        "    })\n",
        "    if i % 5 == 0:\n",
        "        print(f\"Processed {i}/{len(items)}\")\n",
        "\n",
        "va_rate = sum(r[\"va\"] for r in results) / len(results)\n",
        "ex_rate = sum(r[\"ex\"] for r in results) / len(results)\n",
        "em_rate = sum(r[\"em\"] for r in results) / len(results)\n",
        "print(\"VA:\", va_rate, \"EX:\", ex_rate, \"EM:\", em_rate)\n",
        "\n",
        "Path(\"results/agent\").mkdir(parents=True, exist_ok=True)\n",
        "save_path = Path(\"results/agent/results_react_200.json\")\n",
        "save_path.write_text(json.dumps({\n",
        "    \"va_rate\": va_rate,\n",
        "    \"ex_rate\": ex_rate,\n",
        "    \"em_rate\": em_rate,\n",
        "    \"items\": results,\n",
        "}, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "print(\"Saved to\", save_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
# Methodology (Open-Source Replica of Ojuri et al., 2025)

This project reproduces the methodology and evaluation setup of Ojuri et al. (2025)—model adaptation, execution-feedback refinement, and execution-based evaluation—but using only open-source LLMs and Colab-class hardware. This enables a controlled comparison between proprietary NL→SQL pipelines and resource-feasible open-source alternatives.

---

# 1. Problem Framing

The task is to convert natural language queries into SQL queries executable on the ClassicModels MySQL database. This enables non-technical business users to query relational data without learning SQL syntax or database schemas.

Unlike text classification or summarisation, NL→SQL is a structured generation task with high sensitivity to schema grounding, SQL syntax, and relational correctness. Errors typically cluster into:

- syntactic errors (invalid SQL)
- semantic errors (wrong joins/filters)
- logical errors (aggregation mistakes)
- schema errors (wrong columns/tables)

The methodology is designed to evaluate how different adaptation strategies address these error modes.

---

# 2. Experimental Pipeline (End-to-End)

The experimental pipeline consists of the following checkpoints:

1. **User NL Query → Schema Awareness**
2. **LLM Strategy → SQL Candidate Generation**
3. **SQL → DB Execution**
4. **Result Comparison → Metrics**
5. **(Optional) Agentic Refinement Loop**
6. **Aggregate Evaluation**

The pipeline is implemented via:

- model inference scaffolding (`nl2sql/`)
- SQL execution engine (`QueryRunner`)
- evaluation modules (`evaluator.py`)
- notebooks for each strategy (`02/03/05_*.ipynb`)

This modularity ensures each strategy can be swapped without altering evaluation logic, improving comparability and reproducibility.

## Architecture snapshot (what lives where)
- `nl2sql/`: stable harness (schema access, prompting, safety/postprocess, execution via `QueryRunner`, metrics in `eval.py`).
- `notebooks/`: orchestration per strategy (`02` prompt baselines, planned `03` agentic eval, `05` QLoRA train/eval).
- Flow: NLQ → schema text → strategy (prompt/QLoRA/agent) → SQL candidate → safety filter → DB execution → metrics (VA/EX/EM; TS planned).
- Safety: SELECT-only, read-only execution; deterministic decoding for eval; leakage checks on train/test/exemplars.

---

# 3. Domain and Data

- Domain: **ClassicModels (MySQL)**
- Benchmarks: **200 held-out NL→SQL test examples**
- Training set: **~200 supervised examples (JSONL)**

**Rationale:** ClassicModels provides realistic business workloads involving joins, aggregates, and ordering. It also matches the domain used in Ojuri et al. (2025), enabling like-for-like reproduction against GPT-4 agent baselines.

**Dataset Reflection (from notebooks):**  
Zero-shot prompting struggled with join-heavy queries, reinforcing the importance of supervising schema patterns rather than relying solely on ICL.

---

# 4. Model Adaptation Strategies

Three adaptation families are evaluated:

| Strategy | Modifies Weights | Training | Iterative | Expected Consistency |
|---|---|---|---|---|
| Zero-Shot ICL | ✗ | ✗ | ✗ | Low |
| Few-Shot ICL | ✗ | ✗ | ✗ | Moderate |
| QLoRA FT | ✓ (Adapters) | ✓ | ✗ | High |
| ReAct / ExCoT | Optional | ✗ | ✓ | Highest |

This mirrors the structure in Ojuri et al. (2025), who compared GPT-4 prompting vs GPT-4 fine-tuned vs GPT-4 agent.

---

# 5. Training Configuration (Current Best Practice)

- Base model: **Llama-3-8B-Instruct**
- Quantization: **4-bit NF4**
- PEFT: **LoRA adapters only**
- Optimizer: `paged_adamw_8bit`
- VRAM target: **≤ 10GB (Colab T4)**
- Dtype: **bf16 on Ampere, fp16 fallback**
- Hyperparameters:
  - r = 32
  - α = 64
  - dropout = 0.05
  - LR ~ 1e-4
  - epochs ≈ 3
  - batch size = 1 w/ gradient accumulation = 8

This configuration allows fine-tuning without full model gradient updates, avoiding >60GB VRAM requirements of full FT.

---

# 6. Agentic Refinement (Planned)

The methodology includes planned integration of an execution-feedback agent based on:

- **ReAct** (Thought → Action → Observation) (Yao et al., 2023)
- **ExCoT** (execution traces) (Zhai et al., 2025)

Agentic refinement operationalises the hypothesis that:

1. prompting → generates a hypothesis
2. DB execution → tests the hypothesis
3. refinement → corrects structured errors

Ojuri et al. (2025) observed significant EX gains when including this loop.

---

# 7. Evaluation Criteria (Aligned with Ojuri et al., 2025)

Evaluation adopts the same semantic metrics:

| Metric | Target | Description |
|---|---|---|
| VA | Syntax | SQL executes without syntax error |
| EX | Semantics | Result-set matches expected output |
| EM | Surface | String match (strict baseline) |
| TS | Semantics+ | Perturbed DB test-suite accuracy |

Execution-based metrics (VA/EX/TS) are considered more meaningful for NL→SQL than surface metrics such as BLEU or exact string match.

---

# 8. Hardware and Compute Constraints

- **Training hardware:** Google Colab (T4, occasional L4/A100 availability)
- **VRAM budget:** 8–12GB
- **Inference:** consumer-grade GPU feasible

Constraint reflection (from QLoRA notebook):  
Training peaked at ~9.3GB VRAM—confirming PEFT viability under constrained compute conditions.

These constraints shape methodology; they are not incidental.

---

# 9. Comparative Method Justification

### Why not full fine-tuning?
Requires >60GB VRAM; infeasible for open-source reproduction.

### Why not prompting-only?
Few-shot ICL exhibited instability and schema sensitivity, consistent with Mosbach et al. (2023).

### Why agent loops?
Execution feedback reduces logical/aggregation/join errors beyond static decoding (Yao et al., 2023; Ojuri et al., 2025).

### Why ClassicModels?
Matches proprietary baseline domain and workloads; avoids distributional confounds.

### Implementation influences (provenance)
The implementation drew on three pillars: (1) NL→SQL literature for schema grounding, refinement, and execution-based evaluation; (2) open-source ecosystems (HF Transformers/PEFT, SQLAlchemy, Cloud SQL connectors) for idiomatic model loading, fine-tuning, and DB access; and (3) official framework docs for API-level details on inference, quantization, and execution. The orchestration here is original to this reproduction, but follows standard applied NLP practice of combining established tools to answer new research questions.

### Agent (ReAct/ExCoT) plan
- Pattern: Thought → Action (run SQL) → Observation (success/error) → Refinement; iterate a few steps.
- Implementation: new `03_agentic_eval.ipynb` will load the base or QLoRA-adapter model, build a ReAct loop around `QueryRunner`, and log traces.
- Justification: literature shows execution-feedback loops boost semantic accuracy beyond single-pass decoding [16], [2]; our runs still show low k=0 EX, so an agent is the next lever.

---

# 10. Reproducibility Measures

To ensure methodological transparency:

- hardware config logged
- software versions pinned
- test set frozen
- adapters checkpointed
- training seeds fixed
- evaluation deterministic
- no proprietary API calls

This aligns with reproducibility expectations in ML systems research.

---

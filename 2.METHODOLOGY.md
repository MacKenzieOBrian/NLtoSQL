# Methodology (Open-Source Replica)

This section describes the methodology for reproducing and extending the NL→SQL agent pipeline proposed by Ojuri et al. [10] using **open-source models**, **parameter-efficient adapters**, and **execution-guided agentic refinement** under Colab-class GPU constraints. The design is framed as a controlled comparison across three adaptation strategies: prompting only, QLoRA adapters, and execution-guided agents.

---

## 1. Literature-Backed Decision Journey

Each methodological step follows directly from the State of the Art:

**(1) Closed-schema evaluation on ClassicModels (small split)**  
ClassicModels is widely used in instructional DB settings and in Ojuri et al. [10] as a realistic business schema involving joins, aggregates and ranking. Using a **single, fixed schema** mirrors controlled evaluation in Spider [19] and LLM-based DB interfaces [1], [8], [9]. The objective is not cross-schema generalisation but **semantic correctness within a known schema**, enabling comparison with GPT-4 agents in [10].

**(2) Prompt-only baseline (zero-shot and few-shot ICL)**  
In-context learning (ICL) [6] is a standard baseline for adapting LLMs without weight updates and is widely used in recent text-to-SQL studies [1], [8], [9], [20]. However, Mosbach et al. [3] show that ICL is unstable and underperforms supervised fine-tuning on domain tasks. Including zero-shot and few-shot prompting allows us to ask: *how far can we get with prompting alone on ClassicModels?*

**(3) QLoRA adapters for domain alignment under VRAM constraints**  
Parameter-efficient fine-tuning (PEFT) is recommended when full fine-tuning is impractical [12]. Ding et al. [12] and practitioner guides [4], [5] show that QLoRA enables fine-tuning 7–13B models using ~8–12 GB VRAM via 4-bit quantisation plus LoRA adapters. Ojuri et al. [10] report that small open models fine-tuned on ClassicModels reduce the gap to GPT-4. This motivates QLoRA for encoding **schema semantics** while respecting **Colab-class hardware**.

**(4) Execution-guided refinement and agentic reasoning**  
Execution-guided decoding improves both syntactic validity and semantic correctness in NL→SQL [18]. ReAct-style agents [16] and ExCoT [2] further show that interleaving SQL generation with database execution improves join and aggregation reasoning. Ojuri et al. [10] attribute part of their ClassicModels performance to execution feedback in an agent loop. This project therefore adds an execution-guided refinement layer on top of prompting and QLoRA.

**(5) Execution-centric metrics (VA/EX/TS) over string match**  
String match is known to penalise logically equivalent SQL [18], [19]. Modern benchmarks recommend execution accuracy (EX) and Test-Suite variants (TS) [1], [18], [19]. In line with [10], this project adopts **VA/EX** as primary metrics, treating the database as the semantic arbiter.

---

## 2. Pipeline Decomposition and Control

To avoid confounding adaptation with schema handling or evaluation, the NL→SQL pipeline is decomposed into five stable components:

1. **NLQ ingestion** — raw question.  
2. **Schema-grounded prompt construction** — system instruction + ClassicModels schema summary.  
3. **Adaptation strategy** — one of:
   - prompt-only (ICL)
   - QLoRA adapters
   - execution-guided agent  
4. **SQL post-processing and safety guards** — first-SELECT extraction + mild structural clamps.  
5. **Execution and scoring** — live DB execution → VA/EX/EM (TS planned).  

All three adaptation strategies share the same schema text, prompt format, DB execution engine, and evaluation harness. Only the adaptation mechanism changes, isolating its effect.

---

## 3. Controlled Prompt and Decoding Settings

Prompting variables are held constant following [3], [6], [8], [14]:

| Variable | Setting |
|---|---|
| System instruction | fixed (MySQL, single-SELECT contract) |
| Schema summary | fixed, ClassicModels-specific |
| Exemplars (k) | {0, 3} |
| Decoding | deterministic for baselines |
| NLQ text | unmodified natural language |
| Postprocess | guarded first-SELECT extraction |

This ensures performance differences originate from **weights (QLoRA)** or **control logic (agent)** rather than prompt variance.

---

## 4. Hypotheses and Adaptation Strategies

The study is framed as a comparison across three adaptation strategies motivated by [3], [8], [10], [12], [16], [18]:

**H1 — Prompt-only recovers syntax but struggles with semantics.**  
Expected: executable SQL (VA↑) but low EX on joins/aggregates.

**H2 — QLoRA adapters improve semantic mappings.**  
Expected: higher EX via internalisation of schema semantics (joins, revenue via `orderdetails × priceEach`, grouping).

**H3 — Execution-guided agency improves semantic correctness further.**  
Expected: EX↑ via iterative self-correction from DB feedback, without weight updates.

These map onto the strategies:

| Strategy | Weights Updated? | Tool Feedback? | Grounded In |
|---|---|---|---|
| Prompt-only (ICL) | ✗ | ✗ | [1], [3], [6], [8], [20] |
| QLoRA adapters | ✓ (adapters) | ✗ | [4], [5], [10], [12] |
| Agentic refinement | ✗ | ✓ | [2], [10], [16], [18], [21] |

---

## 5. Data and Evaluation Substrate

- **Domain:** ClassicModels (MySQL)
- **Training:** ~200 NL→SQL supervised pairs (for QLoRA only)
- **Test set:** 200 held-out queries
- **DB:** live execution on ClassicModels

Evaluation metrics:

- **VA — Validity Accuracy:** executes without error  
- **EX — Execution Accuracy:** matches gold result set  
- **EM — Exact Match:** string-equality baseline (reported but de-emphasised)  
- **TS — Test-Suite (planned):** perturbation-based robustness [18]

This aligns with execution-centric evaluation used in [1], [10], [18], [19].

---

## 6. Model Adaptation and Agent Design

### 6.1 QLoRA Configuration

Following [4], [5], [12]:

- Base: Llama-3-8B-Instruct
- Quantisation: 4-bit NF4 + 16-bit compute
- PEFT: LoRA adapters on attention/projection layers
- VRAM target: ≤ 10 GB (Colab T4)
- Expected effect: encode ClassicModels schema semantics

### 6.2 Execution-Guided Agent (ReAct-Inspired)

The implemented agent is a **minimal execution-guided reranker**, inspired by ReAct [16] and ExCoT [2]:

1. **Candidate generation**  
   Multi-candidate SQL sampling via standard + tabular prompts, encouraging table/join reasoning [8], [9].

2. **Filtering & clamping**  
   `clean_candidate` enforces a **single-SELECT contract**; projection and ORDER/LIMIT clamping follow constrained decoding ideas from PICARD [13].

3. **Execution gating**  
   MySQL execution discards syntactically invalid SQL (VA=0), consistent with execution-guided decoding [18].

4. **Error-aware repair**  
   MySQL errors mapped to hints (unknown column/table, ambiguous column, GROUP BY), analogous to ExCoT’s execution trace use [2].

5. **Semantic reranking**  
   Lightweight lexical intent score rewards aggregates/joins implied by NLQ; relates to critic-based reranking in [1], [8], [20].

6. **Deterministic fallback**  
   Few-shot ICL baseline ensures the agent does not underperform a static prompt, mirroring self-consistency baselines [6], [8].

This layer probes whether **execution feedback alone** (no weight updates) can lift EX beyond QLoRA.

**Largest stability gain (output control + semantic acceptance):**  
The most impactful change was **controlling generation termination** (stop at `;` / prompt‑echo trimming) and adding a **semantic acceptance gate** (using `semantic_score` as a threshold, not only for reranking). This directly addresses two documented failure modes: (i) prompt‑echo junk corrupting otherwise valid SQL, and (ii) “valid‑but‑irrelevant” repairs (e.g., trivial executable SQL that does not answer the question). These controls align with constrained decoding (PICARD‑style) and critic‑based reranking literature [13], and reinforce the VA vs EX distinction emphasized in execution‑guided decoding [18].

---

## 7. Methodological Reflection

The staged methodology — **prompt → QLoRA → execution-guided agent** — follows progression in [3], [8], [10], [12], [16], [18]:

- **Prompt-only:** tests schema grounding without supervision.  
- **QLoRA:** tests whether adapters internalise schema semantics under realistic VRAM constraints.  
- **Execution-guided agency:** tests whether VA-heavy but EX-weak outputs can be corrected through DB feedback without touching weights.

Reporting VA/EX/EM for each stage decomposes errors into syntax, schema grounding, and semantic reasoning categories, aligning with NL→SQL taxonomies from [1], [8], [18], [19], and enabling comparison to GPT-4 agents in [10].

---

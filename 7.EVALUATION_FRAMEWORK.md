# Evaluation Framework

This project reproduces the evaluation methodology of Ojuri et al. (2025) to compare proprietary NL→SQL agents with open-source alternatives under execution-centric metrics. Evaluation focuses on correctness, semantic robustness, and compute feasibility rather than surface text similarity.

---

## 1. Evaluation Metrics

The system measures correctness along three dimensions:

---

### 1.1 Valid SQL (VA)

**Definition**  
Checks whether generated SQL is syntactically valid and executable by the database engine.

**Measurement**  
Binary: `1` if query executes without syntax errors.

**Rationale**  
Single-shot decoding frequently yields malformed SQL. VA isolates syntactic errors from semantic ones.

**Literature**  
Execution-based syntax validation is established in NL→SQL evaluation (Zhong et al., 2020).

---

### 1.2 Execution Accuracy (EX)

**Definition**  
Determines whether the executed SQL returns the same result-set as the ground truth on the evaluation database.

**Measurement**  
For each query:  
`EX = 1` if result-sets match exactly; else `0`.

**Rationale**  
Execution accuracy captures functional correctness; string-equivalent SQL is not required.

**Literature**  
Execution accuracy is the primary metric in Spider and related work (Yu et al., 2018).

---

### 1.3 Test-Suite Accuracy (TS)

**Definition**  
Evaluates logical correctness under perturbed database states.

**Measurement**  
`TS = 1` only if outputs match across all perturbed DB replicas.

**Rationale**  
Prevents “accidental correctness” where a wrong query produces correct output on specific data.

**Literature**  
Introduced in Zhong et al. (2020) and adopted in Ojuri et al. (2025).

---

## 2. Dataset and Benchmark

Evaluation uses a 200-item NL→SQL test set over the ClassicModels database, matching the domain used in Ojuri et al. (2025) to enable like-for-like comparison.

The test set is disjoint from the fine-tuning dataset to avoid leakage.

---

## 3. Evaluation Pipeline (Implementation)

Evaluation proceeds as follows:

1. **Generate SQL** using selected strategy:
   - zero-shot prompting
   - few-shot prompting
   - QLoRA fine-tuning
   - (planned) ReAct agentic refinement

2. **Execute SQL** against the MySQL instance using `QueryRunner`

3. **Compare result-sets** against ground truth to compute:
   - VA
   - EX
   - EM (strict surface, included for baseline)
   - TS (planned)

4. **Aggregate metrics** across all 200 samples

Repositories:

- `nl2sql/eval.py` — evaluation utilities
- `notebooks/02_baseline_prompting_eval.ipynb`
- `notebooks/05_qlora_train_eval.ipynb`
- `notebooks/03_agentic_eval.ipynb` (planned)

---

## 4. Evaluation Code Pattern

A minimal evaluable loop is:

```python
results = []

for sample in test_set:
    sql = generate_sql_with_strategy(strategy, sample, schema_text)
    try:
        rows = query_runner.run(sql)
        va = 1
        ex = int(rows == sample["expected_rows"])
    except Exception:
        va = 0
        ex = 0
    
    results.append({"va": va, "ex": ex})

final = {
    "VA": mean(r["va"] for r in results),
    "EX": mean(r["ex"] for r in results),
}

# Motivating agentic evaluation
# Current QLoRA runs: adapter-only EX remains low (0.065 at k=0) despite high VA.
# ReAct/ExCoT-style execution feedback is expected to raise EX/TS by iteratively correcting semantic errors.


# Method and System Decisions

The core methodological rule in this project is to change one major factor at a time while holding the evaluation contract steady. The fixed benchmark is `data/classicmodels_test_200.json`, and the shared scoring path is implemented in `nl2sql/evaluation/eval.py` via the compatibility import `nl2sql/eval.py`. This allows baseline, QLoRA, and agentic runs to be compared without introducing hidden differences in scoring logic.

The baseline system uses prompt-based generation with controlled `k`-shot settings. The purpose of this stage is to establish a non-fine-tuned reference before any adaptation claims are made. The notebook `notebooks/02_baseline_prompting_eval.ipynb` orchestrates sweeps, but the core behavior relies on package modules such as `nl2sql/core/llm.py`, `nl2sql/core/postprocess.py`, and the evaluator functions in `nl2sql/evaluation/eval.py`.

The QLoRA stage uses parameter-efficient adaptation so the project remains feasible on constrained hardware. Training and evaluation are managed in `notebooks/05_qlora_train_eval.ipynb`, with adapter outputs stored under `results/adapters/` and run outputs under `results/qlora/`. The important design choice here is protocol matching. The adapted model is evaluated through the same evaluator pathway and benchmark context used for baseline runs, so the comparison remains method-fair.

The ReAct stage is implemented in `nl2sql/agent/react_pipeline.py` and `nl2sql/agent/agent_tools.py`. The loop is model-driven and bounded by explicit invariants, including repair and step budgets and a finish gate that requires successful execution state. This design balances freedom and safety. It preserves trace interpretability while preventing uncontrolled tool loops and unsafe SQL behavior.

Safety and semantic quality controls are layered, not single-point. SQL extraction and guardrails occur before execution. Runtime query execution is controlled by `nl2sql/core/query_runner.py` with read-only token checks and bounded fetch behavior. Additional validation in `nl2sql/core/validation.py` and intent checks in `nl2sql/agent/intent_rules.py` help separate syntactic executability from semantic relevance.

Reproducibility decisions are embedded in artifact handling. Runs store metadata, rates, and item-level outcomes in JSON. Analysis is regenerated by `scripts/generate_research_comparison.py` into deterministic tables and figures under `results/analysis/`. This means writing can reference stable output files rather than manually assembled summary notes.

These system decisions support a clear writing stance: the project is not a collection of disconnected notebooks, but a controlled experimental stack where design choices are visible in module boundaries and measurable in paired outcomes.

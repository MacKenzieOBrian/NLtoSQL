{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build a DB-validated training set (LLM-assisted)\n",
        "\n",
        "This notebook generates NLQâ†’SQL training pairs using an LLM, then **validates every SQL** against the live ClassicModels database (VA must be True) before saving.\n",
        "\n",
        "Why this exists:\n",
        "- QLoRA needs a **training** dataset that is **separate** from the 200-item benchmark (`data/classicmodels_test_200.json`).\n",
        "- If you generate data with an LLM, you must be strict: only keep examples that execute, are SELECT-only, and do not overlap the test NLQs.\n",
        "\n",
        "Output:\n",
        "- `data/train/classicmodels_train_200.jsonl` (JSON Lines: `{ \"nlq\": ..., \"sql\": ... }`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# If opened directly in Colab, clone the repo first\n",
        "if Path(\"data/classicmodels_test_200.json\").exists() is False and Path(\"/content\").exists():\n",
        "    repo_dir = Path(\"/content/NLtoSQL\")\n",
        "    if repo_dir.exists():\n",
        "        shutil.rmtree(repo_dir)\n",
        "    !git clone https://github.com/MacKenzieOBrian/NLtoSQL.git \"{repo_dir}\"\n",
        "    os.chdir(repo_dir)\n",
        "\n",
        "sys.path.insert(0, os.getcwd())\n",
        "print(\"cwd:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Install dependencies (Colab)\n",
        "\n",
        "Install pinned deps and restart runtime if Colab asks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab  # noqa: F401\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip -q install -r requirements.txt\n",
        "else:\n",
        "    print(\"Not in Colab; ensure requirements are installed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Auth (GCP + Hugging Face)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GCP auth\n",
        "try:\n",
        "    from google.colab import auth\n",
        "except ModuleNotFoundError:\n",
        "    auth = None\n",
        "if auth:\n",
        "    auth.authenticate_user()\n",
        "else:\n",
        "    print(\"Not running in Colab; ensure ADC/service account auth is configured.\")\n",
        "\n",
        "# Hugging Face auth\n",
        "hf_token = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
        "if hf_token:\n",
        "    os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = hf_token\n",
        "    print(\"Using HF token from env\")\n",
        "else:\n",
        "    try:\n",
        "        from huggingface_hub import notebook_login\n",
        "        notebook_login()\n",
        "    except Exception as e:\n",
        "        print(\"HF auth not configured:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Load test set (to avoid overlap)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "test_set = json.loads(open(\"data/classicmodels_test_200.json\", \"r\", encoding=\"utf-8\").read())\n",
        "test_nlqs = {x[\"nlq\"].strip() for x in test_set}\n",
        "print(\"Loaded test NLQs:\", len(test_nlqs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) DB engine + schema summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "from nl2sql.db import create_engine_with_connector\n",
        "from nl2sql.query_runner import QueryRunner\n",
        "from nl2sql.schema import build_schema_summary\n",
        "\n",
        "INSTANCE_CONNECTION_NAME = os.getenv(\"INSTANCE_CONNECTION_NAME\")\n",
        "DB_USER = os.getenv(\"DB_USER\")\n",
        "DB_PASS = os.getenv(\"DB_PASS\")\n",
        "DB_NAME = os.getenv(\"DB_NAME\", \"classicmodels\")\n",
        "\n",
        "if not INSTANCE_CONNECTION_NAME:\n",
        "    INSTANCE_CONNECTION_NAME = input(\"Enter INSTANCE_CONNECTION_NAME: \").strip()\n",
        "if not DB_USER:\n",
        "    DB_USER = input(\"Enter DB_USER: \").strip()\n",
        "if not DB_PASS:\n",
        "    DB_PASS = getpass(\"Enter DB_PASS: \")\n",
        "\n",
        "engine, connector = create_engine_with_connector(\n",
        "    instance_connection_name=INSTANCE_CONNECTION_NAME,\n",
        "    user=DB_USER,\n",
        "    password=DB_PASS,\n",
        "    db_name=DB_NAME,\n",
        ")\n",
        "\n",
        "SCHEMA_SUMMARY = build_schema_summary(engine, db_name=DB_NAME, max_cols_per_table=50)\n",
        "qr = QueryRunner(engine, max_rows=50)\n",
        "print(\"Schema summary length:\", len(SCHEMA_SUMMARY))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Load the LLM\n",
        "\n",
        "You can generate training pairs with the same base model used in baseline (or swap to a different generator model).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID, token=True)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    token=True,\n",
        ")\n",
        "\n",
        "print(\"Model loaded on:\", model.device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Generate + validate pairs\n",
        "\n",
        "Policy (strict):\n",
        "- Keep only **single SELECT** statements.\n",
        "- Must execute successfully (VA=True) on the live DB.\n",
        "- Discard if NLQ overlaps the benchmark NLQs.\n",
        "- Deduplicate by normalized SQL and NLQ.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "from nl2sql.llm import extract_first_select\n",
        "from nl2sql.postprocess import normalize_sql\n",
        "\n",
        "JSON_RE = re.compile(r\"\\{.*\\}\", re.DOTALL)\n",
        "\n",
        "SYSTEM = \"\"\"You create training data for NL-to-SQL over the ClassicModels MySQL database.\n",
        "Return ONE JSON object with exactly these keys: nlq, sql.\n",
        "\n",
        "Rules:\n",
        "- sql must be a single MySQL SELECT statement.\n",
        "- No comments, no markdown, no extra keys.\n",
        "- Use only tables/columns present in the schema.\n",
        "- Make the NLQ natural and specific.\n",
        "\"\"\"\n",
        "\n",
        "def propose_one(rng: random.Random) -> dict:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM},\n",
        "        {\"role\": \"user\", \"content\": \"Schema:\\n\" + SCHEMA_SUMMARY},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Generate a new, non-trivial training example (joins/aggregations/filters encouraged).\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    input_ids = tok.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=256,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tok.eos_token_id,\n",
        "        )\n",
        "\n",
        "    gen_ids = out[0][input_ids.shape[-1] :]\n",
        "    text = tok.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "    m = JSON_RE.search(text)\n",
        "    if not m:\n",
        "        raise ValueError(\"No JSON object found\")\n",
        "    obj = json.loads(m.group(0))\n",
        "    if set(obj.keys()) != {\"nlq\", \"sql\"}:\n",
        "        raise ValueError(f\"Bad keys: {sorted(obj.keys())}\")\n",
        "\n",
        "    nlq = str(obj[\"nlq\"]).strip()\n",
        "    sql = str(obj[\"sql\"]).strip()\n",
        "    sql = extract_first_select(sql) or extract_first_select(text) or sql\n",
        "    obj = {\"nlq\": nlq, \"sql\": sql}\n",
        "    return obj\n",
        "\n",
        "def is_acceptable(candidate: dict) -> tuple[bool, str | None]:\n",
        "    nlq = candidate.get(\"nlq\", \"\").strip()\n",
        "    sql = candidate.get(\"sql\", \"\").strip()\n",
        "\n",
        "    if not nlq or not sql:\n",
        "        return False, \"empty\"\n",
        "    if nlq in test_nlqs:\n",
        "        return False, \"nlq-overlaps-test\"\n",
        "\n",
        "    sql_norm = normalize_sql(sql)\n",
        "    if not sql_norm.startswith(\"select \"):\n",
        "        return False, \"not-select\"\n",
        "\n",
        "    meta = qr.run(sql, capture_df=False)\n",
        "    if not meta.success:\n",
        "        return False, f\"va-false: {meta.error}\"\n",
        "    return True, None\n",
        "\n",
        "TARGET_N = 200\n",
        "MAX_ATTEMPTS = 1200\n",
        "SEED = 7\n",
        "\n",
        "rng = random.Random(SEED)\n",
        "seen_sql = set()\n",
        "seen_nlq = set()\n",
        "accepted = []\n",
        "\n",
        "for attempt in range(1, MAX_ATTEMPTS + 1):\n",
        "    try:\n",
        "        cand = propose_one(rng)\n",
        "    except Exception as e:\n",
        "        if attempt % 50 == 0:\n",
        "            print(\"attempt\", attempt, \"parse-fail\", str(e)[:120])\n",
        "        continue\n",
        "\n",
        "    ok, reason = is_acceptable(cand)\n",
        "    if not ok:\n",
        "        if attempt % 50 == 0:\n",
        "            print(\"attempt\", attempt, \"reject\", reason)\n",
        "        continue\n",
        "\n",
        "    sql_key = normalize_sql(cand[\"sql\"]) \n",
        "    nlq_key = cand[\"nlq\"].strip().lower()\n",
        "    if sql_key in seen_sql or nlq_key in seen_nlq:\n",
        "        continue\n",
        "\n",
        "    seen_sql.add(sql_key)\n",
        "    seen_nlq.add(nlq_key)\n",
        "    accepted.append(cand)\n",
        "\n",
        "    if len(accepted) % 25 == 0:\n",
        "        print(\"accepted\", len(accepted), \"/\", TARGET_N)\n",
        "    if len(accepted) >= TARGET_N:\n",
        "        break\n",
        "\n",
        "print(\"Final accepted:\", len(accepted))\n",
        "accepted[:3]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Save to `data/train/`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "out_dir = Path(\"data/train\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "out_path = out_dir / \"classicmodels_train_200.jsonl\"\n",
        "\n",
        "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for item in accepted:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Wrote:\", out_path, \"lines:\", len(accepted))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}


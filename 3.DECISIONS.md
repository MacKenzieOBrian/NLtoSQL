# Decision Record

Why each choice was made, what we skipped, and where it lives. Pair this with `2.METHODOLOGY.md` for the story, `5.CONFIG.md` for env/runtime, and `8.RESULTS_SUMMARY.md` for outcomes.

---

## 1. Research Design & Experimental Framing

| Decision | Why (Justification) | Alternatives Rejected | Where |
|---|---|---|---|
| Replicate Ojuri et al. methodology using OSS | Enables fair comparison under open constraints | Evaluate GPT-4 directly → proprietary, non-reproducible | `STATE_OF_THE_ART.md`, `METHODOLOGY.md` |
| Comparative framing (prompting vs PEFT vs agent) | Avoids one-shot system claims; creates measurable uplift ladder | Only report final system → hides where improvements come from | `notebooks/` + dissertation Chapter 5 |
| ClassicModels as domain benchmark | Matches Ojuri; business queries; multi-table joins | Spider → domain mismatch; BIRD → compute heavy | `DATA.md` |

**Dissertation note:** This framing follows best practice for ablation-style comparisons in NL→SQL (e.g., [10], [18]) and keeps the study reproducible under open constraints.

---

## 2. Dataset & Evaluation Discipline

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| Fixed 200-item test benchmark | Enables repeated experiments with stable metrics | Dynamic or streaming evaluation → non-deterministic | `data/classicmodels_test_200.json` |
| Disjoint train/test | Prevents leakage and invalid generalisation claims | “Few-shot only” approach → hides training impact | `notebooks/04_build_training_set.ipynb` |
| Schema included in evaluation inputs | Required for column/table grounding | Retrieval-based schema → increases architectural confounds | `DATA.md`, `prompting.py` |

**Dissertation note:** Leakage and schema omission are known threats to validity in text-to-SQL; enforcing splits and schema context aligns with evaluation hygiene in [18].

---

## 3a. Prompt pipeline hygiene (baseline + QLoRA evals)

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| Prompt structure: system + schema + k exemplars + NLQ | Reduces schema errors; matches NL→SQL best practice | NLQ-only or schema-free prompts | `prompting.py`, `02_baseline_prompting_eval.ipynb` |
| Deterministic decoding (`do_sample=False`, bounded tokens) | Removes sampling variance from VA/EX reporting | Temperature/top-p sampling during eval | `llm.py` |
| Ordered schema (PK/name-first) | Lightweight bias toward correct columns | Unordered dump → more column confusion | `prompting.py` |
| Post-process: first `SELECT …;`, minimal projection | Strips chatter, avoids over-selection | Raw model output | `postprocess.py` |
| Exemplar leakage guard | Prevents test NLQ appearing as exemplar | No guard → inflated EX | `prompting.py` / eval loop |

**Reflection:** The baseline gains logged on 2025-12-14 came solely from this hygiene; no weights were changed, making the baseline defensible.

---

## 3. Why We Run (k=0) and (k=3) in Multiple Phases

| Phase | What Changes | What It Tests | Why It Matters |
|---|---|---|---|
| Baseline (`k=0`) | Prompting, no exemplars | intrinsic LM SQL reasoning | establishes floor |
| Baseline (`k=3`) | + exemplars | ICL conditioning uplift | measures prompt-only gains |
| QLoRA (`k=0`) | trained adapters | SFT uplift alone | isolates fine-tuning effect |
| QLoRA (`k=3`) | adapters + exemplars | coexistence effects | tests complement vs substitution |
| Agentic (planned) | + execution loop | semantic correction | expected largest EX/TS gain |

**Reflection:** This ladder allows stronger causal inference than reporting a single configuration.

---

## 4. Metric Selection (Ojuri-Aligned)

| Metric | Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|---|
| VA | executability | distinguishes syntax vs logic | static parsing → misses runtime issues | `eval.py` |
| EM | surface strict match | debugging | used alone misleading for SQL | `eval.py` |
| EX | execution accuracy | functional correctness | BLEU/ROUGE | `eval.py` |
| TS | test-suite accuracy (planned) | semantic robustness | accuracy-only metrics | documented in `ARCHITECTURE.md` |

**Meta:** Execution-based metrics are standard in NL→SQL because SQL has many surface-equivalent programs.

**Dissertation note:** VA/EX/TS mirror Ojuri et al. (2025) and Zhong et al. (2020) [18], enabling like-for-like comparison.

---

## 5. Safety, Security & Data Access

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| Read-only SQL execution | prevents destructive outputs | trust model outputs → unsafe | `query_runner.py` |
| Block DDL/DML tokens | hallucinated UPDATE/DELETE possible | evaluate full SQL grammar → unnecessary | `eval.py` |
| Cloud SQL + Connector | private IP; IAM auth | public DB endpoint → security risk | `db.py`, `CONFIG.md` |

**Dissertation note:** These controls satisfy basic DB governance; they are standard mitigations when executing model-generated SQL.

---

## 6. Reproducibility & Experimental Integrity

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| Colab as primary runtime | open access + realistic SME hardware | multi-GPU cluster → unrealistic/examiner-unfair | `CONFIG.md` |
| Pinned dependency versions | avoids Colab binary drift | “latest pip” → breaks reproducibility | `requirements.txt` |
| Deterministic decoding | eliminates sampling noise | sampling-based decoding → introduces variance | `llm.py` |
| Gitignored results | avoid accidental large artifacts | committing artifacts → brittle | `.gitignore` |

**Dissertation note:** These measures document the experimental envelope so results can be rerun by examiners.

---

## 7. PEFT Training Data (QLoRA)

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| 200 supervised examples | realistic small-scale SFT | >5k examples → infeasible on Colab | `data/train/` |
| SQL validated before training | enforces VA=True → avoids garbage learning | train on synthetic → unknown distribution drift | `04_build_training_set.ipynb` |

**Dissertation note:** Small, validated, domain-specific sets are consistent with PEFT guidance [12] and VRAM limits in [4].

---

## Architecture Notes (for traceability)

- Components: schema access + prompting + strategy (prompt/QLoRA/agent) + safety/postprocess + execution/eval. Library code in `nl2sql/`; notebooks orchestrate per strategy.
- Flow: NLQ → schema text → strategy → SQL candidate → safety filter → DB execution → metrics (VA/EX/EM; TS planned).
- Design tensions: schema fidelity vs prompt length; k=0 vs k>0 (separate adapter vs exemplar effects); agent vs deterministic eval (EX/TS gains vs latency/variance).
- Agent plan: ReAct/ExCoT loop (Thought→Action→Observation→Refinement) to correct SQL via execution feedback; to be implemented in `03_agentic_eval.ipynb` and evaluated with the same VA/EX/TS harness.

**Dissertation note:** This structure mirrors common applied-NLP practice: stable library code + thin notebooks for orchestration, making decisions auditable.

---

## 8. Compute Constraints & Deployment Reality

| Decision | Why | Alternatives Rejected | Where |
|---|---|---|---|
| 8B scale + 4-bit quantization | fits Colab VRAM; QLoRA-compatible | full FP16 → 60–80GB | `CONFIG.md` |
| No external GPUs | aligns with SME deployment assumption | cloud GPU clusters | `COMPUTATIONAL_CONSTRAINTS.md` |
| OSS only (model + infra) | transparency + reproducibility | GPT-4 APIs | `DECISIONS.md` |

---

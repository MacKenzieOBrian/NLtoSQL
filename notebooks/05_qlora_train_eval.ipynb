{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50805a1c",
   "metadata": {
    "id": "50805a1c"
   },
   "source": [
    "# QLoRA Fine-tuning + Evaluation (ClassicModels NL→SQL)\n",
    "\n",
    "This notebook is the fine-tuning arm of the replication study. It adapts the base model with QLoRA on non-overlapping train data, then evaluates with the same `nl2sql.eval.eval_run` harness used for baseline prompting so weight adaptation effects are isolated.\n",
    "\n",
    "Replication role:\n",
    "- compare base vs fine-tuned at matched prompt settings (`k=0`, `k=3`),\n",
    "- test whether fine-tuning trends reported by Ojuri et al. (2025) hold in an open-source local setup.\n",
    "\n",
    "## Expected inputs\n",
    "- Test set (fixed): `data/classicmodels_test_200.json`\n",
    "- Training set (you create): `data/train/classicmodels_train_200.jsonl` (JSON Lines with `nlq` + `sql` per row)\n",
    "\n",
    "## Outputs\n",
    "- Adapter checkpoint: `results/adapters/qlora_classicmodels/`\n",
    "- Eval outputs: `results/qlora/results_*_200.json`\n",
    "\n",
    "Note: `results/` is gitignored by default. Download the outputs from Colab when finished.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4edb6f6",
   "metadata": {},
   "source": [
    "## One-time setup (run first in a fresh Colab GPU runtime)\n",
    "Run this cell as the first step in a fresh runtime. Let it finish, then **Runtime → Restart runtime** once, and run the rest of the notebook top-to-bottom. This pins torch/bitsandbytes/triton to CUDA 12.1 so 4-bit loading works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13224190",
   "metadata": {},
   "source": [
    "Setup note: pinned versions reduce Colab drift and keep QLoRA runs repeatable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed567a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "set -e\n",
    "export PIP_DEFAULT_TIMEOUT=120\n",
    "\n",
    "# Clean conflicting preinstalls\n",
    "pip uninstall -y torch torchvision torchaudio bitsandbytes triton transformers accelerate peft trl datasets numpy pandas fsspec requests google-auth || true\n",
    "\n",
    "# Base deps\n",
    "pip install -q --no-cache-dir --force-reinstall   numpy==1.26.4 pandas==2.2.1 fsspec==2024.5.0 requests==2.31.0 google-auth==2.43.0\n",
    "\n",
    "# Torch + CUDA 12.1\n",
    "pip install -q --no-cache-dir --force-reinstall   torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121   --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# bitsandbytes + triton + HF stack\n",
    "pip install -q --no-cache-dir --force-reinstall   bitsandbytes==0.43.3 triton==2.3.1   transformers==4.44.2 accelerate==0.33.0 peft==0.17.0 trl==0.9.6 datasets==2.20.0\n",
    "\n",
    "echo \"Setup complete. Restart runtime once, then run the rest of the notebook.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ea1e2",
   "metadata": {},
   "source": [
    "Prompt/eval: build prompts (system+schema+k exemplars), generate SQL, postprocess, and compute VA/EX/EM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18236266",
   "metadata": {},
   "source": [
    "Prompting note: evaluation prompt structure matches baseline so fine-tuning effects are isolated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786cb02c",
   "metadata": {
    "id": "786cb02c"
   },
   "outputs": [],
   "source": [
    "import os, sys, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# If opened directly in Colab, clone the repo first\n",
    "if Path(\"data/classicmodels_test_200.json\").exists() is False and Path(\"/content\").exists():\n",
    "    repo_dir = Path(\"/content/NLtoSQL\")\n",
    "    if repo_dir.exists():\n",
    "        shutil.rmtree(repo_dir)\n",
    "    !git clone https://github.com/MacKenzieOBrian/NLtoSQL.git \"{repo_dir}\"\n",
    "    os.chdir(repo_dir)\n",
    "\n",
    "sys.path.insert(0, os.getcwd())\n",
    "print(\"cwd:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce274f7",
   "metadata": {
    "id": "2ce274f7"
   },
   "source": [
    "## 0) Install dependencies (Colab)\n",
    "\n",
    "Install pinned dependencies from `requirements.txt`. Colab often needs a **runtime restart** after installs (Runtime → Restart runtime), then rerun from the top.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc1044",
   "metadata": {
    "id": "a9bc1044"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip -q install -r requirements.txt\n",
    "\n",
    "    import torch\n",
    "    import accelerate\n",
    "    import peft\n",
    "    import transformers\n",
    "    import trl\n",
    "\n",
    "    print('torch:', torch.__version__, 'cuda:', torch.cuda.is_available())\n",
    "    print('transformers:', transformers.__version__)\n",
    "    print('accelerate:', accelerate.__version__)\n",
    "    print('peft:', peft.__version__)\n",
    "    print('trl:', trl.__version__)\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        print('WARNING: CUDA is not available. In Colab, use a GPU runtime and avoid installing CPU-only torch wheels.')\n",
    "        print('If you just changed torch packages, do: Runtime -> Restart runtime, then run from the top.')\n",
    "else:\n",
    "    print('Not in Colab; ensure requirements are installed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0f3751",
   "metadata": {
    "id": "7e0f3751"
   },
   "source": [
    "## 1) Authentication (GCP + Hugging Face)\n",
    "\n",
    "- GCP auth is required for Cloud SQL access (VA evaluation).\n",
    "- HF auth is required for gated models (Meta Llama 3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9612d759",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "aef4b022d1234ef4aa54aa6165598220",
      "b1f1bbbcc93c480790e8bda49cb9deed",
      "6ee7c8bde67543159ea57f99777e7eba",
      "cb4a9b84d4734cee8cf680d1ddefd2a1",
      "627775b91ca2481391edd8b046401cdd",
      "54826113194d4df793adf94507d0c2de",
      "90d80fb8037e4fa59c32680bc7ad67c5",
      "a7e2a228207e45cba7a0658a5a2bbb67",
      "929fd603c6a444109964777013c9aecb",
      "0e3c4ebad9b745baacb1dccdcf914889",
      "9194c8df1d984dd7bea8fc183d42ddc9",
      "2d2fb4d67c404daea320222e5d18b596",
      "845fc0eae44143a191dde36685215024",
      "b7af619d67a746c6b474f1d30399cf47",
      "a20f2c42b8bb408fb6adcd321803c24e",
      "fd01cfa9ac134260ba7e48d6dd544a79",
      "570ce41cb38c4ab1b292f250b56144c1",
      "af667a7e59804144aec24b486ba054cb",
      "2b9fff4c8c5c4ff582cadafb7a6cbaa1",
      "33c78060c90e48cd933232bd0a65bbb2"
     ]
    },
    "id": "9612d759",
    "outputId": "df62133e-bf9c-4093-d448-84bb9ca2eae2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef4b022d1234ef4aa54aa6165598220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GCP auth (Colab) — safe to skip locally if using ADC\n",
    "try:\n",
    "    from google.colab import auth\n",
    "except ModuleNotFoundError:\n",
    "    auth = None\n",
    "if auth:\n",
    "    auth.authenticate_user()\n",
    "else:\n",
    "    print(\"Not running in Colab; ensure ADC/service account auth is configured.\")\n",
    "\n",
    "# Hugging Face auth\n",
    "hf_token = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if hf_token:\n",
    "    os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = hf_token\n",
    "    print(\"Using HF token from env\")\n",
    "else:\n",
    "    try:\n",
    "        from huggingface_hub import notebook_login\n",
    "        notebook_login()\n",
    "    except Exception as e:\n",
    "        print(\"HF auth not configured:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc190ec3",
   "metadata": {
    "id": "bc190ec3"
   },
   "source": [
    "## 2) Load benchmark + training set\n",
    "\n",
    "Training set must be separate from the 200-item benchmark.\n",
    "\n",
    "Recommended workflow:\n",
    "- Run `notebooks/04_build_training_set.ipynb` to validate (and edit if needed) `data/train/classicmodels_train_200.jsonl`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7ccb8",
   "metadata": {},
   "source": [
    "Data note: schema-aware prompts are kept consistent with baseline for fair comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529e213",
   "metadata": {
    "id": "7529e213"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "test_path = Path(\"data/classicmodels_test_200.json\")\n",
    "train_path = Path(\"data/train/classicmodels_train_200.jsonl\")\n",
    "\n",
    "test_set = json.loads(test_path.read_text(encoding=\"utf-8\"))\n",
    "print(\"Test items:\", len(test_set))\n",
    "\n",
    "if not train_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing training set at {train_path}. Create it before running QLoRA. \"\n",
    "        \"Expected JSONL lines with keys: nlq, sql.\"\n",
    "    )\n",
    "\n",
    "train_records = []\n",
    "for line in train_path.read_text(encoding=\"utf-8\").splitlines():\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    train_records.append(json.loads(line))\n",
    "\n",
    "print(\"Train items:\", len(train_records))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca8b59",
   "metadata": {
    "id": "77ca8b59"
   },
   "source": [
    "### Leakage check (train vs test)\n",
    "\n",
    "At minimum, ensure there is no exact NLQ overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2905be06",
   "metadata": {
    "id": "2905be06"
   },
   "outputs": [],
   "source": [
    "test_nlqs = {item[\"nlq\"].strip() for item in test_set}\n",
    "train_nlqs = [r.get(\"nlq\", \"\").strip() for r in train_records]\n",
    "overlap = sorted({nlq for nlq in train_nlqs if nlq in test_nlqs})\n",
    "\n",
    "print(\"NLQ overlap count:\", len(overlap))\n",
    "if overlap:\n",
    "    print(\"Example overlaps:\")\n",
    "    for x in overlap[:10]:\n",
    "        print(\"-\", x)\n",
    "    raise ValueError(\"Training set overlaps test set; remove overlapping items before training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6401a9",
   "metadata": {
    "id": "6c6401a9"
   },
   "source": [
    "## 3) DB engine + schema summary\n",
    "\n",
    "Schema grounding is kept consistent with the baseline by using `nl2sql.schema.build_schema_summary`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab014ee8",
   "metadata": {},
   "source": [
    "Schema note: `nl2sql.schema` builds the same schema summary format used in baseline runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e3e880",
   "metadata": {},
   "source": [
    "Environment note: DB connection path is identical to baseline (`nl2sql.db`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1f1e3",
   "metadata": {
    "id": "fab1f1e3"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from nl2sql.db import create_engine_with_connector\n",
    "from nl2sql.schema import build_schema_summary\n",
    "\n",
    "INSTANCE_CONNECTION_NAME = os.getenv(\"INSTANCE_CONNECTION_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"classicmodels\")\n",
    "\n",
    "if not INSTANCE_CONNECTION_NAME:\n",
    "    INSTANCE_CONNECTION_NAME = input(\"Enter INSTANCE_CONNECTION_NAME: \").strip()\n",
    "if not DB_USER:\n",
    "    DB_USER = input(\"Enter DB_USER: \").strip()\n",
    "if not DB_PASS:\n",
    "    DB_PASS = getpass(\"Enter DB_PASS: \")\n",
    "\n",
    "engine, connector = create_engine_with_connector(\n",
    "    instance_connection_name=INSTANCE_CONNECTION_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASS,\n",
    "    db_name=DB_NAME,\n",
    ")\n",
    "\n",
    "SCHEMA_SUMMARY = build_schema_summary(engine, db_name=DB_NAME, max_cols_per_table=50)\n",
    "print(\"Schema summary length:\", len(SCHEMA_SUMMARY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f078ecd9",
   "metadata": {
    "id": "f078ecd9"
   },
   "source": [
    "## 4) Load base model (4-bit) + configure QLoRA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43be14b",
   "metadata": {},
   "source": [
    "Model note: the base model is loaded in 4-bit before attaching LoRA adapters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9263d",
   "metadata": {},
   "source": [
    "Fallback note: if memory is tight, lower batch/accumulation settings rather than changing evaluation logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cd208f",
   "metadata": {
    "id": "11cd208f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError('CUDA is not available. In Colab, switch to a GPU runtime: Runtime -> Change runtime type -> GPU.')\n",
    "\n",
    "cc_major, cc_minor = torch.cuda.get_device_capability(0)\n",
    "use_bf16 = cc_major >= 8  \n",
    "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "\n",
    "print('GPU:', torch.cuda.get_device_name(0))\n",
    "print('Compute capability:', (cc_major, cc_minor))\n",
    "print('Using bf16:', use_bf16, '| compute_dtype:', compute_dtype)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, token=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "# transformers/bitsandbytes 4-bit quantization does not allow some layers to be auto-offloaded\n",
    "# to CPU/disk. Force the whole model onto GPU:0. If you OOM, restart runtime and close other\n",
    "# notebooks/tabs, or use a higher-memory GPU (A100/L4).\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map=device_map,\n",
    "    token=True,\n",
    ")\n",
    "\n",
    "# Deterministic defaults for later evaluation\n",
    "base_model.generation_config.do_sample = False\n",
    "base_model.generation_config.temperature = 1.0\n",
    "base_model.generation_config.top_p = 1.0\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5e4a45",
   "metadata": {
    "id": "ae5e4a45"
   },
   "source": [
    "## 5) Build the SFT dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc08f220",
   "metadata": {
    "id": "cc08f220"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from nl2sql.prompting import SYSTEM_INSTRUCTIONS\n",
    "\n",
    "def format_example(nlq: str, sql: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_INSTRUCTIONS},\n",
    "        {\"role\": \"user\", \"content\": \"Schema:\\n\" + SCHEMA_SUMMARY},\n",
    "        {\"role\": \"user\", \"content\": f\"NLQ: {nlq}\"},\n",
    "        {\"role\": \"assistant\", \"content\": sql.rstrip(\";\") + \";\"},\n",
    "    ]\n",
    "    return tok.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "train_texts = [format_example(r[\"nlq\"], r[\"sql\"]) for r in train_records]\n",
    "train_ds = Dataset.from_dict({\"text\": train_texts})\n",
    "print(train_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74808373",
   "metadata": {
    "id": "74808373"
   },
   "source": [
    "## 6) Train (SFT with TRL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b099d12",
   "metadata": {},
   "source": [
    "Training note: TRL `SFTTrainer` + PEFT LoRA provides the standard supervised QLoRA loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf92f6",
   "metadata": {},
   "source": [
    "Stability note: keep train arguments and seed logged so runs are reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17933a6",
   "metadata": {
    "id": "f17933a6"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"results/adapters/qlora_classicmodels\"\n",
    "\n",
    "# T4 GPUs in Colab do not support bf16; use fp16 in that case.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.05,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    bf16=use_bf16,\n",
    "    fp16=(not use_bf16),\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tok,\n",
    "    train_dataset=train_ds,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=training_args,\n",
    "    max_seq_length=1024,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tok.save_pretrained(output_dir)\n",
    "print(\"Saved adapters to:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a6f4c",
   "metadata": {
    "id": "e53a6f4c"
   },
   "source": [
    "## 7) Evaluate adapters on the same 200-item test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3adb00",
   "metadata": {},
   "source": [
    "Evaluation note: adapters are tested on the same fixed 200-item benchmark as baseline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ccc438",
   "metadata": {},
   "source": [
    "**Seed note (important):**\n",
    "- A `seed` controls exemplar sampling when `k > 0`.\n",
    "- Different seeds mean different few-shot exemplar sets from the same pool.\n",
    "- For `k=0` there are no exemplars, so seed has no practical effect.\n",
    "\n",
    "**Experiment knobs in next cell:**\n",
    "- `K_VALUES` + `SEEDS` for k/seed sweeps.\n",
    "- `EXEMPLAR_STRATEGY` for exemplar-pool ablation (`all`, `brief_sql`, `join_heavy`, `agg_heavy`).\n",
    "- `PROMPT_VARIANT` for prompt ablation (`default`, `schema_only_minimal`, `no_routing_hints`).\n",
    "- `SCHEMA_VARIANT` for schema-context ablation (`full`, `first_80_lines`, `first_40_lines`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30897b4",
   "metadata": {
    "id": "d30897b4"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from peft import PeftModel\n",
    "from nl2sql.eval import eval_run\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import subprocess\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import nl2sql.prompting as prompting_mod\n",
    "\n",
    "DEFAULT_SYSTEM_INSTRUCTIONS = prompting_mod.SYSTEM_INSTRUCTIONS\n",
    "\n",
    "PROMPT_VARIANTS = {\n",
    "    \"default\": DEFAULT_SYSTEM_INSTRUCTIONS,\n",
    "    \"schema_only_minimal\": \"\"\"You are an expert data analyst writing MySQL queries.\n",
    "Given the database schema and a natural language question, write a single SQL SELECT query.\n",
    "\n",
    "Rules:\n",
    "- Output ONLY SQL (no explanation, no markdown).\n",
    "- Output exactly ONE statement, starting with SELECT.\n",
    "- Use only tables/columns listed in the schema.\n",
    "\"\"\",\n",
    "    \"no_routing_hints\": DEFAULT_SYSTEM_INSTRUCTIONS.split(\"- Routing hints:\")[0].rstrip(),\n",
    "}\n",
    "\n",
    "def schema_variant_text(schema_text: str, variant: str) -> str:\n",
    "    lines = schema_text.splitlines()\n",
    "    if variant == \"full\":\n",
    "        return schema_text\n",
    "    if variant == \"first_80_lines\":\n",
    "        return \"\\n\".join(lines[:80])\n",
    "    if variant == \"first_40_lines\":\n",
    "        return \"\\n\".join(lines[:40])\n",
    "    raise ValueError(f\"Unknown SCHEMA_VARIANT: {variant}\")\n",
    "\n",
    "def exemplar_pool_for_strategy(items: list[dict], strategy: str) -> list[dict]:\n",
    "    if strategy == \"all\":\n",
    "        return list(items)\n",
    "\n",
    "    def _sql(x):\n",
    "        return str(x.get(\"sql\", \"\")).strip()\n",
    "\n",
    "    def _is_join(sql: str) -> bool:\n",
    "        s = sql.lower()\n",
    "        return \" join \" in f\" {s} \"\n",
    "\n",
    "    def _is_agg(sql: str) -> bool:\n",
    "        return bool(re.search(r\"\\b(sum|avg|count|min|max)\\s*\\(\", sql.lower()))\n",
    "\n",
    "    if strategy == \"brief_sql\":\n",
    "        ranked = sorted(items, key=lambda x: len(_sql(x)))\n",
    "        keep = max(50, int(0.4 * len(ranked)))\n",
    "        pool = ranked[:keep]\n",
    "    elif strategy == \"join_heavy\":\n",
    "        pool = [x for x in items if _is_join(_sql(x))]\n",
    "    elif strategy == \"agg_heavy\":\n",
    "        pool = [x for x in items if _is_agg(_sql(x))]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown EXEMPLAR_STRATEGY: {strategy}\")\n",
    "\n",
    "    return pool if len(pool) >= 10 else list(items)\n",
    "\n",
    "eval_base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map=device_map,\n",
    "    token=True,\n",
    ")\n",
    "eval_base.generation_config.do_sample = False\n",
    "eval_base.generation_config.temperature = 1.0\n",
    "eval_base.generation_config.top_p = 1.0\n",
    "\n",
    "eval_model = PeftModel.from_pretrained(eval_base, output_dir)\n",
    "\n",
    "try:\n",
    "    commit = subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"]).decode().strip()\n",
    "except Exception:\n",
    "    commit = \"unknown\"\n",
    "\n",
    "run_metadata_base = {\n",
    "    \"commit\": commit,\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"method\": \"qlora\",\n",
    "    \"adapter_dir\": output_dir,\n",
    "}\n",
    "\n",
    "Path(\"results/qlora\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run_qlora_grid(\n",
    "    *,\n",
    "    k_values: list[int],\n",
    "    seeds: list[int],\n",
    "    run_tag: str,\n",
    "    prompt_variant: str,\n",
    "    schema_variant: str,\n",
    "    exemplar_strategy: str,\n",
    "    limit: int | None = None,\n",
    "    copy_canonical: bool = True,\n",
    "):\n",
    "    if not seeds:\n",
    "        raise ValueError(\"Provide at least one seed\")\n",
    "\n",
    "    if prompt_variant not in PROMPT_VARIANTS:\n",
    "        raise ValueError(f\"Unknown PROMPT_VARIANT: {prompt_variant}\")\n",
    "\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%SZ\")\n",
    "    run_dir = Path(\"results/qlora/runs\") / f\"{run_tag}_{ts}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    schema_used = schema_variant_text(SCHEMA_SUMMARY, schema_variant)\n",
    "    exemplar_pool = exemplar_pool_for_strategy(test_set, exemplar_strategy)\n",
    "\n",
    "    rows = []\n",
    "    primary_seed = seeds[0]\n",
    "\n",
    "    old_prompt = prompting_mod.SYSTEM_INSTRUCTIONS\n",
    "    prompting_mod.SYSTEM_INSTRUCTIONS = PROMPT_VARIANTS[prompt_variant]\n",
    "\n",
    "    try:\n",
    "        for k in k_values:\n",
    "            seed_list = [primary_seed] if k == 0 else seeds\n",
    "            for seed in seed_list:\n",
    "                save_path = run_dir / f\"results_k{k}_seed{seed}.json\"\n",
    "\n",
    "                run_meta = dict(run_metadata_base)\n",
    "                run_meta.update({\n",
    "                    \"run_tag\": run_tag,\n",
    "                    \"k\": k,\n",
    "                    \"seed\": seed,\n",
    "                    \"prompt_variant\": prompt_variant,\n",
    "                    \"schema_variant\": schema_variant,\n",
    "                    \"exemplar_strategy\": exemplar_strategy,\n",
    "                    \"exemplar_pool_size\": len(exemplar_pool),\n",
    "                })\n",
    "\n",
    "                items = eval_run(\n",
    "                    test_set=test_set,\n",
    "                    exemplar_pool=exemplar_pool,\n",
    "                    k=k,\n",
    "                    limit=limit,\n",
    "                    seed=seed,\n",
    "                    engine=engine,\n",
    "                    model=eval_model,\n",
    "                    tokenizer=tok,\n",
    "                    schema_summary=schema_used,\n",
    "                    save_path=str(save_path),\n",
    "                    run_metadata=run_meta,\n",
    "                    avoid_exemplar_leakage=True,\n",
    "                )\n",
    "\n",
    "                n = len(items)\n",
    "                va = sum(int(x.va) for x in items) / max(n, 1)\n",
    "                em = sum(int(x.em) for x in items) / max(n, 1)\n",
    "                ex = sum(int(x.ex) for x in items) / max(n, 1)\n",
    "\n",
    "                rows.append({\n",
    "                    \"run_tag\": run_tag,\n",
    "                    \"prompt_variant\": prompt_variant,\n",
    "                    \"schema_variant\": schema_variant,\n",
    "                    \"exemplar_strategy\": exemplar_strategy,\n",
    "                    \"exemplar_pool_size\": len(exemplar_pool),\n",
    "                    \"k\": k,\n",
    "                    \"seed\": seed,\n",
    "                    \"n\": n,\n",
    "                    \"va_rate\": va,\n",
    "                    \"em_rate\": em,\n",
    "                    \"ex_rate\": ex,\n",
    "                    \"json_path\": str(save_path),\n",
    "                })\n",
    "\n",
    "                if copy_canonical and seed == primary_seed and k in {0, 3}:\n",
    "                    target = (\n",
    "                        Path(\"results/qlora/results_zero_shot_200.json\")\n",
    "                        if k == 0\n",
    "                        else Path(\"results/qlora/results_few_shot_k3_200.json\")\n",
    "                    )\n",
    "                    target.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    shutil.copy2(save_path, target)\n",
    "                    print(f\"Updated canonical file: {target}\")\n",
    "    finally:\n",
    "        prompting_mod.SYSTEM_INSTRUCTIONS = old_prompt\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values([\"k\", \"seed\"]).reset_index(drop=True)\n",
    "    df.to_csv(run_dir / \"grid_summary.csv\", index=False)\n",
    "\n",
    "    agg = (\n",
    "        df.groupby([\"prompt_variant\", \"schema_variant\", \"exemplar_strategy\", \"k\"], as_index=False)\n",
    "        .agg(\n",
    "            runs=(\"seed\", \"count\"),\n",
    "            va_mean=(\"va_rate\", \"mean\"),\n",
    "            va_std=(\"va_rate\", \"std\"),\n",
    "            em_mean=(\"em_rate\", \"mean\"),\n",
    "            em_std=(\"em_rate\", \"std\"),\n",
    "            ex_mean=(\"ex_rate\", \"mean\"),\n",
    "            ex_std=(\"ex_rate\", \"std\"),\n",
    "        )\n",
    "    )\n",
    "    agg.to_csv(run_dir / \"grid_summary_by_k.csv\", index=False)\n",
    "\n",
    "    print(\"Saved grid run to:\", run_dir)\n",
    "    return df, agg, run_dir\n",
    "\n",
    "# ============================\n",
    "# QUICK MODE (recommended default)\n",
    "K_VALUES = [0, 3]\n",
    "SEEDS = [7]\n",
    "RUN_TAG = \"qlora_main\"\n",
    "PROMPT_VARIANT = \"default\"\n",
    "SCHEMA_VARIANT = \"full\"\n",
    "EXEMPLAR_STRATEGY = \"all\"\n",
    "\n",
    "# FULL K/S E1 SWEEP (uncomment for primary experiment)\n",
    "# K_VALUES = [0, 1, 3, 5, 8]\n",
    "# SEEDS = [7, 17, 27, 37, 47]\n",
    "# RUN_TAG = \"qlora_e1_k_sweep\"\n",
    "# PROMPT_VARIANT = \"default\"\n",
    "# SCHEMA_VARIANT = \"full\"\n",
    "# EXEMPLAR_STRATEGY = \"all\"\n",
    "\n",
    "# EXEMPLAR STRATEGY ABLATION EXAMPLE (few-shot only; keep k>0 values)\n",
    "# K_VALUES = [3]\n",
    "# SEEDS = [7, 17, 27, 37, 47]\n",
    "# RUN_TAG = \"qlora_exemplar_brief\"\n",
    "# EXEMPLAR_STRATEGY = \"brief_sql\"\n",
    "\n",
    "# PROMPT/SCHEMA ABLATION EXAMPLE\n",
    "# K_VALUES = [0, 3]\n",
    "# SEEDS = [7]\n",
    "# RUN_TAG = \"qlora_prompt_ablation_schema_only\"\n",
    "# PROMPT_VARIANT = \"schema_only_minimal\"\n",
    "# SCHEMA_VARIANT = \"first_80_lines\"\n",
    "\n",
    "qlora_grid, qlora_by_k, qlora_run_dir = run_qlora_grid(\n",
    "    k_values=K_VALUES,\n",
    "    seeds=SEEDS,\n",
    "    run_tag=RUN_TAG,\n",
    "    prompt_variant=PROMPT_VARIANT,\n",
    "    schema_variant=SCHEMA_VARIANT,\n",
    "    exemplar_strategy=EXEMPLAR_STRATEGY,\n",
    "    limit=None,\n",
    ")\n",
    "\n",
    "print(\"\\nPer-run rows:\")\n",
    "display(qlora_grid)\n",
    "print(\"\\nPer-k summary (mean/std across seeds):\")\n",
    "display(qlora_by_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d656bc",
   "metadata": {
    "id": "75d656bc"
   },
   "source": [
    "## 8) Compare against baseline outputs (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b6ce72",
   "metadata": {
    "id": "e0b6ce72"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "baseline_zero = Path(\"results/baseline/results_zero_shot_200.json\")\n",
    "baseline_few  = Path(\"results/baseline/results_few_shot_k3_200.json\")\n",
    "\n",
    "if baseline_zero.exists() and baseline_few.exists():\n",
    "    b0 = json.loads(baseline_zero.read_text(encoding=\"utf-8\"))\n",
    "    b3 = json.loads(baseline_few.read_text(encoding=\"utf-8\"))\n",
    "    q0 = json.loads(Path(\"results/qlora/results_zero_shot_200.json\").read_text(encoding=\"utf-8\"))\n",
    "    q3 = json.loads(Path(\"results/qlora/results_few_shot_k3_200.json\").read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    print(\"Baseline zero-shot:\", \"VA\", round(b0[\"va_rate\"], 3), \"EM\", round(b0.get(\"em_rate\", 0.0), 3), \"EX\", round(b0[\"ex_rate\"], 3))\n",
    "    print(\"QLoRA   zero-shot:\", \"VA\", round(q0[\"va_rate\"], 3), \"EM\", round(q0.get(\"em_rate\", 0.0), 3), \"EX\", round(q0[\"ex_rate\"], 3))\n",
    "    print(\"Baseline few-shot :\", \"VA\", round(b3[\"va_rate\"], 3), \"EM\", round(b3.get(\"em_rate\", 0.0), 3), \"EX\", round(b3[\"ex_rate\"], 3))\n",
    "    print(\"QLoRA   few-shot :\", \"VA\", round(q3[\"va_rate\"], 3), \"EM\", round(q3.get(\"em_rate\", 0.0), 3), \"EX\", round(q3[\"ex_rate\"], 3))\n",
    "else:\n",
    "    print(\"Baseline JSONs not found under results/baseline/. Run the baseline notebook first (or upload the JSONs).\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c7beb01",
   "metadata": {},
   "source": [
    "# ReAct Eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Purpose: evaluate the tool-driven ReAct pipeline with traceable reasoning steps.\n",
    "- Scope: supports both interactive single-question demos and full benchmark execution.\n",
    "- Outputs: agent run artifacts and failure profiles under `results/agent/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e5edc",
   "metadata": {},
   "source": [
    "## 0) Install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a2f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "set -e\n",
    "export PIP_DEFAULT_TIMEOUT=120\n",
    "\n",
    "# Clean conflicting preinstalls\n",
    "pip uninstall -y torch torchvision torchaudio bitsandbytes triton transformers accelerate peft trl datasets numpy pandas fsspec requests google-auth || true\n",
    "\n",
    "# Base deps\n",
    "pip install -q --no-cache-dir --force-reinstall   numpy==1.26.4 pandas==2.2.1 fsspec==2024.5.0 requests==2.31.0 google-auth==2.43.0\n",
    "\n",
    "# Torch + CUDA 12.1\n",
    "pip install -q --no-cache-dir --force-reinstall   torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121   --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# bitsandbytes + triton + HF stack\n",
    "pip install -q --no-cache-dir --force-reinstall   bitsandbytes==0.43.3 triton==2.3.1   transformers==4.44.2 accelerate==0.33.0 peft==0.17.0 trl==0.9.6 datasets==2.20.0\n",
    "\n",
    "echo \"Setup complete. Restart runtime once, then run the rest of the notebook top-to-bottom.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f60137",
   "metadata": {},
   "source": [
    "## 1) Sync Repo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa8947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Clone repo (Colab) + install deps\n",
    "import os\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not os.path.exists('/content/NLtoSQL'):\n",
    "        !git clone https://github.com/MacKenzieOBrian/NLtoSQL.git /content/NLtoSQL\n",
    "    %cd /content/NLtoSQL\n",
    "    !pip -q install -r requirements.txt\n",
    "    import torch, transformers, accelerate, peft\n",
    "    print('torch', torch.__version__, 'cuda', torch.cuda.is_available())\n",
    "else:\n",
    "    print('Not in Colab; using existing workspace')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647ef39",
   "metadata": {},
   "source": [
    "## 2) Auth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b11b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this only if you prefer gcloud-based ADC (no JSON key)\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    %pip install -q --upgrade google-auth google-auth-oauthlib\n",
    "    !gcloud auth application-default login\n",
    "else:\n",
    "    print(\"Not in Colab; skip gcloud auth.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca499c",
   "metadata": {},
   "source": [
    "## 3) DB Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf829a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Environment + DB\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "from nl2sql.db import create_engine_with_connector, safe_connection\n",
    "\n",
    "# Expected env vars (set these in a Colab cell):\n",
    "# INSTANCE_CONNECTION_NAME, DB_USER, DB_PASS, DB_NAME\n",
    "INSTANCE_CONNECTION_NAME = os.getenv(\"INSTANCE_CONNECTION_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\") or \"classicmodels\"\n",
    "\n",
    "if not INSTANCE_CONNECTION_NAME:\n",
    "    INSTANCE_CONNECTION_NAME = input(\"Enter INSTANCE_CONNECTION_NAME: \").strip()\n",
    "if not DB_USER:\n",
    "    DB_USER = input(\"Enter DB_USER: \").strip()\n",
    "if not DB_PASS:\n",
    "    DB_PASS = getpass(\"Enter DB_PASS: \")\n",
    "\n",
    "# Canonical engine builder (shared with scripts + other notebooks).\n",
    "# Uses Cloud SQL Connector under the hood and ADC for credentials.\n",
    "engine, connector = create_engine_with_connector(\n",
    "    instance_connection_name=INSTANCE_CONNECTION_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASS,\n",
    "    db_name=DB_NAME,\n",
    ")\n",
    "\n",
    "with safe_connection(engine) as conn:\n",
    "    conn.execute(text(\"SELECT 1\"))\n",
    "print(\"DB connection OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7a447a",
   "metadata": {},
   "source": [
    "## 4) TS Engines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79134dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b) Engine factory for TS (multiple DB names)\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "\n",
    "def make_engine(db_name: str) -> Engine:\n",
    "    \"\"\"Create a new engine bound to a specific TS replica DB name.\n",
    "\n",
    "    TS (test-suite accuracy) executes the same (gold, pred) SQL across multiple\n",
    "    replica databases (classicmodels_ts_XX). We keep separate engines so each\n",
    "    replica is evaluated independently.\n",
    "    \"\"\"\n",
    "\n",
    "    def getconn_for_db():\n",
    "        return connector.connect(\n",
    "            INSTANCE_CONNECTION_NAME,\n",
    "            \"pymysql\",\n",
    "            user=DB_USER,\n",
    "            password=DB_PASS,\n",
    "            db=db_name,\n",
    "        )\n",
    "\n",
    "    return sqlalchemy.create_engine(\"mysql+pymysql://\", creator=getconn_for_db, future=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29403cf",
   "metadata": {},
   "source": [
    "## 5) Schema + Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6bf8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load schema summary + test set + QueryRunner\n",
    "import json\n",
    "from pathlib import Path\n",
    "from nl2sql.schema import build_schema_summary\n",
    "from nl2sql.query_runner import QueryRunner\n",
    "\n",
    "SCHEMA_SUMMARY = build_schema_summary(engine, db_name=DB_NAME)\n",
    "print(\"Schema contains offices.city:\", \"offices\" in SCHEMA_SUMMARY.lower() and \"city\" in SCHEMA_SUMMARY.lower())\n",
    "\n",
    "test_path = Path(\"data/classicmodels_test_200.json\")\n",
    "full_set = json.loads(test_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# Default target is full benchmark evaluation (200 items).\n",
    "test_set = full_set\n",
    "print(\"Loaded test items:\", len(test_set))\n",
    "\n",
    "# Runner is used for local VA checks in demo sanity cells.\n",
    "runner = QueryRunner(engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6137ff",
   "metadata": {},
   "source": [
    "## 6) Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Load model (base or QLoRA adapters)\n",
    "import os\n",
    "from getpass import getpass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "ADAPTER_PATH = os.getenv(\"ADAPTER_PATH\") or \"results/adapters/qlora_classicmodels\"  # set to None to use base model\n",
    "\n",
    "# Experiment knobs (change one axis at a time for comparable claims):\n",
    "# - MODEL_ID: switch model family (Llama/Qwen/etc).\n",
    "# - ADAPTER_PATH: set to local QLoRA adapter dir for tuned runs; set to None for base-model runs.\n",
    "# Keep all later loop/TS settings unchanged when isolating model effects.\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if not HF_TOKEN:\n",
    "    HF_TOKEN = getpass(\"Enter HF_TOKEN (https://huggingface.co/settings/tokens): \").strip()\n",
    "\n",
    "cc_major, cc_minor = torch.cuda.get_device_capability(0) if torch.cuda.is_available() else (0, 0)\n",
    "use_bf16 = cc_major >= 8\n",
    "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"Using bf16:\", use_bf16)\n",
    "print(\"Adapter path:\", ADAPTER_PATH)\n",
    "\n",
    "# Tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# Quantized base model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map={\"\": 0} if torch.cuda.is_available() else None,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "base_model.generation_config.do_sample = False\n",
    "base_model.generation_config.temperature = 1.0\n",
    "base_model.generation_config.top_p = 1.0\n",
    "\n",
    "# Load adapters if present locally; otherwise use base model\n",
    "adapter_dir = Path(ADAPTER_PATH) if ADAPTER_PATH else None\n",
    "if adapter_dir and adapter_dir.exists():\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_dir, token=HF_TOKEN)\n",
    "    print(\"Loaded adapters from\", adapter_dir)\n",
    "else:\n",
    "    print(\"Adapter path missing; using base model only. Set ADAPTER_PATH to your local adapter folder or upload it to Colab.\")\n",
    "    model = base_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f03e11e",
   "metadata": {},
   "source": [
    "## 7) Pipeline Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8474e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ReAct module imports, context binding, and config ---\n",
    "from nl2sql.agent_tools import AgentContext, set_agent_context\n",
    "from nl2sql.react_pipeline import ReactAblationConfig, run_react_pipeline, evaluate_react_ablation\n",
    "from nl2sql.prompts import REACT_SYSTEM_PROMPT\n",
    "\n",
    "# Bind shared runtime context once.\n",
    "set_agent_context(\n",
    "    AgentContext(\n",
    "        engine=engine,\n",
    "        db_name=DB_NAME,\n",
    "        model=model,\n",
    "        tok=tok,\n",
    "        runner=runner,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    ")\n",
    "\n",
    "# ReAct config knobs (edit for controlled ablations).\n",
    "REACT_CONFIG_NAME = \"react_core_notebook\"\n",
    "REACT_USE_SCHEMA_LINK = True\n",
    "REACT_USE_CONSTRAINT_POLICY = True\n",
    "REACT_USE_REPAIR_POLICY = True\n",
    "REACT_USE_INTENT_GATE = False\n",
    "REACT_MAX_REPAIRS = 1\n",
    "REACT_LINK_MAX_TABLES = 6\n",
    "REACT_MAX_STEPS = 8\n",
    "REACT_MAX_NEW_TOKENS = 256\n",
    "REACT_DO_SAMPLE = False\n",
    "REACT_TEMPERATURE = 0.2\n",
    "REACT_TOP_P = 0.9\n",
    "\n",
    "REACT_CONFIG = ReactAblationConfig(\n",
    "    name=REACT_CONFIG_NAME,\n",
    "    use_schema_link=REACT_USE_SCHEMA_LINK,\n",
    "    use_constraint_policy=REACT_USE_CONSTRAINT_POLICY,\n",
    "    use_repair_policy=REACT_USE_REPAIR_POLICY,\n",
    "    use_intent_gate=REACT_USE_INTENT_GATE,\n",
    "    max_repairs=REACT_MAX_REPAIRS,\n",
    "    link_max_tables=REACT_LINK_MAX_TABLES,\n",
    "    max_steps=REACT_MAX_STEPS,\n",
    "    max_new_tokens=REACT_MAX_NEW_TOKENS,\n",
    "    do_sample=REACT_DO_SAMPLE,\n",
    "    temperature=REACT_TEMPERATURE,\n",
    "    top_p=REACT_TOP_P,\n",
    ")\n",
    "\n",
    "\n",
    "def summarize_trace_brief(trace: list[dict]) -> dict:\n",
    "    # Small notebook helper for readable demo output.\n",
    "    actions = [t.get(\"action\") for t in trace if t.get(\"action\")]\n",
    "    blocked_steps = sum(1 for t in trace if t.get(\"blocked\"))\n",
    "    stop_reason = next((t.get(\"reason\") for t in trace if t.get(\"action\") == \"stop\"), None)\n",
    "    return {\n",
    "        \"steps\": len(trace),\n",
    "        \"actions\": actions,\n",
    "        \"blocked_steps\": blocked_steps,\n",
    "        \"stop_reason\": stop_reason,\n",
    "    }\n",
    "\n",
    "print(\"Using pipeline module:\", run_react_pipeline.__module__)\n",
    "print(\"ReAct config:\", REACT_CONFIG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c9413",
   "metadata": {},
   "source": [
    "## 9) Interactive Demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e205536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12A) Interactive walkthrough (single NLQ trace)\n",
    "DEMO_INTERACTIVE = True\n",
    "DEMO_DEFAULT_NLQ = \"Which customers are in France?\"\n",
    "\n",
    "nlq = \"\"\n",
    "if DEMO_INTERACTIVE:\n",
    "    try:\n",
    "        nlq = input(\"Type a ClassicModels question (blank uses default): \").strip()\n",
    "    except Exception:\n",
    "        nlq = \"\"\n",
    "if not nlq:\n",
    "    nlq = DEMO_DEFAULT_NLQ\n",
    "\n",
    "pred_sql, trace = run_react_pipeline(nlq=nlq, config=REACT_CONFIG)\n",
    "summary = summarize_trace_brief(trace)\n",
    "\n",
    "print()\n",
    "print(\"FINAL SQL:\")\n",
    "print(pred_sql or \"(no prediction)\")\n",
    "print()\n",
    "print(\"TRACE SUMMARY:\")\n",
    "print(summary)\n",
    "if trace:\n",
    "    print()\n",
    "    print(\"LAST TRACE ENTRY:\")\n",
    "    print(trace[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c5f3e",
   "metadata": {},
   "source": [
    "## 10) Sanity Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82103b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12B) Quick sanity check on a small slice\n",
    "from nl2sql.eval import execution_accuracy\n",
    "\n",
    "DEBUG_EX = False  # set True for a quick EX check (slower)\n",
    "DEBUG_TRACE = True\n",
    "\n",
    "for sample in test_set[:5]:\n",
    "    nlq = sample[\"nlq\"]\n",
    "    gold = sample[\"sql\"]\n",
    "    pred, trace = run_react_pipeline(nlq=nlq, config=REACT_CONFIG)\n",
    "\n",
    "    print(\"NLQ:\", nlq)\n",
    "    print(\"PRED:\", pred)\n",
    "    print(\"GOLD:\", gold)\n",
    "\n",
    "    if pred:\n",
    "        meta = runner.run(pred, capture_df=False)\n",
    "        print(\"VA:\", int(meta.success), \"ERR:\", meta.error)\n",
    "        if DEBUG_EX:\n",
    "            ex_ok, pred_err, gold_err = execution_accuracy(engine=engine, pred_sql=pred, gold_sql=gold)\n",
    "            print(\"EX:\", int(ex_ok), \"PRED_ERR:\", pred_err, \"GOLD_ERR:\", gold_err)\n",
    "    else:\n",
    "        print(\"VA:\", 0, \"ERR:\", \"no_prediction\")\n",
    "\n",
    "    if DEBUG_TRACE and trace:\n",
    "        print(\"TRACE:\", summarize_trace_brief(trace))\n",
    "        print(\"TRACE LAST:\", trace[-1])\n",
    "    else:\n",
    "        print(\"TRACE LEN:\", len(trace))\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be998691",
   "metadata": {},
   "source": [
    "## 11) Run Controls\n",
    "This cell sets full-vs-quick mode and TS settings, then offers an optional single-item trace preview.\n",
    "Use it to explain how `run_react_pipeline` behavior maps into final batch evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beacbd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Run controls ===\n",
    "# Default is full benchmark reporting.\n",
    "QUICK_LIMIT = None   # set to 20 for quick checks, None for full 200\n",
    "TS_N = 10            # set to 3 for faster debug, 10 for full TS\n",
    "MAX_ROWS_TS = 500\n",
    "TS_PREFIX = \"classicmodels_ts\"\n",
    "\n",
    "REACT_CONFIG = ReactAblationConfig(\n",
    "    name=REACT_CONFIG_NAME,\n",
    "    use_schema_link=REACT_USE_SCHEMA_LINK,\n",
    "    use_constraint_policy=REACT_USE_CONSTRAINT_POLICY,\n",
    "    use_repair_policy=REACT_USE_REPAIR_POLICY,\n",
    "    use_intent_gate=REACT_USE_INTENT_GATE,\n",
    "    max_repairs=REACT_MAX_REPAIRS,\n",
    "    link_max_tables=REACT_LINK_MAX_TABLES,\n",
    "    max_steps=REACT_MAX_STEPS,\n",
    "    max_new_tokens=REACT_MAX_NEW_TOKENS,\n",
    "    do_sample=REACT_DO_SAMPLE,\n",
    "    temperature=REACT_TEMPERATURE,\n",
    "    top_p=REACT_TOP_P,\n",
    ")\n",
    "\n",
    "print(\"Active config:\", REACT_CONFIG)\n",
    "print(\"Run mode:\", \"full_200\" if QUICK_LIMIT is None else f\"quick_{QUICK_LIMIT}\")\n",
    "print(\"TS replicas:\", TS_N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1: one-item ReAct trace preview\n",
    "TRACE_PREVIEW_INDEX = 0\n",
    "\n",
    "if not test_set:\n",
    "    print(\"test_set is empty; cannot run trace preview.\")\n",
    "else:\n",
    "    idx = max(0, min(TRACE_PREVIEW_INDEX, len(test_set) - 1))\n",
    "    preview_item = test_set[idx]\n",
    "    preview_pred, preview_trace = run_react_pipeline(nlq=preview_item[\"nlq\"], config=REACT_CONFIG)\n",
    "\n",
    "    print(\"Trace preview NLQ:\", preview_item[\"nlq\"])\n",
    "    print(\"Pred SQL:\", preview_pred or \"(no prediction)\")\n",
    "    print(\"Trace steps:\", len(preview_trace))\n",
    "    if preview_trace:\n",
    "        print(\"First step:\", preview_trace[0])\n",
    "        print(\"Last step:\", preview_trace[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2: full single-item ReAct debug report (action-by-action + VA/EM/EX)\n",
    "from nl2sql.agent.react_pipeline import debug_react_single_item\n",
    "\n",
    "REACT_DEBUG_INDEX = 0\n",
    "\n",
    "if not test_set:\n",
    "    print(\"test_set is empty; cannot run single-item ReAct debug.\")\n",
    "else:\n",
    "    idx = max(0, min(REACT_DEBUG_INDEX, len(test_set) - 1))\n",
    "    react_item = test_set[idx]\n",
    "    react_debug = debug_react_single_item(\n",
    "        item=react_item,\n",
    "        engine=engine,\n",
    "        config=REACT_CONFIG,\n",
    "        allow_extra_columns_ex=False,\n",
    "    )\n",
    "\n",
    "    print(\"ReAct single-item debug summary:\")\n",
    "    print({\n",
    "        \"nlq\": react_debug[\"nlq\"],\n",
    "        \"pred_sql\": react_debug[\"pred_sql\"],\n",
    "        \"va\": react_debug[\"va\"],\n",
    "        \"em\": react_debug[\"em\"],\n",
    "        \"ex\": react_debug[\"ex\"],\n",
    "        \"trace_len\": react_debug[\"trace_len\"],\n",
    "        \"action_counts\": react_debug[\"action_counts\"],\n",
    "        \"error\": react_debug[\"error\"],\n",
    "    })\n",
    "\n",
    "    print(\"\\nTrace walkthrough:\")\n",
    "    for step in react_debug[\"trace\"]:\n",
    "        action = step.get(\"action\")\n",
    "        obs = step.get(\"observation\")\n",
    "        blocked = step.get(\"blocked\")\n",
    "        print(f\"step={step.get('step')} action={action} blocked={blocked}\")\n",
    "        if isinstance(obs, dict):\n",
    "            print(\" observation keys:\", sorted(obs.keys()))\n",
    "        else:\n",
    "            print(\" observation:\", obs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8946e8",
   "metadata": {},
   "source": [
    "## 12) Eval Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Full-eval helpers and setup ---\n",
    "import json\n",
    "import shutil\n",
    "import subprocess\n",
    "from datetime import datetime, timezone\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "# Run metadata for reproducibility logs.\n",
    "try:\n",
    "    commit = subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"]).decode().strip()\n",
    "except Exception:\n",
    "    commit = \"unknown\"\n",
    "\n",
    "RUN_TAG = f\"react_{REACT_CONFIG.name}\"\n",
    "RUN_TS = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%SZ\")\n",
    "RUN_DIR = Path(\"results/agent/runs\") / f\"{RUN_TAG}_{RUN_TS}\"\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SUITE_DBS = [f\"{TS_PREFIX}_{i:02d}\" for i in range(1, TS_N + 1)] if TS_N and TS_N > 0 else []\n",
    "\n",
    "@lru_cache(maxsize=32)\n",
    "def make_engine_cached(db_name: str) -> Engine:\n",
    "    return make_engine(db_name)\n",
    "\n",
    "def make_engine_fn(db_name: str) -> Engine:\n",
    "    return make_engine_cached(db_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fcc704",
   "metadata": {},
   "source": [
    "## 13) Run Full Eval\n",
    "This is the long-running evidence cell: it executes `evaluate_react_ablation`, saves JSON, and updates canonical artifacts for downstream comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aeeb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Execute full evaluation loop ---\n",
    "out_path = RUN_DIR / \"results_react_eval.json\"\n",
    "\n",
    "run_metadata = {\n",
    "    \"commit\": commit,\n",
    "    \"notebook\": \"03_agentic_eval.ipynb\",\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"adapter_path\": ADAPTER_PATH,\n",
    "    \"config_name\": REACT_CONFIG.name,\n",
    "    \"quick_limit\": QUICK_LIMIT,\n",
    "    \"ts_n\": TS_N,\n",
    "}\n",
    "\n",
    "report = evaluate_react_ablation(\n",
    "    test_set=test_set,\n",
    "    engine=engine,\n",
    "    config=REACT_CONFIG,\n",
    "    limit=QUICK_LIMIT,\n",
    "    ts_suite_db_names=SUITE_DBS if SUITE_DBS else None,\n",
    "    ts_make_engine_fn=make_engine_fn if SUITE_DBS else None,\n",
    "    ts_max_rows=MAX_ROWS_TS,\n",
    "    progress_every=20,\n",
    "    run_metadata=run_metadata,\n",
    "    save_path=out_path,\n",
    ")\n",
    "\n",
    "results = report.get(\"items\", [])\n",
    "print(\n",
    "    \"ReAct\",\n",
    "    \"VA=\", round(report.get(\"va_rate\", 0.0), 3),\n",
    "    \"EM=\", round(report.get(\"em_rate\", 0.0), 3),\n",
    "    \"EX=\", round(report.get(\"ex_rate\", 0.0), 3),\n",
    "    \"TS=\", \"NA\" if report.get(\"ts_rate\") is None else round(report.get(\"ts_rate\", 0.0), 3),\n",
    ")\n",
    "print(\"Saved report:\", out_path)\n",
    "\n",
    "# Canonical compatibility copy for downstream scripts (full run only).\n",
    "if QUICK_LIMIT is None:\n",
    "    canonical_path = Path(\"results/agent/results_react_200.json\")\n",
    "    canonical_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(out_path, canonical_path)\n",
    "    print(\"Updated canonical file:\", canonical_path)\n",
    "else:\n",
    "    print(\"Quick run detected; canonical file not updated.\")\n",
    "\n",
    "# Optional: persist outputs to Drive (Colab-safe).\n",
    "PERSIST_TO_DRIVE = True\n",
    "DRIVE_PERSIST_ROOT = \"/content/drive/MyDrive/nl2sql_persistent_runs\"\n",
    "\n",
    "def persist_agent_run_to_drive(\n",
    "    out_path: Path,\n",
    "    run_tag: str = \"react_eval\",\n",
    "    persist_root: str = DRIVE_PERSIST_ROOT,\n",
    "):\n",
    "    root = Path(persist_root)\n",
    "    root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    stamp = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%SZ\")\n",
    "    dst = root / f\"{run_tag}_{stamp}\"\n",
    "    dst.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if out_path.exists():\n",
    "        shutil.copy2(out_path, dst / out_path.name)\n",
    "\n",
    "    profile_path = Path(\"results/agent/ex_failure_profile.json\")\n",
    "    if profile_path.exists():\n",
    "        shutil.copy2(profile_path, dst / profile_path.name)\n",
    "\n",
    "    manifest_lines = [\n",
    "        f\"out_path={out_path}\",\n",
    "        f\"run_tag={run_tag}\",\n",
    "        f\"quick_limit={QUICK_LIMIT}\",\n",
    "        f\"ts_n={TS_N}\",\n",
    "        f\"config_name={REACT_CONFIG.name}\",\n",
    "        f\"use_schema_link={REACT_CONFIG.use_schema_link}\",\n",
    "        f\"use_constraint_policy={REACT_CONFIG.use_constraint_policy}\",\n",
    "        f\"use_repair_policy={REACT_CONFIG.use_repair_policy}\",\n",
    "    ]\n",
    "    (dst / \"backup_manifest.txt\").write_text(\"\\n\".join(manifest_lines), encoding=\"utf-8\")\n",
    "    return dst\n",
    "\n",
    "if PERSIST_TO_DRIVE:\n",
    "    try:\n",
    "        backup_dir = persist_agent_run_to_drive(out_path=out_path, run_tag=RUN_TAG)\n",
    "        print(\"Persistent backup saved to:\", backup_dir)\n",
    "    except Exception as e:\n",
    "        print(\"Drive backup skipped/failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f0255",
   "metadata": {},
   "source": [
    "## 14) Failure Profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eaf5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# 15B) EX failure profiling (quick categories)\n",
    "def categorize_ex_failure(item: dict) -> str:\n",
    "    pred = item.get(\"pred_sql\")\n",
    "    va = int(item.get(\"va\", 0))\n",
    "    ex = int(item.get(\"ex\", 0))\n",
    "    err = str(item.get(\"error\") or \"\").lower()\n",
    "\n",
    "    if not pred:\n",
    "        if \"repair_budget_exhausted\" in err:\n",
    "            return \"repair_budget_exhausted\"\n",
    "        return \"no_prediction\"\n",
    "    if va == 0:\n",
    "        if \"guardrail_reject\" in err:\n",
    "            return \"guardrail_reject\"\n",
    "        if \"validate_sql\" in err:\n",
    "            return \"validate_sql_failed\"\n",
    "        return \"invalid_sql\"\n",
    "    if ex == 1:\n",
    "        return \"correct\"\n",
    "    if \"validate_constraints\" in err:\n",
    "        return \"constraint_mismatch\"\n",
    "    if \"intent_mismatch\" in err:\n",
    "        return \"intent_mismatch\"\n",
    "    return \"semantic_mismatch\"\n",
    "\n",
    "counts = Counter(categorize_ex_failure(r) for r in results)\n",
    "print(\"EX failure categories:\")\n",
    "for k, v in counts.most_common():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "profile_path = Path(\"results/agent/ex_failure_profile.json\")\n",
    "profile_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "profile = {\n",
    "    \"counts\": dict(counts),\n",
    "    \"n_items\": len(results),\n",
    "    \"quick_limit\": QUICK_LIMIT,\n",
    "    \"ts_n\": TS_N,\n",
    "    \"config_name\": REACT_CONFIG.name,\n",
    "}\n",
    "profile_path.write_text(json.dumps(profile, indent=2), encoding=\"utf-8\")\n",
    "print(\"Saved failure profile:\", profile_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
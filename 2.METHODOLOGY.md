# Methodology (open-source replica)

This is the working story for the project: we restage the Ojuri et al. idea with open models and Colab-level GPUs. The flow is simple: NL→SQL on ClassicModels, always feed schema context, try prompting vs QLoRA vs (soon) an agent loop, and keep it reproducible on cheap hardware.

For the pipeline I think of it as a straight line: get schema awareness, pick a strategy (prompt / QLoRA / agent) to spit out SQL, run a quick safety/postprocess, execute on the live DB, score VA/EX/EM (TS later). The heavy lifting sits in `nl2sql/` (schema helper, prompt builder, postprocess, QueryRunner, eval); the notebooks (`02`, `05`, and planned `03`) just drive runs. Safety is boring but necessary: SELECT-only, deterministic decoding, and leakage checks on train/test/exemplars.

Prompt hygiene stays consistent: system message, schema summary, a few exemplars (k), then the NLQ. Decoding is deterministic. Schema text is ordered so key/name columns show up first. Post-processing grabs the first `SELECT …;`, drops chatter, and trims list-style outputs to minimal columns. k=0 and k=3 are reported separately to avoid leakage. The Dec 2025 bump came purely from this hygiene—no weight changes—so it’s worth keeping.

---

# 3. Domain and Data

- Domain: **ClassicModels (MySQL)**
- Benchmarks: **200 held-out NL→SQL test examples**
- Training set: **~200 supervised examples (JSONL)**

**Rationale:** ClassicModels provides realistic business workloads involving joins, aggregates, and ordering. It also matches the domain used in Ojuri et al. (2025), enabling like-for-like reproduction against GPT-4 agent baselines.

**Dataset Reflection (from notebooks):**  
Zero-shot prompting struggled with join-heavy queries, reinforcing the importance of supervising schema patterns rather than relying solely on ICL.

---

# 4. Model Adaptation Strategies

Three adaptation families are evaluated:

| Strategy | Modifies Weights | Training | Iterative | Expected Consistency |
|---|---|---|---|---|
| Zero-Shot ICL | ✗ | ✗ | ✗ | Low |
| Few-Shot ICL | ✗ | ✗ | ✗ | Moderate |
| QLoRA FT | ✓ (Adapters) | ✓ | ✗ | High |
| ReAct / ExCoT | Optional | ✗ | ✓ | Highest |

This mirrors the structure in Ojuri et al. (2025), who compared GPT-4 prompting vs GPT-4 fine-tuned vs GPT-4 agent.

---

# 5. Training Configuration (Current Best Practice)

- Base model: **Llama-3-8B-Instruct**
- Quantization: **4-bit NF4**
- PEFT: **LoRA adapters only**
- Optimizer: `paged_adamw_8bit`
- VRAM target: **≤ 10GB (Colab T4)**
- Dtype: **bf16 on Ampere, fp16 fallback**
- Hyperparameters:
  - r = 32
  - α = 64
  - dropout = 0.05
  - LR ~ 1e-4
  - epochs ≈ 3
  - batch size = 1 w/ gradient accumulation = 8

This configuration allows fine-tuning without full model gradient updates, avoiding >60GB VRAM requirements of full FT.

---

# 6. Agentic Refinement (Planned)

The methodology includes planned integration of an execution-feedback agent based on:

- **ReAct** (Thought → Action → Observation) (Yao et al., 2023)
- **ExCoT** (execution traces) (Zhai et al., 2025)

Agentic refinement operationalises the hypothesis that:

1. prompting → generates a hypothesis
2. DB execution → tests the hypothesis
3. refinement → corrects structured errors

Ojuri et al. (2025) observed significant EX gains when including this loop.

---

# 7. Evaluation Criteria (Aligned with Ojuri et al., 2025)

Evaluation adopts the same semantic metrics:

| Metric | Target | Description |
|---|---|---|
| VA | Syntax | SQL executes without syntax error |
| EX | Semantics | Result-set matches expected output |
| EM | Surface | String match (strict baseline) |
| TS | Semantics+ | Perturbed DB test-suite accuracy |

Execution-based metrics (VA/EX/TS) are considered more meaningful for NL→SQL than surface metrics such as BLEU or exact string match.

---

# 8. Hardware and Compute Constraints

- **Training hardware:** Google Colab (T4, occasional L4/A100 availability)
- **VRAM budget:** 8–12GB
- **Inference:** consumer-grade GPU feasible

Constraint reflection (from QLoRA notebook):  
Training peaked at ~9.3GB VRAM—confirming PEFT viability under constrained compute conditions.

These constraints shape methodology; they are not incidental.

---

# 9. Comparative Method Justification

### Why not full fine-tuning?
Requires >60GB VRAM; infeasible for open-source reproduction.

### Why not prompting-only?
Few-shot ICL exhibited instability and schema sensitivity, consistent with Mosbach et al. (2023).

### Why agent loops?
Execution feedback reduces logical/aggregation/join errors beyond static decoding (Yao et al., 2023; Ojuri et al., 2025).

### Why ClassicModels?
Matches proprietary baseline domain and workloads; avoids distributional confounds.

### Implementation influences (provenance)
The implementation drew on three pillars: (1) NL→SQL literature for schema grounding, refinement, and execution-based evaluation; (2) open-source ecosystems (HF Transformers/PEFT, SQLAlchemy, Cloud SQL connectors) for idiomatic model loading, fine-tuning, and DB access; and (3) official framework docs for API-level details on inference, quantization, and execution. The orchestration here is original to this reproduction, but follows standard applied NLP practice of combining established tools to answer new research questions.

### Agent (ReAct/ExCoT) plan
- Pattern: Thought → Action (run SQL) → Observation (success/error) → Refinement; iterate a few steps.
- Implementation: new `03_agentic_eval.ipynb` will load the base or QLoRA-adapter model, build a ReAct loop around `QueryRunner`, and log traces.
- Justification: literature shows execution-feedback loops boost semantic accuracy beyond single-pass decoding [16], [2]; our runs still show low k=0 EX, so an agent is the next lever.

---

# 10. Reproducibility Measures

To ensure methodological transparency:

- hardware config logged
- software versions pinned
- test set frozen
- adapters checkpointed
- training seeds fixed
- evaluation deterministic
- no proprietary API calls

This aligns with reproducibility expectations in ML systems research.

---

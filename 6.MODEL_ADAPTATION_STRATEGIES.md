# Model Adaptation Strategies

This chapter defines the adaptation strategies evaluated in this project and explains why they form a coherent comparison set for NL→SQL. The three strategies—prompting, PEFT fine-tuning, and agentic refinement—cover the major adaptation axes identified in recent LLM-to-SQL literature and enable causal analysis of where semantic competence is acquired (*context vs weights vs feedback*). This structure mirrors Ojuri et al. (2025) and aligns with broader structured reasoning studies in LLMs.

### Adaptation Ladder

Empirically, NL→SQL workflows form a natural adaptation ladder:

| Strategy | Mechanism | Expected Gains |
|---|---|---|
| Zero-shot | no context, no weights | syntax / trivial projections |
| Few-shot | ICL (context only) | schema patterns / joins |
| QLoRA | weight adaptation | semantic mappings (joins, aggregates) |
| Agentic | feedback + retries | stability / error correction |

This matches staged evaluations used in Spider follow-ups and Ojuri et al. (2025), and reflects the taxonomy in recent surveys [Zhu et al., 2024; Hong et al., 2025].

---

## 1. Zero-shot Prompting (No Context, No Weights)

**Decision Rationale:**  
Zero-shot serves as the baseline for unconditioned schema-aware prompting, isolating model priors without exemplars or weight updates.

**Expected Behaviour:**  
Zero-shot typically succeeds on “listing” queries (e.g., `SELECT col FROM table`), but fails on:

- multi-hop joins
- grouping and aggregation
- conditional filtering
- implicit foreign key traversal

These failure modes match patterns reported in Spider/BIRD benchmarking and recent surveys.

**Research Value:**  
Zero-shot establishes a lower bound for SQL competence and provides a reference point for attributing improvements to context or weight adaptation.

---

## 2. Few-shot Prompting (In-Context Learning)

**Decision Rationale:**  
Few-shot prompting isolates the contribution of contextual pattern induction without modifying model parameters. Literature shows that ICL improves structural form but exhibits weak semantic generalisation for compositional tasks, particularly NL→SQL and mathematical reasoning.

**Expected Behaviour:**  
Few-shot improves:

- projection selection
- syntactic validity
- table selection and basic joins

but struggles with:

- numeric semantics (SUM/COUNT)
- grouping logic (GROUP BY)
- JOIN disambiguation
- status literals / domain semantics

**Observed Failure Mode:**  
Even when syntactically valid, ICL frequently produces “executable but wrong” SQL—i.e., VA=1 but EX=0. This mirrors reports that execution validity does not imply semantic faithfulness in structured domains.

**Motivation for Next Step:**  
Few-shot limitations motivate supervised domain adaptation (QLoRA) to internalise schema-specific mappings.

---

## 3. QLoRA (PEFT Fine-tuning)

**Decision Rationale:**  
Parameter-efficient adapters (QLoRA) enable domain adaptation under commodity VRAM budgets and allow semantic alignment without full fine-tuning cost. PEFT methods have been shown to internalise domain-specific behaviour in structured tasks, including NL→SQL.

**Expected Behaviour:**  
QLoRA improves:

- join path selection
- key/foreign-key mapping
- grouping/aggregation
- domain vocabulary (e.g., status literals, productLine)

This corresponds to the primary semantic failure modes in ICL.

**Scope Constraints:**  
While QLoRA improves the *content* of SQL, it does not enable:

- self-correction
- iterative error repair
- hypothesis testing via execution

**Orthogonality to Agentic Methods:**  
Supervised PEFT and agentic feedback are complementary adaptation mechanisms: one shapes priors in weights; the other applies external feedback as a corrective signal at inference time.

---

## 4. Agentic Refinement (Execution-Guided Reranker)

### Concept
Agentic refinement introduces an external interaction loop with the SQL engine. Unlike single-shot decoding, the model emits candidates, observes execution outcomes, and updates its choices accordingly.

Minimal execution-guided pattern (implemented):

for step in range(MAX_STEPS):
cands = generate(prompt)
cands = filter_SELECT_only(cands)
exec_results = run(cands)
best = rerank(exec_results, nlq)
if best.success:
return best.sql
prompt = augment_with_error_hint(best.error)


### Decision Rationale
This aligns with:

- **ReAct** for tool-use reasoning (Yao et al., 2023)
- **ExCoT** for execution-conditioned refinement (Zhai et al., 2025)
- **ValueNet / DIN-SQL** for critic-style reranking in NL→SQL
- the execution-feedback pipeline reported in Ojuri et al. (2025)

### Implementation Note
The implemented agent is an **execution-guided reranker**, not a full ReAct agent. Observations drive reranking and repair, but no explicit “Thought/Action” traces are generated. This distinction matters when interpreting EX gains and comparing against agent baselines.

---

## Summary: Why Compare These?

The strategies differ in **where learning occurs**:

| Strategy | Learns via | Strength | Limitation |
|---|---|---|---|
| Prompting | context only | fast, no training | weak semantics |
| QLoRA | weights | semantic mappings | no self-correction |
| Agentic | feedback loop | robustness, recovery | depends on base competence |

Together, they produce a structured experimental question:

> Can small open models close the gap to proprietary NL→SQL agents when given (a) supervision, (b) execution feedback, and (c) realistic hardware constraints?

**Empirical Relationship (preview):**

- Prompting → improves VA
- QLoRA → improves EX
- Agentic → improves EX stability on error-prone queries

This matches results in proprietary settings (Ojuri et al., 2025), supporting the claim that small open models require both supervised adapters and agentic refinement to approximate GPT-4 agent performance on NL→SQL tasks.


# Model Adaptation Strategies

This section defines the adaptation strategies evaluated in this project and explains why they form a coherent comparison set for NL→SQL. The strategies—prompting, PEFT fine-tuning, and agentic refinement—represent the three major adaptation axes in current LLM-to-SQL literature and enable causal analysis of where semantic competence is acquired (weights vs context vs feedback). This structure mirrors Ojuri et al. (2025) and reflects broader findings in structured LLM reasoning research.

### Adaptation Ladder

Empirically, the three strategies form a natural ladder:

| Strategy | Mechanism | Expected Gains |
|---|---|---|
| Zero-shot | no context, no weights | syntax / trivial projections |
| Few-shot | ICL (context only) | schema patterns / joins |
| QLoRA | weight adaptation | semantic mappings (joins, aggregates) |
| Agentic | feedback + retries | stability / error correction |

This matches the staged evaluation used in Spider follow-ups and Ojuri et al. (2025).

---

## 1. Zero-shot

**Decision Rationale:** Baseline for unconditioned schema-aware prompting; isolates model priors without exemplars or weight changes.

**Expected Behaviour:**  
Zero-shot typically succeeds on “listing” queries (e.g., SELECT columns FROM table) but fails on aggregation, grouping, and multi-hop joins, which require schema-aware reasoning and numeric operations.

---

## 2. Few-shot

**Decision Rationale:**  
Few-shot prompting isolates the contribution of contextual pattern induction without modifying weights. Literature shows that ICL improves structural form but exhibits weak semantic generalisation for compositional tasks, particularly NL→SQL and mathematical reasoning.

**Observed Failure Mode:**  
Even when syntactically valid, ICL models often produce “executable but wrong” SQL—queries that run (VA=1) but compute incorrect results (EX=0). This motivates downstream fine-tuning and agentic refinement.

---

## 3. QLoRA (PEFT Fine-Tuning)

**Decision Rationale:**  
Adapters (QLoRA) allow semantic alignment under commodity VRAM budgets, enabling supervised NL→SQL training without full finetune cost. Prior work reports that adapters improve join selection, aggregation, and grouping behaviour—the core semantic failure modes observed in ICL.

**Scope Constraints:**  
QLoRA alone does not address self-correction or execution stability; incorrect queries remain incorrect, even if better structured. This motivates agentic refinement.

---

## 4. Agentic Refinement (Execution-Guided Reranker)

### Concept
Agentic refinement introduces iterative interaction with an external tool (the SQL engine). The model emits candidate SQL, observes execution outcomes, and refines subsequent attempts. Unlike single-shot decoding, this adds an error-corrective feedback loop.

Pseudo-pattern (implemented):
```
for step in range(MAX_STEPS):
    cands = generate_candidates(prompt, NUM_CANDS)
    cands = filter_SELECT_only(cands)
    exec_results = [run(sql) for sql in cands]
    best = rerank(exec_results, nlq)
    if best.success:
        return best.sql
    observation = classify_error(best.error)
    prompt = augment_with_hint(observation)
```

### Decision Rationale
This aligns with ReAct-style tool-use (Yao et al., 2023) and execution-guided methods such as ValueNet and DIN-SQL, which treat the environment as an oracle for semantic grounding. Ojuri et al. (2025) show that execution feedback measurably improves EX on NL→SQL benchmarks.

### Implementation Note
The implemented agent is an **execution-guided reranker**, not a full ReAct agent—Observations are used for repair hints and reranking, but the model does not emit explicit Thought/Action traces. This distinction is important when attributing EX gains in evaluation.

---

### Summary: Why Compare These?

The strategies differ in where the burden of learning is placed:

| Strategy | Learns via | Strength | Weakness |
|---|---|---|---|
| Prompting | context | fast, no training | weak semantics |
| QLoRA | weights | semantic alignment | no self-correction |
| Agentic | feedback loop | stability, recovery | depends on base competence |

Together, they provide a structured way to investigate whether small open models can approximate proprietary NL→SQL agents under practical constraints.

*Optional tie to evaluation:* In results, prompting improves VA but not EX; QLoRA improves EX but not robustness; execution guidance improves robustness but not semantic leaps. This supports the claim that small open models need both adapters + agentic refinement to close the gap.

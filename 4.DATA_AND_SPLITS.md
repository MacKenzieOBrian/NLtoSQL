# Data & Splits (ClassicModels)

This project uses a fixed evaluation benchmark and a small supervised train set to replicate Ojuri et al. (2025) under open-source constraints. Strict splits and validation are critical to keep VA/EX comparisons meaningful.

## Benchmark (Test Set)
- File: `data/classicmodels_test_200.json`
- Size: 200 NLQ→SQL pairs (SELECT-only)
- Purpose: held-out evaluation for VA/EX (EM logged; TS planned)
- Validation: gold SQL executes on the live ClassicModels DB
- Format: `nlq`, `sql` (+ optional metadata: difficulty/tables/patterns)

## Few-Shot Exemplar Policy
- Used only for inference conditioning (no training).
- Must not leak test items; exemplar source must be documented.
- Runs of interest: `k=0` (no exemplars) and `k>0` (few-shot uplift). After QLoRA, both are still measured to separate adapter learning from in-context effects.

## Training Set (QLoRA SFT)
- File: `data/train/classicmodels_train_200.jsonl`
- Size: ~200 NLQ→SQL pairs (SELECT-only)
- Requirements: disjoint from test set; SQL executes (VA=True); schema-grounded; coverage of joins/aggregations/filters/orders; stable JSONL format (`nlq`, `sql`).
- Validation: via `notebooks/04_build_training_set.ipynb` (leakage check + executability).


## Artifacts
- Baseline outputs: `results/baseline/results_zero_shot_200.json`, `results/baseline/results_few_shot_k3_200.json`.
- QLoRA outputs (latest): `results/qlora/results_zero_shot_200.json`, `results/qlora/results_few_shot_k3_200.json`.
- Agentic/TS outputs: planned under `results/agent/…`.

## Why this structure
- Coverage: focused business SQL patterns provide enough signal for QLoRA under small VRAM budgets [12], [4].  
- Reproducibility: frozen splits and logged artifacts allow like-for-like reruns and comparison to Ojuri et al. (2025) [10].  


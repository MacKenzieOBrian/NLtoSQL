# AI prompts I used (learning, not outsourcing)

I swapped the old reflections for a list of prompts that helped me learn or grab small patterns. They’re framed to keep ownership of the code while getting quick guidance.

- Dependency/setup help  
  “On Colab, I get `triton.ops` missing with bitsandbytes. Show a minimal pip install block for torch 2.3.1+cu121 with bnb/triton pinned, and explain why a restart is needed.”

- Avoiding NumPy/ABI issues  
  “Colab keeps preloading NumPy; how do I avoid `numpy.dtype size changed` when I need torch+bnb? Give a one-cell install sequence.”

- Prompt + postprocess pattern  
  “Given a schema summary, build a prompt template for NL→SQL (system + schema + k exemplars + question). Keep decoding deterministic.”
  “After an LLM returns text, how do I extract the first `SELECT ...;` safely? Show a tiny Python helper.”

- Quick eval scaffolding  
  “What’s a simple way to compute VA/EX/EM for NL→SQL if I have a DB engine and gold SQL? Pseudocode is fine.”

- QLoRA basics  
  “Show a minimal TRL `SFTTrainer` + PEFT LoRA config for a 4-bit Llama-3-8B run on Colab (bf16 if available). Keep batch sizes tiny and explain the key params.”
  “How should I set `BitsAndBytesConfig` for 4-bit NF4 in Transformers? Short code snippet only.”

- Agentic loop outline  
  “Outline a tiny ReAct-style loop for NL→SQL: thought → propose SQL → run it → observe errors/results → refine up to 3 steps. Keep it framework-agnostic.”

- DB safety  
  “Using the GCP Cloud SQL Connector + SQLAlchemy creator, how do I ensure read-only SELECT-only execution? Show the creator pattern.”

These are about understanding and unblocking, not letting AI write the dissertation. All wiring, experiments, and decisions stayed in my hands.

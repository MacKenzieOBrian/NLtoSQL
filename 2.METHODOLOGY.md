# Methodology (open-source replica)

This section describes the methodology for reproducing and extending the NL→SQL agent pipeline proposed by Ojuri et al. (2025), using open-source models, supervised adapters, and execution-guided agentic refinement under Colab-class GPU constraints. The process is treated as a controlled, comparable evaluation across three adaptation strategies.

For the pipeline I think of it as a straight line: get schema awareness, pick a strategy (prompt / QLoRA / agent) to spit out SQL, run a quick safety/postprocess, execute on the live DB, score VA/EX/EM (TS later).

At a high level, the NL→SQL pipeline decomposes into five stable components:

1. NLQ ingestion  
2. schema-grounded prompt construction  
3. adaptation strategy (prompt / QLoRA / agentic)  
4. SQL postprocessing + safety guards  
5. execution + scoring (VA/EX/EM)  

This decomposition ensures each adaptation mechanism can be swapped without confounding changes to schema processing or evaluation.

Methodological Framing: NL→SQL is treated as a controlled study over three adaptation strategies (prompt-only, QLoRA adapters, execution-guided agent), all sharing the same schema-grounded prompt, deterministic decoding, live ClassicModels execution, and VA/EX/EM scoring. This keeps differences attributable to adaptation mechanisms, not pipeline or evaluation drift.

Prompt Variables (controlled):
- System instruction: fixed  
- Schema summary: fixed, ordered key-first  
- Exemplars (k): varied at {0, 3}  
- Decoding: deterministic (no sampling)  
- NLQ: unmodified natural language  
- Postprocess: guarded first-SELECT extraction  

These controls ensure differences in performance originate from model adaptation rather than prompt drift or decoding randomness.

Hypothesis Layer:
H1 — Prompt-only recovers syntax/schema but not semantic joins/aggregates.  
H2 — QLoRA adapters recover semantic mappings for joins/aggregates by learning domain-schema correspondences.  
H3 — Execution-guided agency improves semantic correctness further by allowing iterative self-correction from DB feedback.  

The pipeline tests each hypothesis in isolation and in combination.

---

# 3. Data & Evaluation Substrate

- Domain: **ClassicModels (MySQL)**
- Benchmarks: **200 held-out NL→SQL test examples**
- Training set: **~200 supervised examples (JSONL)**

**Rationale:** ClassicModels provides realistic business workloads involving joins, aggregates, and ordering. It also matches the domain used in Ojuri et al. (2025), enabling like-for-like reproduction against GPT-4 agent baselines.

Crucially, evaluation is grounded in **live execution** against ClassicModels rather than textual matching alone. This treats the DB as the semantic arbiter (EX), aligning with contemporary DB-interface literature.

---

# 4. Model Adaptation Strategies

Three adaptation families are evaluated:

| Strategy | Modifies Weights | Training | Iterative | Expected Consistency |
|---|---|---|---|---|
| Zero-Shot ICL | ✗ | ✗ | ✗ | Low |
| Few-Shot ICL | ✗ | ✗ | ✗ | Moderate |
| QLoRA FT | ✓ (Adapters) | ✓ | ✗ | High |
| ReAct / ExCoT | Optional | ✗ | ✓ | Highest |

This mirrors the structure in Ojuri et al. (2025), who compared GPT-4 prompting vs GPT-4 fine-tuned vs GPT-4 agent.

Justification: This prompt vs FT vs agent layout matches comparative text-to-SQL studies (e.g., Ojuri et al., Spider follow-ups) and lets us report not only accuracy but where the model improves (syntax vs semantics vs reasoning) under strict VRAM limits.

Experimental Design:
- Zero-shot and few-shot represent purely prompt-based conditioning.
- QLoRA introduces weight adaptation with minimal VRAM requirements.
- Agentic refinement introduces iterative interaction and DB feedback.

This allows studying both where semantic competence is learned (weights vs context vs feedback) and under what constraints.

ReAct Alignment (current implementation):
The implemented agent is a minimal execution-guided reranker. It samples multiple SQL candidates, filters to SELECT-only, executes them, classifies errors, injects repair hints, and reranks executables using a lightweight semantic score. Unlike full ReAct, Observations are not yet encoded as structured Thought/Action traces.

### 4.1 Lightweight Agent Helper Layer

**Current refinements:** Prompt stripping (decode only the reply), projection/ORDER clamps, execution-aware retry, and now a dependency-free helper layer (`nl2sql/agent_utils.py`) that (a) enforces a single-SELECT contract (`clean_candidate`), (b) diversifies prompts with a tabular variant, (c) injects a deterministic few-shot baseline candidate so the agent cannot underperform the prompt baseline, (d) classifies common MySQL errors into hints for repair, and (e) reranks executables with a lightweight semantic score (reward aggregates/joins/grouping when the NLQ implies them). Small slices reach VA/EX/EM=1.0; full-set runs still show low EX due to deeper semantic gaps.

Rationale:
This layer tests whether execution feedback alone can close the gap left after QLoRA, without requiring structured multi-step reasoning or full tool grammars.

**Planned next tweaks:** Extend semantic scoring rules (e.g., enforce SUM + GROUP BY on “total per …”; productLine → join products+orderdetails), widen repair to non-error semantic hints, and, if needed, retrain adapters on targeted aggregates/status/count patterns. TS will be added once EX stabilises.

### 4.2 Scope & Boundaries

The prototype is intentionally limited to execution-guided reranking rather than full ReAct. This boundary isolates a specific research question: *Does execution feedback alone meaningfully improve EX?* Results indicate it improves VA but EX remains limited.

### 4.3 Methodological Reflection

The staged design (prompt → QLoRA → execution-guided agent) decomposes semantic failure into interpretable categories (syntax, schema, joins, aggregates). This explains why prompt-only fails, what QLoRA fixes, and where agentic feedback is needed but insufficient.

### References by Design Choice

Schema-grounding → Zhu et al. (2024); Rajkumar et al. (2024)  
ICL vs FT → Mosbach et al. (2023); Chia et al. (2023); Li et al. (2023)  
PEFT/QLoRA → Hu et al. (2021); Dettmers et al. (2023); Ding et al. (2023)  
Constrained SQL → Scholak et al. (2021); Binder et al. (SQL-PaLM)  
Agentic refinement → Yao et al. (ReAct, 2023); Shinn et al. (Reflexion, 2023)  
Execution reranking → DIN-SQL / ValueNet; Gao et al. (2023)  
Benchmarks → BIRD, Spider, Hong et al. (2025)  
Statistical comparison → Dietterich (McNemar, 1998)

Comparative Justification:
The methodological choice closely mirrors Ojuri et al. (2025), enabling meaningful like-for-like comparison between open reproduction and proprietary GPT-4 agent baselines.

# 5. Training Configuration (Current Best Practice)

- Base model: **Llama-3-8B-Instruct**
- Quantization: **4-bit NF4**
- PEFT: **LoRA adapters only**
- Optimizer: `paged_adamw_8bit`
- VRAM target: **≤ 10GB (Colab T4)**
- Dtype: **bf16 on Ampere, fp16 fallback**
- Hyperparameters:
  - r = 32
  - α = 64
  - dropout = 0.05
  - LR ~ 1e-4
  - epochs ≈ 3
  - batch size = 1 w/ gradient accumulation = 8

This configuration allows fine-tuning without full model gradient updates, avoiding >60GB VRAM requirements of full FT.

---


# 9. Comparative Method Justification

### Why not full fine-tuning?
Requires >60GB VRAM; infeasible for open-source reproduction.

### Why not prompting-only?
Few-shot ICL exhibited instability and schema sensitivity, consistent with Mosbach et al. (2023).

### Why agent loops?
Execution feedback reduces logical/aggregation/join errors beyond static decoding (Yao et al., 2023; Ojuri et al., 2025).

### Why ClassicModels?
Matches proprietary baseline domain and workloads; avoids distributional confounds.

### Implementation influences (provenance)
The implementation drew on three pillars: (1) NL→SQL literature for schema grounding, refinement, and execution-based evaluation; (2) open-source ecosystems (HF Transformers/PEFT, SQLAlchemy, Cloud SQL connectors) for idiomatic model loading, fine-tuning, and DB access; and (3) official framework docs for API-level details on inference, quantization, and execution. The orchestration here is original to this reproduction, but follows standard applied NLP practice of combining established tools to answer new research questions.

Evaluation Philosophy: Reporting VA/EX/EM for each adaptation stage gives a diagnostic view of failure modes: prompt-only (low VA/low EX → syntax + semantic failures); QLoRA (high VA/moderate EX → semantic failures dominate); agentic (high VA/low–moderate EX → semantic stability is the frontier). This matches common NL→SQL error taxonomies (joins, aggregation, grouping, filters), lending external validity to the design.

External Validity: ClassicModels covers realistic multi-table business queries (orders, sales totals by product line, customer segments, geographies). Using a live DB (not just static gold) keeps executability as ground truth, aligning with DB-interface research that treats execution as the semantic arbiter.

---

# 10. Narrative for dissertation write-up (learning journey)

- **Baseline (few-shot, deterministic):** Stable VA/EX/EM anchor; schema-grounded prompt + `guarded_postprocess` kept projections minimal. This is the primary result set to report.
- **QLoRA adapters:** Improved VA and modest EX uplift with k=3; limited gains at k=0. Justifies keeping adapters as “best prompt+FT” comparator but not relying on them for zero-shot.
- **ReAct agent:** Multiple iterations: projection/ORDER clamps → select-only filters → execution-guided retry → sampled candidates + one-shot repair. Result: higher VA, but EX still volatile. Frame as exploratory; discuss failure modes (hallucinated columns, aggregation drift) and why execution feedback helps but is not yet converged.
- **Design choices tied to literature:** ReAct loop (Yao et al., 2023), constrained decoding/PICARD (Scholak et al., 2021) for projection/ORDER discipline, Spider EM strictness (Yu et al., 2018) motivating minimal outputs, execution-guided repair (DIN-SQL/ValueNet-style) for error recovery.
- **Reporting plan:** Present baseline and QLoRA as main quantitative results; include ReAct as an interpretability/agentic experiment with trace examples and a candid limitations paragraph. Note future work: richer error-aware retries, schema-level allowlist/grammar constraints, TS metric once schema perturbations are ready.

**Link to logbook:** Major changes mapped to dated entries (see LOGBOOK.md: 2025-12-14 baseline hygiene; 2026-01-12 QLoRA run; 2026-01-24 to 2026-01-31 ReAct iterations). Use these dates/commits in the dissertation timeline.

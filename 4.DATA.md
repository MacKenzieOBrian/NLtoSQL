# Data & Benchmarks

This project evaluates NL→SQL performance on a fixed benchmark built over the ClassicModels MySQL database. Dataset discipline is central because the dissertation compares prompting, fine-tuning, and agentic refinement strategies; without strict splits and validation, comparisons become scientifically meaningless.

For design rationale and scope boundaries, see `DECISIONS.md`.

---

## 1. Benchmark Dataset (ClassicModels-200)

- File: `data/classicmodels_test_200.json`
- Size: 200 examples
- Purpose: held-out evaluation for VA/EX/TS metrics
- Status: gold SQL validated to execute successfully on ClassicModels DB

### 1.1 Record Format

Each test record contains:

- `nlq`: natural language query
- `sql`: gold SQL (SELECT-only)

Optional fields planned:

- `difficulty`
- `tables`
- `patterns` (join/aggregation/subquery)
- `notes`

**Rationale:** These enable analysis of model failure modes without altering evaluation core.

**Reflection:** Notebook work showed most early failures occurred in join-heavy and aggregation queries, motivating optional difficulty annotation later.

---

## 2. Few-Shot Exemplar Policy (Evaluation Hygiene)

Few-shot prompting uses NLQ→SQL exemplars for inference conditioning only.

Evaluation constraints:

- exemplars must not leak test queries
- exemplar source must be documented
- exemplar ordering must be deterministic

Exemplar configurations of interest:

- `k = 0`: isolates model competence without ICL
- `k > 0`: measures exemplar-induced improvements

After QLoRA fine-tuning, both are still meaningful:

- `k = 0` evaluates adapter learning
- `k > 0` evaluates remaining gains from ICL

**Notebook Observation:** Few-shot provided significant EX improvements over zero-shot prior to fine-tuning, consistent with literature on schema grounding effects.

---

## 3. Training Dataset (QLoRA SFT)

File: `data/train/classicmodels_train_200.jsonl`

Purpose: supervised fine-tuning for parameter-efficient adaptation.

### 3.1 Training Disciplinary Requirements

The training set must satisfy:

- **No leakage:** disjoint from test set
- **Validated:** gold SQL executes (VA=True)
- **Schema-grounded:** uses actual tables/columns
- **Coverage:** includes joins, aggregates, filtering, ordering, grouping
- **Format-stable:** `{nlq, sql, schema}` for consistent tokenization

Leakage is disallowed because it breaks the ability to measure generalization. TS results become invalid if leakage occurs.

Training set validated via `notebooks/04_build_training_set.ipynb`.

---

## 4. Planned Supplements for TS (Test-Suite)

Test-suite accuracy (TS) requires multiple DB variants.

Planned supplemental datasets:

- **Distilled DB variants:** same schema, perturbed data
- **Schema cache:** JSON schema snapshot for reproducible prompting
- **Pattern annotations:** optional for failure mode taxonomy

**Rationale:** TS detects semantic correctness beyond specific data values, avoiding accidental correctness.

---

## 5. Validation & QC Procedures

QC consists of:

1. **Executability Check**
   - Evaluate VA by running gold SQL against the DB
2. **Formatting & Normalization**
   - Normalise whitespace/aliases to avoid EM false negatives
3. **De-duplication**
   - Remove near-identical NLQs to maintain pattern coverage
4. **Manual Spot Check**
   - Audit sample (≈20–50 items) for semantic alignment

**Limitation:** Executability does not imply semantic correctness; manual review remains necessary for semantic QC.

---

## 6. Output Artifacts

Evaluation notebooks emit JSON results under `results/`:

- `results/baseline/results_zero_shot_200.json`
- `results/baseline/results_few_shot_k3_200.json`

- `results/qlora/results_qlora_k0_200.json` (planned)
- `results/agentic/results_react_200.json` (planned)

Each record includes:

- `nlq`
- `gold_sql`
- `pred_sql`
- `va` (valid SQL)
- `ex` (execution accuracy)
- `em` (string match)
- `error` (if applicable)
- run metadata (seed, k, timestamp, commit)

**Rationale:** Metadata supports reproducibility, auditability, and statistical comparison.

---

## 7. Alignment With Ojuri et al. (2025)

Metric alignment:

| Metric | This Project | Ojuri et al. |
|---|---|---|
| EM | ✔ (baseline) | ✔ |
| VA | ✔ | ✔ |
| EX | ✔ | ✔ |
| TS | planned | ✔ |

Benchmark alignment:

| Component | This Project | Ojuri et al. |
|---|---|---|
| Domain | ClassicModels | ClassicModels |
| Model Strategies | Prompting / QLoRA / Agent | Prompting / GPT-4 FT / Agent |
| Compute | Colab (T4/L4) | Proprietary GPU cluster |
| APIs | None | GPT-4 API |

This provides traceability for reproduction fidelity.

---

## 8. Meta Reflection

Dataset considerations in this project focus on:

- **fairness** (no leakage)
- **reliability** (execution validation)
- **coverage** (business SQL patterns)
- **reproducibility** (frozen splits)
- **comparability** (Ojuri alignment)
- **compute realism** (SME deployment)

This reframes the data component from “just support code” into a methodological pillar of the study.
